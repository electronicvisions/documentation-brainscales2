<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>hxtorch.spiking.functional.lif.LIF &mdash; BrainScaleS-2 Documentation 0.0.1 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/visions.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="hxtorch.spiking.functional.lif.NamedTuple" href="hxtorch.spiking.functional.lif.NamedTuple.html" />
    <link rel="prev" title="hxtorch.spiking.functional.lif.CUBALIFParams" href="hxtorch.spiking.functional.lif.CUBALIFParams.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> BrainScaleS-2 Documentation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../brainscales2-demos/index.html">Demos &amp; Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software_components.html">Software Components</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../apis.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api_pynn-brainscales2.html">PyNN API</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api_hxtorch.html">hxtorch API</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../api_hxtorch.html#hxtorch-from-python">hxtorch from Python</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="hxtorch.html">hxtorch</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_hxtorch.html#hxtorch-from-c">hxtorch from C++</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_grenade.html">grenade API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_haldls.html">haldls API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_lola.html">lola API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_stadls.html">stadls API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_fisch.html">fisch API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_hxcomm.html">hxcomm API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_calix.html">calix API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_hate.html">hate API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">BrainScaleS-2 Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../apis.html">BrainScaleS-2 API Documentation</a> &raquo;</li>
          <li><a href="../api_hxtorch.html">API Reference: hxtorch</a> &raquo;</li>
          <li><a href="hxtorch.html">hxtorch</a> &raquo;</li>
          <li><a href="hxtorch.spiking.html">hxtorch.spiking</a> &raquo;</li>
          <li><a href="hxtorch.spiking.functional.html">hxtorch.spiking.functional</a> &raquo;</li>
          <li><a href="hxtorch.spiking.functional.lif.html">hxtorch.spiking.functional.lif</a> &raquo;</li>
      <li>hxtorch.spiking.functional.lif.LIF</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/_autosummary/hxtorch.spiking.functional.lif.LIF.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="hxtorch-spiking-functional-lif-lif">
<h1>hxtorch.spiking.functional.lif.LIF<a class="headerlink" href="#hxtorch-spiking-functional-lif-lif" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt id="hxtorch.spiking.functional.lif.LIF">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">hxtorch.spiking.functional.lif.</span></code><code class="sig-name descname"><span class="pre">LIF</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>LIF forward mock and backward</p>
<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.__init__">
<code class="sig-name descname"><span class="pre">__init__</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.__init__" title="hxtorch.spiking.functional.lif.LIF.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a>(*args, **kwargs)</p></td>
<td><p>Initialize self.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.apply" title="hxtorch.spiking.functional.lif.LIF.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(*args, **kwargs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.backward" title="hxtorch.spiking.functional.lif.LIF.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(ctx, grad_output)</p></td>
<td><p>Implements LIF backward</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(ctx, input)</p></td>
<td><p>Gets overridden</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.jvp" title="hxtorch.spiking.functional.lif.LIF.jvp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jvp</span></code></a>(ctx, *grad_inputs)</p></td>
<td><p>Defines a formula for differentiating the operation with forward mode automatic differentiation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.mark_dirty" title="hxtorch.spiking.functional.lif.LIF.mark_dirty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mark_dirty</span></code></a>(*args)</p></td>
<td><p>Marks given tensors as modified in an in-place operation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.mark_non_differentiable" title="hxtorch.spiking.functional.lif.LIF.mark_non_differentiable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mark_non_differentiable</span></code></a>(*args)</p></td>
<td><p>Marks outputs as non-differentiable.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.mark_shared_storage" title="hxtorch.spiking.functional.lif.LIF.mark_shared_storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mark_shared_storage</span></code></a>(*pairs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.maybe_clear_saved_tensors" title="hxtorch.spiking.functional.lif.LIF.maybe_clear_saved_tensors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maybe_clear_saved_tensors</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.name" title="hxtorch.spiking.functional.lif.LIF.name"><code class="xref py py-obj docutils literal notranslate"><span class="pre">name</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.register_hook" title="hxtorch.spiking.functional.lif.LIF.register_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_hook</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.register_prehook" title="hxtorch.spiking.functional.lif.LIF.register_prehook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_prehook</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.save_for_backward" title="hxtorch.spiking.functional.lif.LIF.save_for_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_for_backward</span></code></a>(*tensors)</p></td>
<td><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.save_for_forward" title="hxtorch.spiking.functional.lif.LIF.save_for_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_for_forward</span></code></a>(*tensors)</p></td>
<td><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.set_materialize_grads" title="hxtorch.spiking.functional.lif.LIF.set_materialize_grads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_materialize_grads</span></code></a>(value)</p></td>
<td><p>Sets whether to materialize grad tensors.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.setup_context" title="hxtorch.spiking.functional.lif.LIF.setup_context"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup_context</span></code></a>(ctx, inputs, output)</p></td>
<td><p>There are two ways to define the forward pass of an autograd.Function.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.vjp" title="hxtorch.spiking.functional.lif.LIF.vjp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vjp</span></code></a>(ctx, *grad_outputs)</p></td>
<td><p>Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.vmap" title="hxtorch.spiking.functional.lif.LIF.vmap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vmap</span></code></a>(info, in_dims, *args)</p></td>
<td><p>Defines a rule for the behavior of this autograd.Function underneath <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code>.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.dirty_tensors" title="hxtorch.spiking.functional.lif.LIF.dirty_tensors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dirty_tensors</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.generate_vmap_rule" title="hxtorch.spiking.functional.lif.LIF.generate_vmap_rule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.is_traceable" title="hxtorch.spiking.functional.lif.LIF.is_traceable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_traceable</span></code></a></p></td>
<td><p>Bool that specifies if PyTorch should attempt to autogenerate <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code> support for this autograd.Function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.materialize_grads" title="hxtorch.spiking.functional.lif.LIF.materialize_grads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">materialize_grads</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.metadata" title="hxtorch.spiking.functional.lif.LIF.metadata"><code class="xref py py-obj docutils literal notranslate"><span class="pre">metadata</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.needs_input_grad" title="hxtorch.spiking.functional.lif.LIF.needs_input_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">needs_input_grad</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.next_functions" title="hxtorch.spiking.functional.lif.LIF.next_functions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">next_functions</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.non_differentiable" title="hxtorch.spiking.functional.lif.LIF.non_differentiable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">non_differentiable</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.requires_grad" title="hxtorch.spiking.functional.lif.LIF.requires_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.saved_for_forward" title="hxtorch.spiking.functional.lif.LIF.saved_for_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">saved_for_forward</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.saved_tensors" title="hxtorch.spiking.functional.lif.LIF.saved_tensors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">saved_tensors</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.saved_variables" title="hxtorch.spiking.functional.lif.LIF.saved_variables"><code class="xref py py-obj docutils literal notranslate"><span class="pre">saved_variables</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.to_save" title="hxtorch.spiking.functional.lif.LIF.to_save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_save</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.apply">
<em class="property"><span class="pre">classmethod</span> </em><code class="sig-name descname"><span class="pre">apply</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.apply" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.backward">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">backward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.backward" title="Permalink to this definition"></a></dt>
<dd><p>Implements LIF backward</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.dirty_tensors">
<code class="sig-name descname"><span class="pre">dirty_tensors</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.dirty_tensors" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.forward">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.forward" title="Permalink to this definition"></a></dt>
<dd><p>Gets overridden</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.generate_vmap_rule">
<code class="sig-name descname"><span class="pre">generate_vmap_rule</span></code><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.generate_vmap_rule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.is_traceable">
<code class="sig-name descname"><span class="pre">is_traceable</span></code><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.is_traceable" title="Permalink to this definition"></a></dt>
<dd><p>Bool that specifies if PyTorch should attempt to autogenerate
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code> support for this autograd.Function. You may set this to
True only if this autograd.Function’s forward, backward, and jvp (if they
exist) are written using PyTorch operations; otherwise, please override
<code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.vmap()</span></code> to add support for <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code>.</p>
<p>Please see <span class="xref std std-ref">func-autograd-function</span> for more details.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.jvp">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">jvp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Any</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.jvp" title="Permalink to this definition"></a></dt>
<dd><p>Defines a formula for differentiating the operation with forward mode
automatic differentiation.
This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.mark_dirty">
<code class="sig-name descname"><span class="pre">mark_dirty</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.mark_dirty" title="Permalink to this definition"></a></dt>
<dd><p>Marks given tensors as modified in an in-place operation.</p>
<p><strong>This should be called at most once, only from inside the</strong>
<a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> <strong>method, and all arguments should be inputs.</strong></p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.mark_non_differentiable">
<code class="sig-name descname"><span class="pre">mark_non_differentiable</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.mark_non_differentiable" title="Permalink to this definition"></a></dt>
<dd><p>Marks outputs as non-differentiable.</p>
<p><strong>This should be called at most once, only from inside the</strong>
<a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> <strong>method, and all arguments should be tensor outputs.</strong></p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.mark_shared_storage">
<code class="sig-name descname"><span class="pre">mark_shared_storage</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.mark_shared_storage" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.materialize_grads">
<code class="sig-name descname"><span class="pre">materialize_grads</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.materialize_grads" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.maybe_clear_saved_tensors">
<code class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.maybe_clear_saved_tensors" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.metadata">
<code class="sig-name descname"><span class="pre">metadata</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.metadata" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.name">
<code class="sig-name descname"><span class="pre">name</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.needs_input_grad">
<code class="sig-name descname"><span class="pre">needs_input_grad</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.needs_input_grad" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.next_functions">
<code class="sig-name descname"><span class="pre">next_functions</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.next_functions" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.non_differentiable">
<code class="sig-name descname"><span class="pre">non_differentiable</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.non_differentiable" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.register_hook">
<code class="sig-name descname"><span class="pre">register_hook</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.register_hook" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.register_prehook">
<code class="sig-name descname"><span class="pre">register_prehook</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.register_prehook" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.requires_grad">
<code class="sig-name descname"><span class="pre">requires_grad</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.requires_grad" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.save_for_backward">
<code class="sig-name descname"><span class="pre">save_for_backward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.save_for_backward" title="Permalink to this definition"></a></dt>
<dd><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, only from inside the
<a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> method, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.backward" title="hxtorch.spiking.functional.lif.LIF.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.backward" title="hxtorch.spiking.functional.lif.LIF.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.saved_tensors" title="hxtorch.spiking.functional.lif.LIF.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <span class="xref std std-ref">extending-autograd</span> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.save_for_forward">
<code class="sig-name descname"><span class="pre">save_for_forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.save_for_forward" title="Permalink to this definition"></a></dt>
<dd><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp()</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be only called once, from inside the <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>
method, and only be called with tensors.</p>
<p>In <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.jvp" title="hxtorch.spiking.functional.lif.LIF.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp()</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.saved_tensors" title="hxtorch.spiking.functional.lif.LIF.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <span class="xref std std-ref">extending-autograd</span> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.saved_for_forward">
<code class="sig-name descname"><span class="pre">saved_for_forward</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.saved_for_forward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.saved_tensors">
<code class="sig-name descname"><span class="pre">saved_tensors</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.saved_tensors" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.saved_variables">
<code class="sig-name descname"><span class="pre">saved_variables</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.saved_variables" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.set_materialize_grads">
<code class="sig-name descname"><span class="pre">set_materialize_grads</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.set_materialize_grads" title="Permalink to this definition"></a></dt>
<dd><p>Sets whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p><strong>This should be called only from inside the</strong> <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> <strong>method</strong></p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.backward" title="hxtorch.spiking.functional.lif.LIF.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> and <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.jvp" title="hxtorch.spiking.functional.lif.LIF.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp()</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.setup_context">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">setup_context</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Any</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.setup_context" title="Permalink to this definition"></a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature forward(ctx, <a href="#id1"><span class="problematic" id="id2">*</span></a>args, <a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs).
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature forward(<a href="#id5"><span class="problematic" id="id6">*</span></a>args, <a href="#id7"><span class="problematic" id="id8">**</span></a>kwargs) and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward()</span></code> and <span class="xref std std-ref">extending-autograd</span> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.functional.lif.LIF.to_save">
<code class="sig-name descname"><span class="pre">to_save</span></code><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.to_save" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.vjp">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">vjp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Any</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.vjp" title="Permalink to this definition"></a></dt>
<dd><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.backward" title="hxtorch.spiking.functional.lif.LIF.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#hxtorch.spiking.functional.lif.LIF.forward" title="hxtorch.spiking.functional.lif.LIF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.functional.lif.LIF.vmap">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">vmap</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.functional.lif.LIF.vmap" title="Permalink to this definition"></a></dt>
<dd><p>Defines a rule for the behavior of this autograd.Function underneath
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code>. For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function()</span></code> to support
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code>, you must either override this staticmethod, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <span class="xref std std-ref">func-autograd-function</span> for more details.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hxtorch.spiking.functional.lif.CUBALIFParams.html" class="btn btn-neutral float-left" title="hxtorch.spiking.functional.lif.CUBALIFParams" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hxtorch.spiking.functional.lif.NamedTuple.html" class="btn btn-neutral float-right" title="hxtorch.spiking.functional.lif.NamedTuple" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Electronic Vision(s).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>