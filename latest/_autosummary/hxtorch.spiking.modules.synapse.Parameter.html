<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>hxtorch.spiking.modules.synapse.Parameter &mdash; BrainScaleS-2 Documentation 0.0.1 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/visions.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="hxtorch.spiking.modules.synapse.Projection" href="hxtorch.spiking.modules.synapse.Projection.html" />
    <link rel="prev" title="hxtorch.spiking.modules.synapse" href="hxtorch.spiking.modules.synapse.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> BrainScaleS-2 Documentation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../brainscales2-demos/index.html">Demos &amp; Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software_components.html">Software Components</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../apis.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api_pynn-brainscales2.html">PyNN API</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api_hxtorch.html">hxtorch API</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../api_hxtorch.html#hxtorch-from-python">hxtorch from Python</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="hxtorch.html">hxtorch</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_hxtorch.html#hxtorch-from-c">hxtorch from C++</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_grenade.html">grenade API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_haldls.html">haldls API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_lola.html">lola API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_stadls.html">stadls API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_fisch.html">fisch API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_hxcomm.html">hxcomm API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_calix.html">calix API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_hate.html">hate API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">BrainScaleS-2 Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../apis.html">BrainScaleS-2 API Documentation</a> &raquo;</li>
          <li><a href="../api_hxtorch.html">API Reference: hxtorch</a> &raquo;</li>
          <li><a href="hxtorch.html">hxtorch</a> &raquo;</li>
          <li><a href="hxtorch.spiking.html">hxtorch.spiking</a> &raquo;</li>
          <li><a href="hxtorch.spiking.modules.html">hxtorch.spiking.modules</a> &raquo;</li>
          <li><a href="hxtorch.spiking.modules.synapse.html">hxtorch.spiking.modules.synapse</a> &raquo;</li>
      <li>hxtorch.spiking.modules.synapse.Parameter</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/_autosummary/hxtorch.spiking.modules.synapse.Parameter.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="hxtorch-spiking-modules-synapse-parameter">
<h1>hxtorch.spiking.modules.synapse.Parameter<a class="headerlink" href="#hxtorch-spiking-modules-synapse-parameter" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt id="hxtorch.spiking.modules.synapse.Parameter">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">hxtorch.spiking.modules.synapse.</span></code><code class="sig-name descname"><span class="pre">Parameter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
<p>A kind of Tensor that is to be considered a module parameter.</p>
<p>Parameters are <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> subclasses, that have a
very special property when used with <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> s - when they’re
assigned as Module attributes they are automatically added to the list of
its parameters, and will appear e.g. in <code class="xref py py-meth docutils literal notranslate"><span class="pre">parameters()</span></code> iterator.
Assigning a Tensor doesn’t have such effect. This is because one might
want to cache some temporary state, like last hidden state of the RNN, in
the model. If there was no such class as <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter" title="hxtorch.spiking.modules.synapse.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>, these
temporaries would get registered too.</p>
<dl>
<dt>Args:</dt><dd><p>data (Tensor): parameter tensor.
requires_grad (bool, optional): if the parameter requires gradient. See</p>
<blockquote>
<div><p><span class="xref std std-ref">locally-disable-grad-doc</span> for more details. Default: <cite>True</cite></p>
</div></blockquote>
</dd>
</dl>
<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.__init__">
<code class="sig-name descname"><span class="pre">__init__</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.__init__" title="hxtorch.spiking.modules.synapse.Parameter.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a>()</p></td>
<td><p>Initialize self.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.abs" title="hxtorch.spiking.modules.synapse.Parameter.abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">abs</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.abs_" title="hxtorch.spiking.modules.synapse.Parameter.abs_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">abs_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">abs()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.absolute" title="hxtorch.spiking.modules.synapse.Parameter.absolute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">absolute</span></code></a>()</p></td>
<td><p>Alias for <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.abs" title="hxtorch.spiking.modules.synapse.Parameter.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">abs()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.absolute_" title="hxtorch.spiking.modules.synapse.Parameter.absolute_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">absolute_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">absolute()</span></code> Alias for <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.abs_" title="hxtorch.spiking.modules.synapse.Parameter.abs_"><code class="xref py py-func docutils literal notranslate"><span class="pre">abs_()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.acos" title="hxtorch.spiking.modules.synapse.Parameter.acos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acos</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.acos_" title="hxtorch.spiking.modules.synapse.Parameter.acos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acos_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">acos()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.acosh" title="hxtorch.spiking.modules.synapse.Parameter.acosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acosh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acosh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.acosh_" title="hxtorch.spiking.modules.synapse.Parameter.acosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acosh_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">acosh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.add" title="hxtorch.spiking.modules.synapse.Parameter.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a>(other, *[, alpha])</p></td>
<td><p>Add a scalar or tensor to <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.add_" title="hxtorch.spiking.modules.synapse.Parameter.add_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_</span></code></a>(other, *[, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addbmm" title="hxtorch.spiking.modules.synapse.Parameter.addbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addbmm</span></code></a>(batch1, batch2, *[, beta, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addbmm_" title="hxtorch.spiking.modules.synapse.Parameter.addbmm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addbmm_</span></code></a>(batch1, batch2, *[, beta, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addbmm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addcdiv" title="hxtorch.spiking.modules.synapse.Parameter.addcdiv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcdiv</span></code></a>(tensor1, tensor2, *[, value])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcdiv()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addcdiv_" title="hxtorch.spiking.modules.synapse.Parameter.addcdiv_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcdiv_</span></code></a>(tensor1, tensor2, *[, value])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addcdiv()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addcmul" title="hxtorch.spiking.modules.synapse.Parameter.addcmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcmul</span></code></a>(tensor1, tensor2, *[, value])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcmul()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addcmul_" title="hxtorch.spiking.modules.synapse.Parameter.addcmul_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcmul_</span></code></a>(tensor1, tensor2, *[, value])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addcmul()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addmm" title="hxtorch.spiking.modules.synapse.Parameter.addmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmm</span></code></a>(mat1, mat2, *[, beta, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addmm_" title="hxtorch.spiking.modules.synapse.Parameter.addmm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmm_</span></code></a>(mat1, mat2, *[, beta, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addmv" title="hxtorch.spiking.modules.synapse.Parameter.addmv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmv</span></code></a>(mat, vec, *[, beta, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmv()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addmv_" title="hxtorch.spiking.modules.synapse.Parameter.addmv_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmv_</span></code></a>(mat, vec, *[, beta, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addmv()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addr" title="hxtorch.spiking.modules.synapse.Parameter.addr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addr</span></code></a>(vec1, vec2, *[, beta, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addr()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.addr_" title="hxtorch.spiking.modules.synapse.Parameter.addr_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addr_</span></code></a>(vec1, vec2, *[, beta, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addr()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.adjoint" title="hxtorch.spiking.modules.synapse.Parameter.adjoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjoint</span></code></a>()</p></td>
<td><p>Alias for <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.adjoint" title="hxtorch.spiking.modules.synapse.Parameter.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.align_as" title="hxtorch.spiking.modules.synapse.Parameter.align_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">align_as</span></code></a>(other)</p></td>
<td><p>Permutes the dimensions of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to match the dimension order in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> tensor, adding size-one dims for any new names.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.align_to" title="hxtorch.spiking.modules.synapse.Parameter.align_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">align_to</span></code></a>(*names)</p></td>
<td><p>Permutes the dimensions of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to match the order specified in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>, adding size-one dims for any new names.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.all" title="hxtorch.spiking.modules.synapse.Parameter.all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.all()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.allclose" title="hxtorch.spiking.modules.synapse.Parameter.allclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">allclose</span></code></a>(other[, rtol, atol, equal_nan])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.allclose()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.amax" title="hxtorch.spiking.modules.synapse.Parameter.amax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amax</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amax()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.amin" title="hxtorch.spiking.modules.synapse.Parameter.amin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amin</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amin()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.aminmax" title="hxtorch.spiking.modules.synapse.Parameter.aminmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">aminmax</span></code></a>(*[, dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.aminmax()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.angle" title="hxtorch.spiking.modules.synapse.Parameter.angle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">angle</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.angle()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.any" title="hxtorch.spiking.modules.synapse.Parameter.any"><code class="xref py py-obj docutils literal notranslate"><span class="pre">any</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.any()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.apply_" title="hxtorch.spiking.modules.synapse.Parameter.apply_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply_</span></code></a>(callable)</p></td>
<td><p>Applies the function <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> to each element in the tensor, replacing each element with the value returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arccos" title="hxtorch.spiking.modules.synapse.Parameter.arccos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccos</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arccos()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arccos_" title="hxtorch.spiking.modules.synapse.Parameter.arccos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccos_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arccos()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arccosh" title="hxtorch.spiking.modules.synapse.Parameter.arccosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccosh</span></code></a></p></td>
<td><p>acosh() -&gt; Tensor</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arccosh_" title="hxtorch.spiking.modules.synapse.Parameter.arccosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccosh_</span></code></a></p></td>
<td><p>acosh_() -&gt; Tensor</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arcsin" title="hxtorch.spiking.modules.synapse.Parameter.arcsin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsin</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsin()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arcsin_" title="hxtorch.spiking.modules.synapse.Parameter.arcsin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsin_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsin()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arcsinh" title="hxtorch.spiking.modules.synapse.Parameter.arcsinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsinh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsinh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arcsinh_" title="hxtorch.spiking.modules.synapse.Parameter.arcsinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsinh_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsinh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arctan" title="hxtorch.spiking.modules.synapse.Parameter.arctan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arctan2" title="hxtorch.spiking.modules.synapse.Parameter.arctan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan2</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan2()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arctan2_" title="hxtorch.spiking.modules.synapse.Parameter.arctan2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan2_</span></code></a></p></td>
<td><p>atan2_(other) -&gt; Tensor</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arctan_" title="hxtorch.spiking.modules.synapse.Parameter.arctan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arctan()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arctanh" title="hxtorch.spiking.modules.synapse.Parameter.arctanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctanh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctanh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.arctanh_" title="hxtorch.spiking.modules.synapse.Parameter.arctanh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctanh_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arctanh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.argmax" title="hxtorch.spiking.modules.synapse.Parameter.argmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmax</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmax()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.argmin" title="hxtorch.spiking.modules.synapse.Parameter.argmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmin</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmin()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.argsort" title="hxtorch.spiking.modules.synapse.Parameter.argsort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argsort</span></code></a>([dim, descending])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argsort()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.argwhere" title="hxtorch.spiking.modules.synapse.Parameter.argwhere"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argwhere</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argwhere()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.as_strided" title="hxtorch.spiking.modules.synapse.Parameter.as_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided</span></code></a>(size, stride[, storage_offset])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.as_strided_" title="hxtorch.spiking.modules.synapse.Parameter.as_strided_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided_</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.as_strided_scatter" title="hxtorch.spiking.modules.synapse.Parameter.as_strided_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided_scatter</span></code></a>(src, size, stride[, …])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided_scatter()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.as_subclass" title="hxtorch.spiking.modules.synapse.Parameter.as_subclass"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_subclass</span></code></a>(cls)</p></td>
<td><p>Makes a <code class="docutils literal notranslate"><span class="pre">cls</span></code> instance with the same data pointer as <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.asin" title="hxtorch.spiking.modules.synapse.Parameter.asin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asin</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.asin_" title="hxtorch.spiking.modules.synapse.Parameter.asin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asin_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.asinh" title="hxtorch.spiking.modules.synapse.Parameter.asinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asinh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asinh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.asinh_" title="hxtorch.spiking.modules.synapse.Parameter.asinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asinh_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">asinh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.atan" title="hxtorch.spiking.modules.synapse.Parameter.atan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.atan2" title="hxtorch.spiking.modules.synapse.Parameter.atan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan2</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.atan2_" title="hxtorch.spiking.modules.synapse.Parameter.atan2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan2_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">atan2()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.atan_" title="hxtorch.spiking.modules.synapse.Parameter.atan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">atan()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.atanh" title="hxtorch.spiking.modules.synapse.Parameter.atanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atanh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atanh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.atanh_" title="hxtorch.spiking.modules.synapse.Parameter.atanh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atanh_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">atanh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>([gradient, retain_graph, …])</p></td>
<td><p>Computes the gradient of current tensor w.r.t.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.baddbmm" title="hxtorch.spiking.modules.synapse.Parameter.baddbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">baddbmm</span></code></a>(batch1, batch2, *[, beta, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.baddbmm()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.baddbmm_" title="hxtorch.spiking.modules.synapse.Parameter.baddbmm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">baddbmm_</span></code></a>(batch1, batch2, *[, beta, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">baddbmm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bernoulli" title="hxtorch.spiking.modules.synapse.Parameter.bernoulli"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bernoulli</span></code></a>(*[, generator])</p></td>
<td><p>Returns a result tensor where each <span class="math notranslate nohighlight">\(\texttt{result[i]}\)</span> is independently sampled from <span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{self[i]})\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bernoulli_" title="hxtorch.spiking.modules.synapse.Parameter.bernoulli_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bernoulli_</span></code></a>([p, generator])</p></td>
<td><p>Fills each location of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> with an independent sample from <span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{p})\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bfloat16" title="hxtorch.spiking.modules.synapse.Parameter.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.bfloat16()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bfloat16)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bincount" title="hxtorch.spiking.modules.synapse.Parameter.bincount"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bincount</span></code></a>([weights, minlength])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bincount()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_and" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_and</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_and()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_and_" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_and_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_and_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_and()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_left_shift</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_left_shift()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift_" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_left_shift_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_left_shift()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_not" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_not</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_not()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_not_" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_not_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_not_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_not()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_or" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_or</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_or()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_or_" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_or_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_or_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_or()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_right_shift</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_right_shift()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift_" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_right_shift_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_right_shift()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_xor" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_xor</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_xor()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_xor_" title="hxtorch.spiking.modules.synapse.Parameter.bitwise_xor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_xor_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_xor()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bmm" title="hxtorch.spiking.modules.synapse.Parameter.bmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bmm</span></code></a>(batch2)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.bool()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bool)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.broadcast_to" title="hxtorch.spiking.modules.synapse.Parameter.broadcast_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_to</span></code></a>(shape)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.broadcast_to()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.byte" title="hxtorch.spiking.modules.synapse.Parameter.byte"><code class="xref py py-obj docutils literal notranslate"><span class="pre">byte</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.byte()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.uint8)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cauchy_" title="hxtorch.spiking.modules.synapse.Parameter.cauchy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cauchy_</span></code></a>([median, sigma, generator])</p></td>
<td><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ccol_indices" title="hxtorch.spiking.modules.synapse.Parameter.ccol_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ccol_indices</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cdouble" title="hxtorch.spiking.modules.synapse.Parameter.cdouble"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cdouble</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.cdouble()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex128)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ceil" title="hxtorch.spiking.modules.synapse.Parameter.ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ceil</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ceil_" title="hxtorch.spiking.modules.synapse.Parameter.ceil_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ceil_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ceil()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cfloat" title="hxtorch.spiking.modules.synapse.Parameter.cfloat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cfloat</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.cfloat()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex64)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.chalf" title="hxtorch.spiking.modules.synapse.Parameter.chalf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chalf</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.chalf()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex32)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.char" title="hxtorch.spiking.modules.synapse.Parameter.char"><code class="xref py py-obj docutils literal notranslate"><span class="pre">char</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.char()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int8)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cholesky" title="hxtorch.spiking.modules.synapse.Parameter.cholesky"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky</span></code></a>([upper])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cholesky_inverse" title="hxtorch.spiking.modules.synapse.Parameter.cholesky_inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky_inverse</span></code></a>([upper])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_inverse()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cholesky_solve" title="hxtorch.spiking.modules.synapse.Parameter.cholesky_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky_solve</span></code></a>(input2[, upper])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.chunk" title="hxtorch.spiking.modules.synapse.Parameter.chunk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chunk</span></code></a>(chunks[, dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clamp" title="hxtorch.spiking.modules.synapse.Parameter.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp</span></code></a>([min, max])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_" title="hxtorch.spiking.modules.synapse.Parameter.clamp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp_</span></code></a>([min, max])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_max" title="hxtorch.spiking.modules.synapse.Parameter.clamp_max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp_max</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_max_" title="hxtorch.spiking.modules.synapse.Parameter.clamp_max_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp_max_</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_min" title="hxtorch.spiking.modules.synapse.Parameter.clamp_min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp_min</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_min_" title="hxtorch.spiking.modules.synapse.Parameter.clamp_min_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp_min_</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clip" title="hxtorch.spiking.modules.synapse.Parameter.clip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip</span></code></a>([min, max])</p></td>
<td><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clip_" title="hxtorch.spiking.modules.synapse.Parameter.clip_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip_</span></code></a>([min, max])</p></td>
<td><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp_()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.clone" title="hxtorch.spiking.modules.synapse.Parameter.clone"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clone</span></code></a>(*[, memory_format])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clone()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.coalesce" title="hxtorch.spiking.modules.synapse.Parameter.coalesce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">coalesce</span></code></a>()</p></td>
<td><p>Returns a coalesced copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is an <span class="xref std std-ref">uncoalesced tensor</span>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.col_indices" title="hxtorch.spiking.modules.synapse.Parameter.col_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">col_indices</span></code></a>()</p></td>
<td><p>Returns the tensor containing the column indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.conj" title="hxtorch.spiking.modules.synapse.Parameter.conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.conj_physical" title="hxtorch.spiking.modules.synapse.Parameter.conj_physical"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj_physical</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj_physical()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.conj_physical_" title="hxtorch.spiking.modules.synapse.Parameter.conj_physical_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj_physical_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">conj_physical()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.contiguous" title="hxtorch.spiking.modules.synapse.Parameter.contiguous"><code class="xref py py-obj docutils literal notranslate"><span class="pre">contiguous</span></code></a>([memory_format])</p></td>
<td><p>Returns a contiguous in memory tensor containing the same data as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.copy_" title="hxtorch.spiking.modules.synapse.Parameter.copy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copy_</span></code></a>(src[, non_blocking])</p></td>
<td><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.copysign" title="hxtorch.spiking.modules.synapse.Parameter.copysign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copysign</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.copysign()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.copysign_" title="hxtorch.spiking.modules.synapse.Parameter.copysign_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copysign_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">copysign()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.corrcoef" title="hxtorch.spiking.modules.synapse.Parameter.corrcoef"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corrcoef</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.corrcoef()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cos" title="hxtorch.spiking.modules.synapse.Parameter.cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cos</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cos_" title="hxtorch.spiking.modules.synapse.Parameter.cos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cos_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cos()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cosh" title="hxtorch.spiking.modules.synapse.Parameter.cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cosh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cosh_" title="hxtorch.spiking.modules.synapse.Parameter.cosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cosh_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cosh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.count_nonzero" title="hxtorch.spiking.modules.synapse.Parameter.count_nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">count_nonzero</span></code></a>([dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.count_nonzero()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cov" title="hxtorch.spiking.modules.synapse.Parameter.cov"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cov</span></code></a>(*[, correction, fweights, aweights])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cov()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cpu" title="hxtorch.spiking.modules.synapse.Parameter.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>([memory_format])</p></td>
<td><p>Returns a copy of this object in CPU memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cross" title="hxtorch.spiking.modules.synapse.Parameter.cross"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cross</span></code></a>(other[, dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cross()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.crow_indices" title="hxtorch.spiking.modules.synapse.Parameter.crow_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">crow_indices</span></code></a>()</p></td>
<td><p>Returns the tensor containing the compressed row indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cuda" title="hxtorch.spiking.modules.synapse.Parameter.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device, non_blocking, memory_format])</p></td>
<td><p>Returns a copy of this object in CUDA memory.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cummax" title="hxtorch.spiking.modules.synapse.Parameter.cummax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cummax</span></code></a>(dim)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummax()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cummin" title="hxtorch.spiking.modules.synapse.Parameter.cummin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cummin</span></code></a>(dim)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummin()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cumprod" title="hxtorch.spiking.modules.synapse.Parameter.cumprod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumprod</span></code></a>(dim[, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumprod()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cumprod_" title="hxtorch.spiking.modules.synapse.Parameter.cumprod_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumprod_</span></code></a>(dim[, dtype])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cumprod()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cumsum" title="hxtorch.spiking.modules.synapse.Parameter.cumsum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumsum</span></code></a>(dim[, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumsum()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.cumsum_" title="hxtorch.spiking.modules.synapse.Parameter.cumsum_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumsum_</span></code></a>(dim[, dtype])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cumsum()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.data_ptr" title="hxtorch.spiking.modules.synapse.Parameter.data_ptr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">data_ptr</span></code></a>()</p></td>
<td><p>Returns the address of the first element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.deg2rad" title="hxtorch.spiking.modules.synapse.Parameter.deg2rad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deg2rad</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.deg2rad()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.deg2rad_" title="hxtorch.spiking.modules.synapse.Parameter.deg2rad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deg2rad_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">deg2rad()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dense_dim" title="hxtorch.spiking.modules.synapse.Parameter.dense_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dense_dim</span></code></a>()</p></td>
<td><p>Return the number of dense dimensions in a <span class="xref std std-ref">sparse tensor</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dequantize" title="hxtorch.spiking.modules.synapse.Parameter.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a>()</p></td>
<td><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.det" title="hxtorch.spiking.modules.synapse.Parameter.det"><code class="xref py py-obj docutils literal notranslate"><span class="pre">det</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.det()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.detach" title="hxtorch.spiking.modules.synapse.Parameter.detach"><code class="xref py py-obj docutils literal notranslate"><span class="pre">detach</span></code></a></p></td>
<td><p>Returns a new Tensor, detached from the current graph.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.detach_" title="hxtorch.spiking.modules.synapse.Parameter.detach_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">detach_</span></code></a></p></td>
<td><p>Detaches the Tensor from the graph that created it, making it a leaf.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.diag" title="hxtorch.spiking.modules.synapse.Parameter.diag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diag</span></code></a>([diagonal])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.diag_embed" title="hxtorch.spiking.modules.synapse.Parameter.diag_embed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diag_embed</span></code></a>([offset, dim1, dim2])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.diagflat" title="hxtorch.spiking.modules.synapse.Parameter.diagflat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagflat</span></code></a>([offset])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.diagonal" title="hxtorch.spiking.modules.synapse.Parameter.diagonal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagonal</span></code></a>([offset, dim1, dim2])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.diagonal_scatter" title="hxtorch.spiking.modules.synapse.Parameter.diagonal_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagonal_scatter</span></code></a>(src[, offset, dim1, dim2])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal_scatter()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.diff" title="hxtorch.spiking.modules.synapse.Parameter.diff"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diff</span></code></a>([n, dim, prepend, append])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diff()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.digamma" title="hxtorch.spiking.modules.synapse.Parameter.digamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">digamma</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.digamma()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.digamma_" title="hxtorch.spiking.modules.synapse.Parameter.digamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">digamma_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">digamma()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dim" title="hxtorch.spiking.modules.synapse.Parameter.dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dim</span></code></a>()</p></td>
<td><p>Returns the number of dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dist" title="hxtorch.spiking.modules.synapse.Parameter.dist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist</span></code></a>(other[, p])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dist()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.div" title="hxtorch.spiking.modules.synapse.Parameter.div"><code class="xref py py-obj docutils literal notranslate"><span class="pre">div</span></code></a>(value, *[, rounding_mode])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.div_" title="hxtorch.spiking.modules.synapse.Parameter.div_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">div_</span></code></a>(value, *[, rounding_mode])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.divide" title="hxtorch.spiking.modules.synapse.Parameter.divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">divide</span></code></a>(value, *[, rounding_mode])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.divide()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.divide_" title="hxtorch.spiking.modules.synapse.Parameter.divide_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">divide_</span></code></a>(value, *[, rounding_mode])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">divide()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dot" title="hxtorch.spiking.modules.synapse.Parameter.dot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dot</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dot()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.double" title="hxtorch.spiking.modules.synapse.Parameter.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.double()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float64)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dsplit" title="hxtorch.spiking.modules.synapse.Parameter.dsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dsplit</span></code></a>(split_size_or_sections)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dsplit()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.eig" title="hxtorch.spiking.modules.synapse.Parameter.eig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eig</span></code></a>([eigenvectors])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.element_size" title="hxtorch.spiking.modules.synapse.Parameter.element_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">element_size</span></code></a>()</p></td>
<td><p>Returns the size in bytes of an individual element.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.eq" title="hxtorch.spiking.modules.synapse.Parameter.eq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.eq_" title="hxtorch.spiking.modules.synapse.Parameter.eq_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.equal" title="hxtorch.spiking.modules.synapse.Parameter.equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">equal</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.erf" title="hxtorch.spiking.modules.synapse.Parameter.erf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erf</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.erf_" title="hxtorch.spiking.modules.synapse.Parameter.erf_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erf_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">erf()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.erfc" title="hxtorch.spiking.modules.synapse.Parameter.erfc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfc</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.erfc_" title="hxtorch.spiking.modules.synapse.Parameter.erfc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfc_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">erfc()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.erfinv" title="hxtorch.spiking.modules.synapse.Parameter.erfinv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfinv</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfinv()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.erfinv_" title="hxtorch.spiking.modules.synapse.Parameter.erfinv_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfinv_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">erfinv()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.exp" title="hxtorch.spiking.modules.synapse.Parameter.exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.exp2" title="hxtorch.spiking.modules.synapse.Parameter.exp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp2</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp2()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.exp2_" title="hxtorch.spiking.modules.synapse.Parameter.exp2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp2_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">exp2()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.exp_" title="hxtorch.spiking.modules.synapse.Parameter.exp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">exp()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.expand" title="hxtorch.spiking.modules.synapse.Parameter.expand"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expand</span></code></a>(*sizes)</p></td>
<td><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded to a larger size.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.expand_as" title="hxtorch.spiking.modules.synapse.Parameter.expand_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expand_as</span></code></a>(other)</p></td>
<td><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.expm1" title="hxtorch.spiking.modules.synapse.Parameter.expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expm1</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.expm1_" title="hxtorch.spiking.modules.synapse.Parameter.expm1_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expm1_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">expm1()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.exponential_" title="hxtorch.spiking.modules.synapse.Parameter.exponential_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exponential_</span></code></a>([lambd, generator])</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the exponential distribution:</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fill_" title="hxtorch.spiking.modules.synapse.Parameter.fill_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fill_</span></code></a>(value)</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with the specified value.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fill_diagonal_" title="hxtorch.spiking.modules.synapse.Parameter.fill_diagonal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fill_diagonal_</span></code></a>(fill_value[, wrap])</p></td>
<td><p>Fill the main diagonal of a tensor that has at least 2-dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fix" title="hxtorch.spiking.modules.synapse.Parameter.fix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fix</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fix()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fix_" title="hxtorch.spiking.modules.synapse.Parameter.fix_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fix_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">fix()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.flatten" title="hxtorch.spiking.modules.synapse.Parameter.flatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flatten</span></code></a>([start_dim, end_dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.flip" title="hxtorch.spiking.modules.synapse.Parameter.flip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flip</span></code></a>(dims)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flip()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fliplr" title="hxtorch.spiking.modules.synapse.Parameter.fliplr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fliplr</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fliplr()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.flipud" title="hxtorch.spiking.modules.synapse.Parameter.flipud"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flipud</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flipud()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.float" title="hxtorch.spiking.modules.synapse.Parameter.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.float()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float32)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.float_power" title="hxtorch.spiking.modules.synapse.Parameter.float_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float_power</span></code></a>(exponent)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.float_power()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.float_power_" title="hxtorch.spiking.modules.synapse.Parameter.float_power_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float_power_</span></code></a>(exponent)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">float_power()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.floor" title="hxtorch.spiking.modules.synapse.Parameter.floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.floor_" title="hxtorch.spiking.modules.synapse.Parameter.floor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">floor()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.floor_divide" title="hxtorch.spiking.modules.synapse.Parameter.floor_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor_divide</span></code></a>(value)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor_divide()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.floor_divide_" title="hxtorch.spiking.modules.synapse.Parameter.floor_divide_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor_divide_</span></code></a>(value)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">floor_divide()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fmax" title="hxtorch.spiking.modules.synapse.Parameter.fmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmax</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmax()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fmin" title="hxtorch.spiking.modules.synapse.Parameter.fmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmin</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmin()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fmod" title="hxtorch.spiking.modules.synapse.Parameter.fmod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmod</span></code></a>(divisor)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.fmod_" title="hxtorch.spiking.modules.synapse.Parameter.fmod_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmod_</span></code></a>(divisor)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">fmod()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.frac" title="hxtorch.spiking.modules.synapse.Parameter.frac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frac</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.frac_" title="hxtorch.spiking.modules.synapse.Parameter.frac_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frac_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">frac()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.frexp" title="hxtorch.spiking.modules.synapse.Parameter.frexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frexp</span></code></a>(input)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frexp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.gather" title="hxtorch.spiking.modules.synapse.Parameter.gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gather</span></code></a>(dim, index)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gather()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.gcd" title="hxtorch.spiking.modules.synapse.Parameter.gcd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gcd</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gcd()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.gcd_" title="hxtorch.spiking.modules.synapse.Parameter.gcd_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gcd_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">gcd()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ge" title="hxtorch.spiking.modules.synapse.Parameter.ge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ge_" title="hxtorch.spiking.modules.synapse.Parameter.ge_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.geometric_" title="hxtorch.spiking.modules.synapse.Parameter.geometric_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">geometric_</span></code></a>(p, *[, generator])</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the geometric distribution:</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.geqrf" title="hxtorch.spiking.modules.synapse.Parameter.geqrf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">geqrf</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ger" title="hxtorch.spiking.modules.synapse.Parameter.ger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ger</span></code></a>(vec2)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ger()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.get_device" title="hxtorch.spiking.modules.synapse.Parameter.get_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_device</span></code></a>()</p></td>
<td><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.greater" title="hxtorch.spiking.modules.synapse.Parameter.greater"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.greater_" title="hxtorch.spiking.modules.synapse.Parameter.greater_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">greater()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.greater_equal" title="hxtorch.spiking.modules.synapse.Parameter.greater_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_equal</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater_equal()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.greater_equal_" title="hxtorch.spiking.modules.synapse.Parameter.greater_equal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_equal_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">greater_equal()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.gt" title="hxtorch.spiking.modules.synapse.Parameter.gt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.gt_" title="hxtorch.spiking.modules.synapse.Parameter.gt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.half" title="hxtorch.spiking.modules.synapse.Parameter.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.half()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float16)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.hardshrink" title="hxtorch.spiking.modules.synapse.Parameter.hardshrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardshrink</span></code></a>([lambd])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.hardshrink()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.has_names" title="hxtorch.spiking.modules.synapse.Parameter.has_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">has_names</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if any of this tensor’s dimensions are named.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.heaviside" title="hxtorch.spiking.modules.synapse.Parameter.heaviside"><code class="xref py py-obj docutils literal notranslate"><span class="pre">heaviside</span></code></a>(values)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.heaviside()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.heaviside_" title="hxtorch.spiking.modules.synapse.Parameter.heaviside_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">heaviside_</span></code></a>(values)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">heaviside()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.histc" title="hxtorch.spiking.modules.synapse.Parameter.histc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histc</span></code></a>([bins, min, max])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histc()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.histogram" title="hxtorch.spiking.modules.synapse.Parameter.histogram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histogram</span></code></a>(input, bins, *[, range, weight, …])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histogram()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.hsplit" title="hxtorch.spiking.modules.synapse.Parameter.hsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hsplit</span></code></a>(split_size_or_sections)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hsplit()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.hypot" title="hxtorch.spiking.modules.synapse.Parameter.hypot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hypot</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hypot()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.hypot_" title="hxtorch.spiking.modules.synapse.Parameter.hypot_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hypot_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">hypot()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.i0" title="hxtorch.spiking.modules.synapse.Parameter.i0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">i0</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.i0()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.i0_" title="hxtorch.spiking.modules.synapse.Parameter.i0_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">i0_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">i0()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.igamma" title="hxtorch.spiking.modules.synapse.Parameter.igamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igamma</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igamma()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.igamma_" title="hxtorch.spiking.modules.synapse.Parameter.igamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igamma_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">igamma()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.igammac" title="hxtorch.spiking.modules.synapse.Parameter.igammac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igammac</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igammac()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.igammac_" title="hxtorch.spiking.modules.synapse.Parameter.igammac_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igammac_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">igammac()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_add" title="hxtorch.spiking.modules.synapse.Parameter.index_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_add</span></code></a>(dim, index, source, *[, alpha])</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_add_()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_add_" title="hxtorch.spiking.modules.synapse.Parameter.index_add_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_add_</span></code></a>(dim, index, source, *[, alpha])</p></td>
<td><p>Accumulate the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> times <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by adding to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_copy" title="hxtorch.spiking.modules.synapse.Parameter.index_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_copy</span></code></a>(dim, index, tensor2)</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_copy_()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_copy_" title="hxtorch.spiking.modules.synapse.Parameter.index_copy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_copy_</span></code></a>(dim, index, tensor)</p></td>
<td><p>Copies the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_fill" title="hxtorch.spiking.modules.synapse.Parameter.index_fill"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_fill</span></code></a>(dim, index, value)</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_fill_()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_fill_" title="hxtorch.spiking.modules.synapse.Parameter.index_fill_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_fill_</span></code></a>(dim, index, value)</p></td>
<td><p>Fills the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with value <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> by selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_put" title="hxtorch.spiking.modules.synapse.Parameter.index_put"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_put</span></code></a>(indices, values[, accumulate])</p></td>
<td><p>Out-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">index_put_()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_put_" title="hxtorch.spiking.modules.synapse.Parameter.index_put_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_put_</span></code></a>(indices, values[, accumulate])</p></td>
<td><p>Puts values from the tensor <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.values" title="hxtorch.spiking.modules.synapse.Parameter.values"><code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code></a> into the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> using the indices specified in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.indices" title="hxtorch.spiking.modules.synapse.Parameter.indices"><code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code></a> (which is a tuple of Tensors).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_reduce" title="hxtorch.spiking.modules.synapse.Parameter.index_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_reduce</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_reduce_" title="hxtorch.spiking.modules.synapse.Parameter.index_reduce_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_reduce_</span></code></a>(dim, index, source, reduce, *)</p></td>
<td><p>Accumulate the elements of <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by accumulating to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> using the reduction given by the <code class="docutils literal notranslate"><span class="pre">reduce</span></code> argument.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.index_select" title="hxtorch.spiking.modules.synapse.Parameter.index_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_select</span></code></a>(dim, index)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.index_select()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.indices" title="hxtorch.spiking.modules.synapse.Parameter.indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">indices</span></code></a>()</p></td>
<td><p>Return the indices tensor of a <span class="xref std std-ref">sparse COO tensor</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.inner" title="hxtorch.spiking.modules.synapse.Parameter.inner"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inner</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inner()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.int()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int32)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int_repr" title="hxtorch.spiking.modules.synapse.Parameter.int_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int_repr</span></code></a>()</p></td>
<td><p>Given a quantized Tensor, <code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.inverse" title="hxtorch.spiking.modules.synapse.Parameter.inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inverse()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ipu" title="hxtorch.spiking.modules.synapse.Parameter.ipu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ipu</span></code></a>([device, non_blocking, memory_format])</p></td>
<td><p>Returns a copy of this object in IPU memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_coalesced" title="hxtorch.spiking.modules.synapse.Parameter.is_coalesced"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_coalesced</span></code></a>()</p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a <span class="xref std std-ref">sparse COO tensor</span> that is coalesced, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_complex" title="hxtorch.spiking.modules.synapse.Parameter.is_complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_complex</span></code></a>()</p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a complex data type.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_conj" title="hxtorch.spiking.modules.synapse.Parameter.is_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_conj</span></code></a>()</p></td>
<td><p>Returns True if the conjugate bit of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is set to true.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_contiguous" title="hxtorch.spiking.modules.synapse.Parameter.is_contiguous"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_contiguous</span></code></a>([memory_format])</p></td>
<td><p>Returns True if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous in memory in the order specified by memory format.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_distributed" title="hxtorch.spiking.modules.synapse.Parameter.is_distributed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_distributed</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_floating_point" title="hxtorch.spiking.modules.synapse.Parameter.is_floating_point"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_floating_point</span></code></a>()</p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a floating point data type.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_inference" title="hxtorch.spiking.modules.synapse.Parameter.is_inference"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_inference</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.is_inference()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_neg" title="hxtorch.spiking.modules.synapse.Parameter.is_neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_neg</span></code></a>()</p></td>
<td><p>Returns True if the negative bit of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is set to true.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_nonzero" title="hxtorch.spiking.modules.synapse.Parameter.is_nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_nonzero</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_pinned" title="hxtorch.spiking.modules.synapse.Parameter.is_pinned"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_pinned</span></code></a></p></td>
<td><p>Returns true if this tensor resides in pinned memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_same_size" title="hxtorch.spiking.modules.synapse.Parameter.is_same_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_same_size</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_set_to" title="hxtorch.spiking.modules.synapse.Parameter.is_set_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_set_to</span></code></a>(tensor)</p></td>
<td><p>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_shared" title="hxtorch.spiking.modules.synapse.Parameter.is_shared"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_shared</span></code></a>()</p></td>
<td><p>Checks if tensor is in shared memory.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_signed" title="hxtorch.spiking.modules.synapse.Parameter.is_signed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_signed</span></code></a>()</p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a signed data type.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.isclose" title="hxtorch.spiking.modules.synapse.Parameter.isclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isclose</span></code></a>(other[, rtol, atol, equal_nan])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isclose()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.isfinite" title="hxtorch.spiking.modules.synapse.Parameter.isfinite"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isfinite</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isfinite()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.isinf" title="hxtorch.spiking.modules.synapse.Parameter.isinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isinf</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isinf()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.isnan" title="hxtorch.spiking.modules.synapse.Parameter.isnan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isnan</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isnan()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.isneginf" title="hxtorch.spiking.modules.synapse.Parameter.isneginf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isneginf</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isneginf()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.isposinf" title="hxtorch.spiking.modules.synapse.Parameter.isposinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isposinf</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isposinf()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.isreal" title="hxtorch.spiking.modules.synapse.Parameter.isreal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isreal</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isreal()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.istft" title="hxtorch.spiking.modules.synapse.Parameter.istft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">istft</span></code></a>(n_fft[, hop_length, win_length, …])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.istft()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.item" title="hxtorch.spiking.modules.synapse.Parameter.item"><code class="xref py py-obj docutils literal notranslate"><span class="pre">item</span></code></a>()</p></td>
<td><p>Returns the value of this tensor as a standard Python number.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.kron" title="hxtorch.spiking.modules.synapse.Parameter.kron"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kron</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kron()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.kthvalue" title="hxtorch.spiking.modules.synapse.Parameter.kthvalue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kthvalue</span></code></a>(k[, dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kthvalue()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lcm" title="hxtorch.spiking.modules.synapse.Parameter.lcm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lcm</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lcm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lcm_" title="hxtorch.spiking.modules.synapse.Parameter.lcm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lcm_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lcm()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ldexp" title="hxtorch.spiking.modules.synapse.Parameter.ldexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ldexp</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ldexp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ldexp_" title="hxtorch.spiking.modules.synapse.Parameter.ldexp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ldexp_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ldexp()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.le" title="hxtorch.spiking.modules.synapse.Parameter.le"><code class="xref py py-obj docutils literal notranslate"><span class="pre">le</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.le_" title="hxtorch.spiking.modules.synapse.Parameter.le_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">le_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lerp" title="hxtorch.spiking.modules.synapse.Parameter.lerp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lerp</span></code></a>(end, weight)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lerp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lerp_" title="hxtorch.spiking.modules.synapse.Parameter.lerp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lerp_</span></code></a>(end, weight)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lerp()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.less" title="hxtorch.spiking.modules.synapse.Parameter.less"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less</span></code></a></p></td>
<td><p>lt(other) -&gt; Tensor</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.less_" title="hxtorch.spiking.modules.synapse.Parameter.less_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">less()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.less_equal" title="hxtorch.spiking.modules.synapse.Parameter.less_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less_equal</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.less_equal()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.less_equal_" title="hxtorch.spiking.modules.synapse.Parameter.less_equal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less_equal_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">less_equal()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lgamma" title="hxtorch.spiking.modules.synapse.Parameter.lgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lgamma</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lgamma_" title="hxtorch.spiking.modules.synapse.Parameter.lgamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lgamma_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lgamma()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log" title="hxtorch.spiking.modules.synapse.Parameter.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log10" title="hxtorch.spiking.modules.synapse.Parameter.log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log10</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log10_" title="hxtorch.spiking.modules.synapse.Parameter.log10_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log10_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log10()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log1p" title="hxtorch.spiking.modules.synapse.Parameter.log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log1p</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log1p_" title="hxtorch.spiking.modules.synapse.Parameter.log1p_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log1p_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log2" title="hxtorch.spiking.modules.synapse.Parameter.log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log2</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log2_" title="hxtorch.spiking.modules.synapse.Parameter.log2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log2_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log2()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log_" title="hxtorch.spiking.modules.synapse.Parameter.log_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log_normal_" title="hxtorch.spiking.modules.synapse.Parameter.log_normal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_normal_</span></code></a>([mean, std, generator])</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers samples from the log-normal distribution parameterized by the given mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.log_softmax" title="hxtorch.spiking.modules.synapse.Parameter.log_softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_softmax</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logaddexp" title="hxtorch.spiking.modules.synapse.Parameter.logaddexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logaddexp2" title="hxtorch.spiking.modules.synapse.Parameter.logaddexp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp2</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp2()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logcumsumexp" title="hxtorch.spiking.modules.synapse.Parameter.logcumsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logcumsumexp</span></code></a>(dim)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logcumsumexp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logdet" title="hxtorch.spiking.modules.synapse.Parameter.logdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logdet</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logdet()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_and" title="hxtorch.spiking.modules.synapse.Parameter.logical_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_and</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_and()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_and_" title="hxtorch.spiking.modules.synapse.Parameter.logical_and_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_and_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_and()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_not" title="hxtorch.spiking.modules.synapse.Parameter.logical_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_not</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_not()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_not_" title="hxtorch.spiking.modules.synapse.Parameter.logical_not_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_not_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_not()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_or" title="hxtorch.spiking.modules.synapse.Parameter.logical_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_or</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_or()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_or_" title="hxtorch.spiking.modules.synapse.Parameter.logical_or_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_or_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_or()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_xor" title="hxtorch.spiking.modules.synapse.Parameter.logical_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_xor</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_xor()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logical_xor_" title="hxtorch.spiking.modules.synapse.Parameter.logical_xor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_xor_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_xor()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logit" title="hxtorch.spiking.modules.synapse.Parameter.logit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logit</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logit()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logit_" title="hxtorch.spiking.modules.synapse.Parameter.logit_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logit_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logit()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.logsumexp" title="hxtorch.spiking.modules.synapse.Parameter.logsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logsumexp</span></code></a>(dim[, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logsumexp()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.long" title="hxtorch.spiking.modules.synapse.Parameter.long"><code class="xref py py-obj docutils literal notranslate"><span class="pre">long</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.long()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int64)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lstsq" title="hxtorch.spiking.modules.synapse.Parameter.lstsq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lstsq</span></code></a>(other)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lt" title="hxtorch.spiking.modules.synapse.Parameter.lt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lt_" title="hxtorch.spiking.modules.synapse.Parameter.lt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lu" title="hxtorch.spiking.modules.synapse.Parameter.lu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu</span></code></a>([pivot, get_infos])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.lu_solve" title="hxtorch.spiking.modules.synapse.Parameter.lu_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu_solve</span></code></a>(LU_data, LU_pivots)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu_solve()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.map2_" title="hxtorch.spiking.modules.synapse.Parameter.map2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">map2_</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.map_" title="hxtorch.spiking.modules.synapse.Parameter.map_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">map_</span></code></a>(tensor, callable)</p></td>
<td><p>Applies <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> and stores the results in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.masked_fill" title="hxtorch.spiking.modules.synapse.Parameter.masked_fill"><code class="xref py py-obj docutils literal notranslate"><span class="pre">masked_fill</span></code></a>(mask, value)</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_fill_()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.masked_fill_" title="hxtorch.spiking.modules.synapse.Parameter.masked_fill_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">masked_fill_</span></code></a>(mask, value)</p></td>
<td><p>Fills elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is True.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.masked_scatter" title="hxtorch.spiking.modules.synapse.Parameter.masked_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">masked_scatter</span></code></a>(mask, tensor)</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_scatter_()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.masked_scatter_" title="hxtorch.spiking.modules.synapse.Parameter.masked_scatter_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">masked_scatter_</span></code></a>(mask, source)</p></td>
<td><p>Copies elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor at positions where the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is True.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.masked_select" title="hxtorch.spiking.modules.synapse.Parameter.masked_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">masked_select</span></code></a>(mask)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.masked_select()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.matmul" title="hxtorch.spiking.modules.synapse.Parameter.matmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matmul</span></code></a>(tensor2)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.matrix_exp" title="hxtorch.spiking.modules.synapse.Parameter.matrix_exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matrix_exp</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matrix_exp()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.matrix_power" title="hxtorch.spiking.modules.synapse.Parameter.matrix_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matrix_power</span></code></a>(n)</p></td>
<td><p><div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">matrix_power()</span></code> is deprecated, use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_power()</span></code> instead.</p>
</div>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.max" title="hxtorch.spiking.modules.synapse.Parameter.max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.maximum" title="hxtorch.spiking.modules.synapse.Parameter.maximum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maximum</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.maximum()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mean" title="hxtorch.spiking.modules.synapse.Parameter.mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></a>([dim, keepdim, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.median" title="hxtorch.spiking.modules.synapse.Parameter.median"><code class="xref py py-obj docutils literal notranslate"><span class="pre">median</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.median()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.min" title="hxtorch.spiking.modules.synapse.Parameter.min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">min</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.minimum" title="hxtorch.spiking.modules.synapse.Parameter.minimum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">minimum</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.minimum()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mm" title="hxtorch.spiking.modules.synapse.Parameter.mm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mm</span></code></a>(mat2)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mode" title="hxtorch.spiking.modules.synapse.Parameter.mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mode</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mode()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.moveaxis" title="hxtorch.spiking.modules.synapse.Parameter.moveaxis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">moveaxis</span></code></a>(source, destination)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.moveaxis()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.movedim" title="hxtorch.spiking.modules.synapse.Parameter.movedim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">movedim</span></code></a>(source, destination)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.movedim()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.msort" title="hxtorch.spiking.modules.synapse.Parameter.msort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">msort</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.msort()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mul" title="hxtorch.spiking.modules.synapse.Parameter.mul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mul</span></code></a>(value)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mul_" title="hxtorch.spiking.modules.synapse.Parameter.mul_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mul_</span></code></a>(value)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.multinomial" title="hxtorch.spiking.modules.synapse.Parameter.multinomial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multinomial</span></code></a>(num_samples[, replacement, …])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.multiply" title="hxtorch.spiking.modules.synapse.Parameter.multiply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiply</span></code></a>(value)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multiply()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.multiply_" title="hxtorch.spiking.modules.synapse.Parameter.multiply_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiply_</span></code></a>(value)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">multiply()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mv" title="hxtorch.spiking.modules.synapse.Parameter.mv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mv</span></code></a>(vec)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mvlgamma" title="hxtorch.spiking.modules.synapse.Parameter.mvlgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mvlgamma</span></code></a>(p)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mvlgamma()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mvlgamma_" title="hxtorch.spiking.modules.synapse.Parameter.mvlgamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mvlgamma_</span></code></a>(p)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">mvlgamma()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nan_to_num" title="hxtorch.spiking.modules.synapse.Parameter.nan_to_num"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nan_to_num</span></code></a>([nan, posinf, neginf])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nan_to_num()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nan_to_num_" title="hxtorch.spiking.modules.synapse.Parameter.nan_to_num_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nan_to_num_</span></code></a>([nan, posinf, neginf])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">nan_to_num()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nanmean" title="hxtorch.spiking.modules.synapse.Parameter.nanmean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanmean</span></code></a>([dim, keepdim, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmean()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nanmedian" title="hxtorch.spiking.modules.synapse.Parameter.nanmedian"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanmedian</span></code></a>([dim, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmedian()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nanquantile" title="hxtorch.spiking.modules.synapse.Parameter.nanquantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanquantile</span></code></a>(q[, dim, keepdim, interpolation])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanquantile()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nansum" title="hxtorch.spiking.modules.synapse.Parameter.nansum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nansum</span></code></a>([dim, keepdim, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nansum()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.narrow" title="hxtorch.spiking.modules.synapse.Parameter.narrow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">narrow</span></code></a>(dimension, start, length)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.narrow_copy" title="hxtorch.spiking.modules.synapse.Parameter.narrow_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">narrow_copy</span></code></a>(dimension, start, length)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow_copy()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ndimension" title="hxtorch.spiking.modules.synapse.Parameter.ndimension"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ndimension</span></code></a>()</p></td>
<td><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ne" title="hxtorch.spiking.modules.synapse.Parameter.ne"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ne_" title="hxtorch.spiking.modules.synapse.Parameter.ne_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.neg" title="hxtorch.spiking.modules.synapse.Parameter.neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">neg</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.neg_" title="hxtorch.spiking.modules.synapse.Parameter.neg_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">neg_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.negative" title="hxtorch.spiking.modules.synapse.Parameter.negative"><code class="xref py py-obj docutils literal notranslate"><span class="pre">negative</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.negative()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.negative_" title="hxtorch.spiking.modules.synapse.Parameter.negative_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">negative_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">negative()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nelement" title="hxtorch.spiking.modules.synapse.Parameter.nelement"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nelement</span></code></a>()</p></td>
<td><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new" title="hxtorch.spiking.modules.synapse.Parameter.new"><code class="xref py py-obj docutils literal notranslate"><span class="pre">new</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_empty" title="hxtorch.spiking.modules.synapse.Parameter.new_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">new_empty</span></code></a>(size, *[, dtype, device, …])</p></td>
<td><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with uninitialized data.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_empty_strided" title="hxtorch.spiking.modules.synapse.Parameter.new_empty_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">new_empty_strided</span></code></a>(size, stride[, dtype, …])</p></td>
<td><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> and strides <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.stride" title="hxtorch.spiking.modules.synapse.Parameter.stride"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code></a> filled with uninitialized data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_full" title="hxtorch.spiking.modules.synapse.Parameter.new_full"><code class="xref py py-obj docutils literal notranslate"><span class="pre">new_full</span></code></a>(size, fill_value, *[, dtype, …])</p></td>
<td><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_ones" title="hxtorch.spiking.modules.synapse.Parameter.new_ones"><code class="xref py py-obj docutils literal notranslate"><span class="pre">new_ones</span></code></a>(size, *[, dtype, device, …])</p></td>
<td><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_tensor" title="hxtorch.spiking.modules.synapse.Parameter.new_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">new_tensor</span></code></a>(data, *[, dtype, device, …])</p></td>
<td><p>Returns a new Tensor with <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.data" title="hxtorch.spiking.modules.synapse.Parameter.data"><code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code></a> as the tensor data.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_zeros" title="hxtorch.spiking.modules.synapse.Parameter.new_zeros"><code class="xref py py-obj docutils literal notranslate"><span class="pre">new_zeros</span></code></a>(size, *[, dtype, device, …])</p></td>
<td><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nextafter" title="hxtorch.spiking.modules.synapse.Parameter.nextafter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nextafter</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nextafter()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nextafter_" title="hxtorch.spiking.modules.synapse.Parameter.nextafter_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nextafter_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">nextafter()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.nonzero" title="hxtorch.spiking.modules.synapse.Parameter.nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nonzero</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nonzero()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.norm" title="hxtorch.spiking.modules.synapse.Parameter.norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code></a>([p, dim, keepdim, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.normal_" title="hxtorch.spiking.modules.synapse.Parameter.normal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">normal_</span></code></a>([mean, std, generator])</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements samples from the normal distribution parameterized by <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mean" title="hxtorch.spiking.modules.synapse.Parameter.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.std" title="hxtorch.spiking.modules.synapse.Parameter.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.not_equal" title="hxtorch.spiking.modules.synapse.Parameter.not_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">not_equal</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.not_equal()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.not_equal_" title="hxtorch.spiking.modules.synapse.Parameter.not_equal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">not_equal_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">not_equal()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.numel" title="hxtorch.spiking.modules.synapse.Parameter.numel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numel</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.numel()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.numpy" title="hxtorch.spiking.modules.synapse.Parameter.numpy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy</span></code></a>(*[, force])</p></td>
<td><p>Returns the tensor as a NumPy <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.orgqr" title="hxtorch.spiking.modules.synapse.Parameter.orgqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orgqr</span></code></a>(input2)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.orgqr()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ormqr" title="hxtorch.spiking.modules.synapse.Parameter.ormqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ormqr</span></code></a>(input2, input3[, left, transpose])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ormqr()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.outer" title="hxtorch.spiking.modules.synapse.Parameter.outer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">outer</span></code></a>(vec2)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.outer()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.permute" title="hxtorch.spiking.modules.synapse.Parameter.permute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">permute</span></code></a>(*dims)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.permute()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.pin_memory" title="hxtorch.spiking.modules.synapse.Parameter.pin_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pin_memory</span></code></a>()</p></td>
<td><p>Copies the tensor to pinned memory, if it’s not already pinned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.pinverse" title="hxtorch.spiking.modules.synapse.Parameter.pinverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pinverse</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pinverse()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.polygamma" title="hxtorch.spiking.modules.synapse.Parameter.polygamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">polygamma</span></code></a>(n)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.polygamma()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.polygamma_" title="hxtorch.spiking.modules.synapse.Parameter.polygamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">polygamma_</span></code></a>(n)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">polygamma()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.positive" title="hxtorch.spiking.modules.synapse.Parameter.positive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">positive</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.positive()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.pow" title="hxtorch.spiking.modules.synapse.Parameter.pow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pow</span></code></a>(exponent)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.pow_" title="hxtorch.spiking.modules.synapse.Parameter.pow_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pow_</span></code></a>(exponent)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.prelu" title="hxtorch.spiking.modules.synapse.Parameter.prelu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prelu</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.prod" title="hxtorch.spiking.modules.synapse.Parameter.prod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prod</span></code></a>([dim, keepdim, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.prod()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.put" title="hxtorch.spiking.modules.synapse.Parameter.put"><code class="xref py py-obj docutils literal notranslate"><span class="pre">put</span></code></a>(input, index, source[, accumulate])</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.put_()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.put_" title="hxtorch.spiking.modules.synapse.Parameter.put_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">put_</span></code></a>(index, source[, accumulate])</p></td>
<td><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into the positions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.q_per_channel_axis" title="hxtorch.spiking.modules.synapse.Parameter.q_per_channel_axis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_axis</span></code></a>()</p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.q_per_channel_scales" title="hxtorch.spiking.modules.synapse.Parameter.q_per_channel_scales"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_scales</span></code></a>()</p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.q_per_channel_zero_points" title="hxtorch.spiking.modules.synapse.Parameter.q_per_channel_zero_points"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_zero_points</span></code></a>()</p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.q_scale" title="hxtorch.spiking.modules.synapse.Parameter.q_scale"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_scale</span></code></a>()</p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.q_zero_point" title="hxtorch.spiking.modules.synapse.Parameter.q_zero_point"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_zero_point</span></code></a>()</p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.qr" title="hxtorch.spiking.modules.synapse.Parameter.qr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">qr</span></code></a>([some])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.qscheme" title="hxtorch.spiking.modules.synapse.Parameter.qscheme"><code class="xref py py-obj docutils literal notranslate"><span class="pre">qscheme</span></code></a>()</p></td>
<td><p>Returns the quantization scheme of a given QTensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.quantile" title="hxtorch.spiking.modules.synapse.Parameter.quantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantile</span></code></a>(q[, dim, keepdim, interpolation])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantile()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.rad2deg" title="hxtorch.spiking.modules.synapse.Parameter.rad2deg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rad2deg</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rad2deg()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.rad2deg_" title="hxtorch.spiking.modules.synapse.Parameter.rad2deg_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rad2deg_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">rad2deg()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.random_" title="hxtorch.spiking.modules.synapse.Parameter.random_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">random_</span></code></a>([from, to, generator])</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the discrete uniform distribution over <code class="docutils literal notranslate"><span class="pre">[from,</span> <span class="pre">to</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ravel" title="hxtorch.spiking.modules.synapse.Parameter.ravel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ravel</span></code></a>()</p></td>
<td><p>see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ravel()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.reciprocal" title="hxtorch.spiking.modules.synapse.Parameter.reciprocal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reciprocal</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.reciprocal_" title="hxtorch.spiking.modules.synapse.Parameter.reciprocal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reciprocal_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">reciprocal()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.record_stream" title="hxtorch.spiking.modules.synapse.Parameter.record_stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">record_stream</span></code></a>(stream)</p></td>
<td><p>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code class="xref py py-attr docutils literal notranslate"><span class="pre">stream</span></code> are complete.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.refine_names" title="hxtorch.spiking.modules.synapse.Parameter.refine_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">refine_names</span></code></a>(*names)</p></td>
<td><p>Refines the dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> according to <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.register_hook" title="hxtorch.spiking.modules.synapse.Parameter.register_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.reinforce" title="hxtorch.spiking.modules.synapse.Parameter.reinforce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reinforce</span></code></a>(reward)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.relu" title="hxtorch.spiking.modules.synapse.Parameter.relu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">relu</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.relu_" title="hxtorch.spiking.modules.synapse.Parameter.relu_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">relu_</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.remainder" title="hxtorch.spiking.modules.synapse.Parameter.remainder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">remainder</span></code></a>(divisor)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.remainder()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.remainder_" title="hxtorch.spiking.modules.synapse.Parameter.remainder_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">remainder_</span></code></a>(divisor)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">remainder()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.rename" title="hxtorch.spiking.modules.synapse.Parameter.rename"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rename</span></code></a>(*names, **rename_map)</p></td>
<td><p>Renames dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.rename_" title="hxtorch.spiking.modules.synapse.Parameter.rename_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rename_</span></code></a>(*names, **rename_map)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">rename()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.renorm" title="hxtorch.spiking.modules.synapse.Parameter.renorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">renorm</span></code></a>(p, dim, maxnorm)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.renorm()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.renorm_" title="hxtorch.spiking.modules.synapse.Parameter.renorm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">renorm_</span></code></a>(p, dim, maxnorm)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">renorm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.repeat" title="hxtorch.spiking.modules.synapse.Parameter.repeat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">repeat</span></code></a>(*sizes)</p></td>
<td><p>Repeats this tensor along the specified dimensions.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.repeat_interleave" title="hxtorch.spiking.modules.synapse.Parameter.repeat_interleave"><code class="xref py py-obj docutils literal notranslate"><span class="pre">repeat_interleave</span></code></a>(repeats[, dim, output_size])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad_" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on this tensor: sets this tensor’s <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> attribute in-place.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.reshape" title="hxtorch.spiking.modules.synapse.Parameter.reshape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reshape</span></code></a>(*shape)</p></td>
<td><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> but with the specified shape.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.reshape_as" title="hxtorch.spiking.modules.synapse.Parameter.reshape_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reshape_as</span></code></a>(other)</p></td>
<td><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.resize" title="hxtorch.spiking.modules.synapse.Parameter.resize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize</span></code></a>(*sizes)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.resize_" title="hxtorch.spiking.modules.synapse.Parameter.resize_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_</span></code></a>(*sizes[, memory_format])</p></td>
<td><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.resize_as" title="hxtorch.spiking.modules.synapse.Parameter.resize_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_as</span></code></a>(tensor)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.resize_as_" title="hxtorch.spiking.modules.synapse.Parameter.resize_as_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_as_</span></code></a>(tensor[, memory_format])</p></td>
<td><p>Resizes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to be the same size as the specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.resize_as_sparse_" title="hxtorch.spiking.modules.synapse.Parameter.resize_as_sparse_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_as_sparse_</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.resolve_conj" title="hxtorch.spiking.modules.synapse.Parameter.resolve_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resolve_conj</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_conj()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.resolve_neg" title="hxtorch.spiking.modules.synapse.Parameter.resolve_neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resolve_neg</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_neg()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.retain_grad" title="hxtorch.spiking.modules.synapse.Parameter.retain_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">retain_grad</span></code></a>()</p></td>
<td><p>Enables this Tensor to have their <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated during <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.roll" title="hxtorch.spiking.modules.synapse.Parameter.roll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roll</span></code></a>(shifts, dims)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.roll()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.rot90" title="hxtorch.spiking.modules.synapse.Parameter.rot90"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rot90</span></code></a>(k, dims)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rot90()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.round" title="hxtorch.spiking.modules.synapse.Parameter.round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">round</span></code></a>([decimals])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.round_" title="hxtorch.spiking.modules.synapse.Parameter.round_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">round_</span></code></a>([decimals])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">round()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.row_indices" title="hxtorch.spiking.modules.synapse.Parameter.row_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">row_indices</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.rsqrt" title="hxtorch.spiking.modules.synapse.Parameter.rsqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rsqrt</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rsqrt()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.rsqrt_" title="hxtorch.spiking.modules.synapse.Parameter.rsqrt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rsqrt_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">rsqrt()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.scatter" title="hxtorch.spiking.modules.synapse.Parameter.scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter</span></code></a>(dim, index, src)</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_" title="hxtorch.spiking.modules.synapse.Parameter.scatter_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_</span></code></a>(dim, index, src[, reduce])</p></td>
<td><p>Writes all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_add" title="hxtorch.spiking.modules.synapse.Parameter.scatter_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_add</span></code></a>(dim, index, src)</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_add_" title="hxtorch.spiking.modules.synapse.Parameter.scatter_add_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_add_</span></code></a>(dim, index, src)</p></td>
<td><p>Adds all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in a similar fashion as <code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_reduce" title="hxtorch.spiking.modules.synapse.Parameter.scatter_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_reduce</span></code></a>(dim, index, src, reduce, *[, …])</p></td>
<td><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_reduce_()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_reduce_" title="hxtorch.spiking.modules.synapse.Parameter.scatter_reduce_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_reduce_</span></code></a>(dim, index, src, reduce, *)</p></td>
<td><p>Reduces all values from the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor to the indices specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor using the applied reduction defined via the <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> argument (<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.select" title="hxtorch.spiking.modules.synapse.Parameter.select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select</span></code></a>(dim, index)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.select_scatter" title="hxtorch.spiking.modules.synapse.Parameter.select_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select_scatter</span></code></a>(src, dim, index)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select_scatter()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.set_" title="hxtorch.spiking.modules.synapse.Parameter.set_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_</span></code></a>([source, storage_offset, size, stride])</p></td>
<td><p>Sets the underlying storage, size, and strides.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sgn" title="hxtorch.spiking.modules.synapse.Parameter.sgn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgn</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sgn()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sgn_" title="hxtorch.spiking.modules.synapse.Parameter.sgn_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgn_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sgn()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.share_memory_" title="hxtorch.spiking.modules.synapse.Parameter.share_memory_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory_</span></code></a>()</p></td>
<td><p>Moves the underlying storage to shared memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.short" title="hxtorch.spiking.modules.synapse.Parameter.short"><code class="xref py py-obj docutils literal notranslate"><span class="pre">short</span></code></a>([memory_format])</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.short()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int16)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sigmoid" title="hxtorch.spiking.modules.synapse.Parameter.sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigmoid</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sigmoid_" title="hxtorch.spiking.modules.synapse.Parameter.sigmoid_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigmoid_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sigmoid()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sign" title="hxtorch.spiking.modules.synapse.Parameter.sign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sign</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sign()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sign_" title="hxtorch.spiking.modules.synapse.Parameter.sign_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sign_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sign()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.signbit" title="hxtorch.spiking.modules.synapse.Parameter.signbit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">signbit</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.signbit()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sin" title="hxtorch.spiking.modules.synapse.Parameter.sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sin</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sin_" title="hxtorch.spiking.modules.synapse.Parameter.sin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sin_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sin()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sinc" title="hxtorch.spiking.modules.synapse.Parameter.sinc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinc</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinc()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sinc_" title="hxtorch.spiking.modules.synapse.Parameter.sinc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinc_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sinc()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sinh" title="hxtorch.spiking.modules.synapse.Parameter.sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sinh_" title="hxtorch.spiking.modules.synapse.Parameter.sinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinh_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sinh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">size</span></code></a>([dim])</p></td>
<td><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.slice_scatter" title="hxtorch.spiking.modules.synapse.Parameter.slice_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slice_scatter</span></code></a>(src[, dim, start, end, step])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slice_scatter()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.slogdet" title="hxtorch.spiking.modules.synapse.Parameter.slogdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slogdet</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slogdet()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.smm" title="hxtorch.spiking.modules.synapse.Parameter.smm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">smm</span></code></a>(mat)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.smm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.softmax" title="hxtorch.spiking.modules.synapse.Parameter.softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">softmax</span></code></a>(dim)</p></td>
<td><p>Alias for <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.softmax()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.solve" title="hxtorch.spiking.modules.synapse.Parameter.solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">solve</span></code></a>(other)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sort" title="hxtorch.spiking.modules.synapse.Parameter.sort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sort</span></code></a>([dim, descending])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_dim" title="hxtorch.spiking.modules.synapse.Parameter.sparse_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_dim</span></code></a>()</p></td>
<td><p>Return the number of sparse dimensions in a <span class="xref std std-ref">sparse tensor</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_mask" title="hxtorch.spiking.modules.synapse.Parameter.sparse_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_mask</span></code></a>(mask)</p></td>
<td><p>Returns a new <span class="xref std std-ref">sparse tensor</span> with values from a strided tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> filtered by the indices of the sparse tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_resize_" title="hxtorch.spiking.modules.synapse.Parameter.sparse_resize_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_resize_</span></code></a>(size, sparse_dim, dense_dim)</p></td>
<td><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> <span class="xref std std-ref">sparse tensor</span> to the desired size and the number of sparse and dense dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_resize_and_clear_" title="hxtorch.spiking.modules.synapse.Parameter.sparse_resize_and_clear_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_resize_and_clear_</span></code></a>(size, sparse_dim, …)</p></td>
<td><p>Removes all specified elements from a <span class="xref std std-ref">sparse tensor</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> to the desired size and the number of sparse and dense dimensions.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.split" title="hxtorch.spiking.modules.synapse.Parameter.split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">split</span></code></a>(split_size[, dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.split_with_sizes" title="hxtorch.spiking.modules.synapse.Parameter.split_with_sizes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">split_with_sizes</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sqrt" title="hxtorch.spiking.modules.synapse.Parameter.sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sqrt_" title="hxtorch.spiking.modules.synapse.Parameter.sqrt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.square" title="hxtorch.spiking.modules.synapse.Parameter.square"><code class="xref py py-obj docutils literal notranslate"><span class="pre">square</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.square()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.square_" title="hxtorch.spiking.modules.synapse.Parameter.square_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">square_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">square()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.squeeze" title="hxtorch.spiking.modules.synapse.Parameter.squeeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeeze</span></code></a>([dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.squeeze_" title="hxtorch.spiking.modules.synapse.Parameter.squeeze_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeeze_</span></code></a>([dim])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">squeeze()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sspaddmm" title="hxtorch.spiking.modules.synapse.Parameter.sspaddmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sspaddmm</span></code></a>(mat1, mat2, *[, beta, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sspaddmm()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.std" title="hxtorch.spiking.modules.synapse.Parameter.std"><code class="xref py py-obj docutils literal notranslate"><span class="pre">std</span></code></a>([dim, correction, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.std()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.stft" title="hxtorch.spiking.modules.synapse.Parameter.stft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stft</span></code></a>(n_fft[, hop_length, win_length, …])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.stft()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.storage" title="hxtorch.spiking.modules.synapse.Parameter.storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">storage</span></code></a>()</p></td>
<td><p>Returns the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">TypedStorage</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.storage_offset" title="hxtorch.spiking.modules.synapse.Parameter.storage_offset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">storage_offset</span></code></a>()</p></td>
<td><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s offset in the underlying storage in terms of number of storage elements (not bytes).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.storage_type" title="hxtorch.spiking.modules.synapse.Parameter.storage_type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">storage_type</span></code></a>()</p></td>
<td><p>Returns the type of the underlying storage.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.stride" title="hxtorch.spiking.modules.synapse.Parameter.stride"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stride</span></code></a>(dim)</p></td>
<td><p>Returns the stride of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sub" title="hxtorch.spiking.modules.synapse.Parameter.sub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sub</span></code></a>(other, *[, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sub()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sub_" title="hxtorch.spiking.modules.synapse.Parameter.sub_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sub_</span></code></a>(other, *[, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.subtract" title="hxtorch.spiking.modules.synapse.Parameter.subtract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">subtract</span></code></a>(other, *[, alpha])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.subtract()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.subtract_" title="hxtorch.spiking.modules.synapse.Parameter.subtract_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">subtract_</span></code></a>(other, *[, alpha])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">subtract()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sum" title="hxtorch.spiking.modules.synapse.Parameter.sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sum</span></code></a>([dim, keepdim, dtype])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sum()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sum_to_size" title="hxtorch.spiking.modules.synapse.Parameter.sum_to_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sum_to_size</span></code></a>(*size)</p></td>
<td><p>Sum <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor to <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.svd" title="hxtorch.spiking.modules.synapse.Parameter.svd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">svd</span></code></a>([some, compute_uv])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.swapaxes" title="hxtorch.spiking.modules.synapse.Parameter.swapaxes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapaxes</span></code></a>(axis0, axis1)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapaxes()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.swapaxes_" title="hxtorch.spiking.modules.synapse.Parameter.swapaxes_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapaxes_</span></code></a>(axis0, axis1)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">swapaxes()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.swapdims" title="hxtorch.spiking.modules.synapse.Parameter.swapdims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapdims</span></code></a>(dim0, dim1)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapdims()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.swapdims_" title="hxtorch.spiking.modules.synapse.Parameter.swapdims_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapdims_</span></code></a>(dim0, dim1)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">swapdims()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.symeig" title="hxtorch.spiking.modules.synapse.Parameter.symeig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">symeig</span></code></a>([eigenvectors])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.t" title="hxtorch.spiking.modules.synapse.Parameter.t"><code class="xref py py-obj docutils literal notranslate"><span class="pre">t</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.t()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.t_" title="hxtorch.spiking.modules.synapse.Parameter.t_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">t_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.take" title="hxtorch.spiking.modules.synapse.Parameter.take"><code class="xref py py-obj docutils literal notranslate"><span class="pre">take</span></code></a>(indices)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.take_along_dim" title="hxtorch.spiking.modules.synapse.Parameter.take_along_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">take_along_dim</span></code></a>(indices, dim)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take_along_dim()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tan" title="hxtorch.spiking.modules.synapse.Parameter.tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tan</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tan_" title="hxtorch.spiking.modules.synapse.Parameter.tan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tan_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">tan()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tanh" title="hxtorch.spiking.modules.synapse.Parameter.tanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tanh</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tanh()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tanh_" title="hxtorch.spiking.modules.synapse.Parameter.tanh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tanh_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">tanh()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tensor_split" title="hxtorch.spiking.modules.synapse.Parameter.tensor_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor_split</span></code></a>(indices_or_sections[, dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor_split()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tile" title="hxtorch.spiking.modules.synapse.Parameter.tile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tile</span></code></a>(*reps)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tile()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Performs Tensor dtype and/or device conversion.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_dense" title="hxtorch.spiking.modules.synapse.Parameter.to_dense"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_dense</span></code></a>()</p></td>
<td><p>Creates a strided copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a strided tensor, otherwise returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_mkldnn" title="hxtorch.spiking.modules.synapse.Parameter.to_mkldnn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_mkldnn</span></code></a>()</p></td>
<td><p>Returns a copy of the tensor in <code class="docutils literal notranslate"><span class="pre">torch.mkldnn</span></code> layout.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor" title="hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_padded_tensor</span></code></a>(padding[, output_size])</p></td>
<td><p>See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor" title="hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_padded_tensor()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse" title="hxtorch.spiking.modules.synapse.Parameter.to_sparse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_sparse</span></code></a>(sparseDims)</p></td>
<td><p>Returns a sparse copy of the tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsc" title="hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_sparse_bsc</span></code></a>(blocksize, dense_dim)</p></td>
<td><p>Convert a tensor to a block sparse column (BSC) storage format of given blocksize.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsr" title="hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_sparse_bsr</span></code></a>(blocksize, dense_dim)</p></td>
<td><p>Convert a tensor to a block sparse row (BSR) storage format of given blocksize.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_coo" title="hxtorch.spiking.modules.synapse.Parameter.to_sparse_coo"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_sparse_coo</span></code></a>()</p></td>
<td><p>Convert a tensor to <span class="xref std std-ref">coordinate format</span>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_csc" title="hxtorch.spiking.modules.synapse.Parameter.to_sparse_csc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_sparse_csc</span></code></a>()</p></td>
<td><p>Convert a tensor to compressed column storage (CSC) format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_csr" title="hxtorch.spiking.modules.synapse.Parameter.to_sparse_csr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_sparse_csr</span></code></a>([dense_dim])</p></td>
<td><p>Convert a tensor to compressed row storage format (CSR).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tolist" title="hxtorch.spiking.modules.synapse.Parameter.tolist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tolist</span></code></a>()</p></td>
<td><p>Returns the tensor as a (nested) list.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.topk" title="hxtorch.spiking.modules.synapse.Parameter.topk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">topk</span></code></a>(k[, dim, largest, sorted])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.trace" title="hxtorch.spiking.modules.synapse.Parameter.trace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trace</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trace()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.transpose" title="hxtorch.spiking.modules.synapse.Parameter.transpose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transpose</span></code></a>(dim0, dim1)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.transpose_" title="hxtorch.spiking.modules.synapse.Parameter.transpose_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transpose_</span></code></a>(dim0, dim1)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.triangular_solve" title="hxtorch.spiking.modules.synapse.Parameter.triangular_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triangular_solve</span></code></a>(A[, upper, transpose, …])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triangular_solve()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tril" title="hxtorch.spiking.modules.synapse.Parameter.tril"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tril</span></code></a>([diagonal])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tril()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.tril_" title="hxtorch.spiking.modules.synapse.Parameter.tril_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tril_</span></code></a>([diagonal])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">tril()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.triu" title="hxtorch.spiking.modules.synapse.Parameter.triu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triu</span></code></a>([diagonal])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triu()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.triu_" title="hxtorch.spiking.modules.synapse.Parameter.triu_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triu_</span></code></a>([diagonal])</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">triu()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.true_divide" title="hxtorch.spiking.modules.synapse.Parameter.true_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">true_divide</span></code></a>(value)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.true_divide()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.true_divide_" title="hxtorch.spiking.modules.synapse.Parameter.true_divide_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">true_divide_</span></code></a>(value)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">true_divide_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.trunc" title="hxtorch.spiking.modules.synapse.Parameter.trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trunc</span></code></a>()</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.trunc_" title="hxtorch.spiking.modules.synapse.Parameter.trunc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trunc_</span></code></a>()</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">trunc()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.type" title="hxtorch.spiking.modules.synapse.Parameter.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>([dtype, non_blocking])</p></td>
<td><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to the specified type.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.type_as" title="hxtorch.spiking.modules.synapse.Parameter.type_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type_as</span></code></a>(tensor)</p></td>
<td><p>Returns this tensor cast to the type of the given tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unbind" title="hxtorch.spiking.modules.synapse.Parameter.unbind"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unbind</span></code></a>([dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unbind()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unflatten" title="hxtorch.spiking.modules.synapse.Parameter.unflatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unflatten</span></code></a>(dim, sizes)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unflatten()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unfold" title="hxtorch.spiking.modules.synapse.Parameter.unfold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unfold</span></code></a>(dimension, size, step)</p></td>
<td><p>Returns a view of the original tensor which contains all slices of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> from <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.uniform_" title="hxtorch.spiking.modules.synapse.Parameter.uniform_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">uniform_</span></code></a>([from, to])</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the continuous uniform distribution:</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unique" title="hxtorch.spiking.modules.synapse.Parameter.unique"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unique</span></code></a>([sorted, return_inverse, …])</p></td>
<td><p>Returns the unique elements of the input tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unique_consecutive" title="hxtorch.spiking.modules.synapse.Parameter.unique_consecutive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unique_consecutive</span></code></a>([return_inverse, …])</p></td>
<td><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unsafe_chunk" title="hxtorch.spiking.modules.synapse.Parameter.unsafe_chunk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsafe_chunk</span></code></a>(chunks[, dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsafe_chunk()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unsafe_split" title="hxtorch.spiking.modules.synapse.Parameter.unsafe_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsafe_split</span></code></a>(split_size[, dim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsafe_split()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unsafe_split_with_sizes" title="hxtorch.spiking.modules.synapse.Parameter.unsafe_split_with_sizes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsafe_split_with_sizes</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unsqueeze" title="hxtorch.spiking.modules.synapse.Parameter.unsqueeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsqueeze</span></code></a>(dim)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.unsqueeze_" title="hxtorch.spiking.modules.synapse.Parameter.unsqueeze_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsqueeze_</span></code></a>(dim)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.untyped_storage" title="hxtorch.spiking.modules.synapse.Parameter.untyped_storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">untyped_storage</span></code></a>()</p></td>
<td><p>Returns the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.values" title="hxtorch.spiking.modules.synapse.Parameter.values"><code class="xref py py-obj docutils literal notranslate"><span class="pre">values</span></code></a>()</p></td>
<td><p>Return the values tensor of a <span class="xref std std-ref">sparse COO tensor</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.var" title="hxtorch.spiking.modules.synapse.Parameter.var"><code class="xref py py-obj docutils literal notranslate"><span class="pre">var</span></code></a>([dim, correction, keepdim])</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.var()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.vdot" title="hxtorch.spiking.modules.synapse.Parameter.vdot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vdot</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vdot()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.view" title="hxtorch.spiking.modules.synapse.Parameter.view"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view</span></code></a>(*shape)</p></td>
<td><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a different <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.shape" title="hxtorch.spiking.modules.synapse.Parameter.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.view_as" title="hxtorch.spiking.modules.synapse.Parameter.view_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view_as</span></code></a>(other)</p></td>
<td><p>View this tensor as the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.vsplit" title="hxtorch.spiking.modules.synapse.Parameter.vsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vsplit</span></code></a>(split_size_or_sections)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vsplit()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.where" title="hxtorch.spiking.modules.synapse.Parameter.where"><code class="xref py py-obj docutils literal notranslate"><span class="pre">where</span></code></a>(condition, y)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.where(condition,</span> <span class="pre">y)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">torch.where(condition,</span> <span class="pre">self,</span> <span class="pre">y)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.xlogy" title="hxtorch.spiking.modules.synapse.Parameter.xlogy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xlogy</span></code></a>(other)</p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.xlogy()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.xlogy_" title="hxtorch.spiking.modules.synapse.Parameter.xlogy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xlogy_</span></code></a>(other)</p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">xlogy()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.xpu" title="hxtorch.spiking.modules.synapse.Parameter.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xpu</span></code></a>([device, non_blocking, memory_format])</p></td>
<td><p>Returns a copy of this object in XPU memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.zero_" title="hxtorch.spiking.modules.synapse.Parameter.zero_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_</span></code></a>()</p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with zeros.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.H" title="hxtorch.spiking.modules.synapse.Parameter.H"><code class="xref py py-obj docutils literal notranslate"><span class="pre">H</span></code></a></p></td>
<td><p>Returns a view of a matrix (2-D tensor) conjugated and transposed.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.T" title="hxtorch.spiking.modules.synapse.Parameter.T"><code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code></a></p></td>
<td><p>Returns a view of this tensor with its dimensions reversed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.data" title="hxtorch.spiking.modules.synapse.Parameter.data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">data</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.device" title="hxtorch.spiking.modules.synapse.Parameter.device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">device</span></code></a></p></td>
<td><p>Is the <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> where this Tensor is.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dtype</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">grad</span></code></a></p></td>
<td><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default and becomes a Tensor the first time a call to <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> computes gradients for <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad_fn" title="hxtorch.spiking.modules.synapse.Parameter.grad_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_fn</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.imag" title="hxtorch.spiking.modules.synapse.Parameter.imag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">imag</span></code></a></p></td>
<td><p>Returns a new tensor containing imaginary values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_cpu" title="hxtorch.spiking.modules.synapse.Parameter.is_cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_cpu</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the CPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_cuda" title="hxtorch.spiking.modules.synapse.Parameter.is_cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_cuda</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_ipu" title="hxtorch.spiking.modules.synapse.Parameter.is_ipu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_ipu</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the IPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_leaf" title="hxtorch.spiking.modules.synapse.Parameter.is_leaf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_leaf</span></code></a></p></td>
<td><p>All Tensors that have <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">False</span></code> will be leaf Tensors by convention.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_meta" title="hxtorch.spiking.modules.synapse.Parameter.is_meta"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_meta</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is a meta tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_mkldnn" title="hxtorch.spiking.modules.synapse.Parameter.is_mkldnn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_mkldnn</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_mps" title="hxtorch.spiking.modules.synapse.Parameter.is_mps"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_mps</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the MPS device, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_nested" title="hxtorch.spiking.modules.synapse.Parameter.is_nested"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_nested</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_ort" title="hxtorch.spiking.modules.synapse.Parameter.is_ort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_ort</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_quantized" title="hxtorch.spiking.modules.synapse.Parameter.is_quantized"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_quantized</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is quantized, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_sparse" title="hxtorch.spiking.modules.synapse.Parameter.is_sparse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_sparse</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_sparse_csr" title="hxtorch.spiking.modules.synapse.Parameter.is_sparse_csr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_sparse_csr</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse CSR storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_vulkan" title="hxtorch.spiking.modules.synapse.Parameter.is_vulkan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_vulkan</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_xpu" title="hxtorch.spiking.modules.synapse.Parameter.is_xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_xpu</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the XPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.layout" title="hxtorch.spiking.modules.synapse.Parameter.layout"><code class="xref py py-obj docutils literal notranslate"><span class="pre">layout</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mH" title="hxtorch.spiking.modules.synapse.Parameter.mH"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mH</span></code></a></p></td>
<td><p>Accessing this property is equivalent to calling <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.adjoint" title="hxtorch.spiking.modules.synapse.Parameter.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mT" title="hxtorch.spiking.modules.synapse.Parameter.mT"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mT</span></code></a></p></td>
<td><p>Returns a view of this tensor with the last two dimensions transposed.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.name" title="hxtorch.spiking.modules.synapse.Parameter.name"><code class="xref py py-obj docutils literal notranslate"><span class="pre">name</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">names</span></code></a></p></td>
<td><p>Stores names for each of this tensor’s dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.ndim" title="hxtorch.spiking.modules.synapse.Parameter.ndim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ndim</span></code></a></p></td>
<td><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.output_nr" title="hxtorch.spiking.modules.synapse.Parameter.output_nr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">output_nr</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.real" title="hxtorch.spiking.modules.synapse.Parameter.real"><code class="xref py py-obj docutils literal notranslate"><span class="pre">real</span></code></a></p></td>
<td><p>Returns a new tensor containing real values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor for a complex-valued input tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this Tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.retains_grad" title="hxtorch.spiking.modules.synapse.Parameter.retains_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">retains_grad</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if this Tensor is non-leaf and its <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> is enabled to be populated during <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.shape" title="hxtorch.spiking.modules.synapse.Parameter.shape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shape</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.volatile" title="hxtorch.spiking.modules.synapse.Parameter.volatile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">volatile</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.H">
<code class="sig-name descname"><span class="pre">H</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.H" title="Permalink to this definition"></a></dt>
<dd><p>Returns a view of a matrix (2-D tensor) conjugated and transposed.</p>
<p><code class="docutils literal notranslate"><span class="pre">x.H</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.transpose(0,</span> <span class="pre">1).conj()</span></code> for complex matrices and
<code class="docutils literal notranslate"><span class="pre">x.transpose(0,</span> <span class="pre">1)</span></code> for real matrices.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">mH</span></code>: An attribute that also works on batches of matrices.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.T">
<code class="sig-name descname"><span class="pre">T</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.T" title="Permalink to this definition"></a></dt>
<dd><p>Returns a view of this tensor with its dimensions reversed.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">n</span></code> is the number of dimensions in <code class="docutils literal notranslate"><span class="pre">x</span></code>,
<code class="docutils literal notranslate"><span class="pre">x.T</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.permute(n-1,</span> <span class="pre">n-2,</span> <span class="pre">...,</span> <span class="pre">0)</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The use of <code class="xref py py-func docutils literal notranslate"><span class="pre">Tensor.T()</span></code> on tensors of dimension other than 2 to reverse their shape
is deprecated and it will throw an error in a future release. Consider <code class="xref py py-attr docutils literal notranslate"><span class="pre">mT</span></code>
to transpose batches of matrices or <cite>x.permute(*torch.arange(x.ndim - 1, -1, -1))</cite> to reverse
the dimensions of a tensor.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.abs">
<code class="sig-name descname"><span class="pre">abs</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.abs" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.abs_">
<code class="sig-name descname"><span class="pre">abs_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.abs_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">abs()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.absolute">
<code class="sig-name descname"><span class="pre">absolute</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.absolute" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.abs" title="hxtorch.spiking.modules.synapse.Parameter.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.absolute_">
<code class="sig-name descname"><span class="pre">absolute_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.absolute_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">absolute()</span></code>
Alias for <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.abs_" title="hxtorch.spiking.modules.synapse.Parameter.abs_"><code class="xref py py-func docutils literal notranslate"><span class="pre">abs_()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.acos">
<code class="sig-name descname"><span class="pre">acos</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.acos" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.acos_">
<code class="sig-name descname"><span class="pre">acos_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.acos_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">acos()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.acosh">
<code class="sig-name descname"><span class="pre">acosh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.acosh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acosh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.acosh_">
<code class="sig-name descname"><span class="pre">acosh_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.acosh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">acosh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.add">
<code class="sig-name descname"><span class="pre">add</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.add" title="Permalink to this definition"></a></dt>
<dd><p>Add a scalar or tensor to <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If both <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> are specified, each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is scaled by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> before being used.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is a tensor, the shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<span class="xref std std-ref">broadcastable</span> with the shape of the underlying
tensor</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.add()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.add_">
<code class="sig-name descname"><span class="pre">add_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.add_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addbmm">
<code class="sig-name descname"><span class="pre">addbmm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addbmm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addbmm_">
<code class="sig-name descname"><span class="pre">addbmm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addbmm_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addbmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addcdiv">
<code class="sig-name descname"><span class="pre">addcdiv</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addcdiv" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcdiv()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addcdiv_">
<code class="sig-name descname"><span class="pre">addcdiv_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addcdiv_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addcdiv()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addcmul">
<code class="sig-name descname"><span class="pre">addcmul</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addcmul" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcmul()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addcmul_">
<code class="sig-name descname"><span class="pre">addcmul_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addcmul_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addcmul()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addmm">
<code class="sig-name descname"><span class="pre">addmm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addmm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addmm_">
<code class="sig-name descname"><span class="pre">addmm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addmm_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addmv">
<code class="sig-name descname"><span class="pre">addmv</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addmv" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmv()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addmv_">
<code class="sig-name descname"><span class="pre">addmv_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addmv_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addmv()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addr">
<code class="sig-name descname"><span class="pre">addr</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addr" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addr()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.addr_">
<code class="sig-name descname"><span class="pre">addr_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.addr_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">addr()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.adjoint">
<code class="sig-name descname"><span class="pre">adjoint</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.adjoint" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.adjoint" title="hxtorch.spiking.modules.synapse.Parameter.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.align_as">
<code class="sig-name descname"><span class="pre">align_as</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.align_as" title="Permalink to this definition"></a></dt>
<dd><p>Permutes the dimensions of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to match the dimension order
in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> tensor, adding size-one dims for any new names.</p>
<p>This operation is useful for explicit broadcasting by names (see examples).</p>
<p>All of the dims of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be named in order to use this method.
The resulting tensor is a view on the original tensor.</p>
<p>All dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be present in <code class="docutils literal notranslate"><span class="pre">other.names</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> may contain named dimensions that are not in <code class="docutils literal notranslate"><span class="pre">self.names</span></code>;
the output tensor has a size-one dimension for each of those new names.</p>
<p>To align a tensor to a specific order, use <code class="xref py py-meth docutils literal notranslate"><span class="pre">align_to()</span></code>.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example 1: Applying a mask</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">127</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">imgs</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">align_as</span><span class="p">(</span><span class="n">imgs</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>


<span class="c1"># Example 2: Applying a per-channel-scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">scale_channels</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
<span class="o">&gt;&gt;&gt;</span>    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>    <span class="k">return</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">scale</span><span class="o">.</span><span class="n">align_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_channels</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">more_imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">videos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">))</span>

<span class="c1"># scale_channels is agnostic to the dimension order of the input</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale_channels</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale_channels</span><span class="p">(</span><span class="n">more_imgs</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale_channels</span><span class="p">(</span><span class="n">videos</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.align_to">
<code class="sig-name descname"><span class="pre">align_to</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.align_to" title="Permalink to this definition"></a></dt>
<dd><p>Permutes the dimensions of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to match the order
specified in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>, adding size-one dims for any new names.</p>
<p>All of the dims of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be named in order to use this method.
The resulting tensor is a view on the original tensor.</p>
<p>All dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be present in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>.
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> may contain additional names that are not in <code class="docutils literal notranslate"><span class="pre">self.names</span></code>;
the output tensor has a size-one dimension for each of those new names.</p>
<p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> may contain up to one Ellipsis (<code class="docutils literal notranslate"><span class="pre">...</span></code>).
The Ellipsis is expanded to be equal to all dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
that are not mentioned in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>, in the order that they appear
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>Python 2 does not support Ellipsis but one may use a string literal
instead (<code class="docutils literal notranslate"><span class="pre">'...'</span></code>).</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>names (iterable of str): The desired dimension ordering of the</dt><dd><p>output tensor. May contain up to one Ellipsis that is expanded
to all unmentioned dim names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">)</span>

<span class="go"># Move the F and E dims to the front while keeping the rest in order</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_tensor</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span><span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.all">
<code class="sig-name descname"><span class="pre">all</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.all" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.all()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.allclose">
<code class="sig-name descname"><span class="pre">allclose</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rtol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">atol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equal_nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.allclose" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.allclose()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.amax">
<code class="sig-name descname"><span class="pre">amax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.amax" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amax()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.amin">
<code class="sig-name descname"><span class="pre">amin</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.amin" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.aminmax">
<code class="sig-name descname"><span class="pre">aminmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">*</span></em>, <em class="sig-param"><span class="pre">dim=None</span></em>, <em class="sig-param"><span class="pre">keepdim=False)</span> <span class="pre">-&gt;</span> <span class="pre">(Tensor</span> <span class="pre">min</span></em>, <em class="sig-param"><span class="pre">Tensor</span> <span class="pre">max</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.aminmax" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.aminmax()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.angle">
<code class="sig-name descname"><span class="pre">angle</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.angle" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.angle()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.any">
<code class="sig-name descname"><span class="pre">any</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.any" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.any()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.apply_">
<code class="sig-name descname"><span class="pre">apply_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">callable</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.apply_" title="Permalink to this definition"></a></dt>
<dd><p>Applies the function <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> to each element in the tensor, replacing
each element with the value returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function only works with CPU tensors and should not be used in code
sections that require high performance.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arccos">
<code class="sig-name descname"><span class="pre">arccos</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arccos" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arccos()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arccos_">
<code class="sig-name descname"><span class="pre">arccos_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arccos_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arccos()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arccosh">
<code class="sig-name descname"><span class="pre">arccosh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arccosh" title="Permalink to this definition"></a></dt>
<dd><p>acosh() -&gt; Tensor</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arccosh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arccosh_">
<code class="sig-name descname"><span class="pre">arccosh_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arccosh_" title="Permalink to this definition"></a></dt>
<dd><p>acosh_() -&gt; Tensor</p>
<p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arccosh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arcsin">
<code class="sig-name descname"><span class="pre">arcsin</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arcsin" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arcsin_">
<code class="sig-name descname"><span class="pre">arcsin_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arcsin_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arcsinh">
<code class="sig-name descname"><span class="pre">arcsinh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arcsinh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsinh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arcsinh_">
<code class="sig-name descname"><span class="pre">arcsinh_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arcsinh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsinh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arctan">
<code class="sig-name descname"><span class="pre">arctan</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arctan" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arctan2">
<code class="sig-name descname"><span class="pre">arctan2</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arctan2" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arctan2_">
<code class="sig-name descname"><span class="pre">arctan2_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arctan2_" title="Permalink to this definition"></a></dt>
<dd><p>atan2_(other) -&gt; Tensor</p>
<p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arctan2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arctan_">
<code class="sig-name descname"><span class="pre">arctan_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arctan_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arctan()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arctanh">
<code class="sig-name descname"><span class="pre">arctanh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arctanh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctanh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.arctanh_">
<code class="sig-name descname"><span class="pre">arctanh_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.arctanh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">arctanh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.argmax">
<code class="sig-name descname"><span class="pre">argmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">LongTensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.argmax" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmax()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.argmin">
<code class="sig-name descname"><span class="pre">argmin</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">LongTensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.argmin" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.argsort">
<code class="sig-name descname"><span class="pre">argsort</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">descending</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">LongTensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.argsort" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argsort()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.argwhere">
<code class="sig-name descname"><span class="pre">argwhere</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.argwhere" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argwhere()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.as_strided">
<code class="sig-name descname"><span class="pre">as_strided</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.as_strided" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.as_strided_">
<code class="sig-name descname"><span class="pre">as_strided_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.as_strided_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.as_strided_scatter">
<code class="sig-name descname"><span class="pre">as_strided_scatter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.as_strided_scatter" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided_scatter()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.as_subclass">
<code class="sig-name descname"><span class="pre">as_subclass</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.as_subclass" title="Permalink to this definition"></a></dt>
<dd><p>Makes a <code class="docutils literal notranslate"><span class="pre">cls</span></code> instance with the same data pointer as <code class="docutils literal notranslate"><span class="pre">self</span></code>. Changes
in the output mirror changes in <code class="docutils literal notranslate"><span class="pre">self</span></code>, and the output stays attached
to the autograd graph. <code class="docutils literal notranslate"><span class="pre">cls</span></code> must be a subclass of <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.asin">
<code class="sig-name descname"><span class="pre">asin</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.asin" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.asin_">
<code class="sig-name descname"><span class="pre">asin_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.asin_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.asinh">
<code class="sig-name descname"><span class="pre">asinh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.asinh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asinh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.asinh_">
<code class="sig-name descname"><span class="pre">asinh_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.asinh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">asinh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.atan">
<code class="sig-name descname"><span class="pre">atan</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.atan" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.atan2">
<code class="sig-name descname"><span class="pre">atan2</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.atan2" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.atan2_">
<code class="sig-name descname"><span class="pre">atan2_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.atan2_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">atan2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.atan_">
<code class="sig-name descname"><span class="pre">atan_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.atan_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">atan()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.atanh">
<code class="sig-name descname"><span class="pre">atanh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.atanh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atanh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.atanh_">
<code class="sig-name descname"><span class="pre">atanh_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.atanh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">atanh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.backward">
<code class="sig-name descname"><span class="pre">backward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradient</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="Permalink to this definition"></a></dt>
<dd><p>Computes the gradient of current tensor w.r.t. graph leaves.</p>
<p>The graph is differentiated using the chain rule. If the tensor is
non-scalar (i.e. its data has more than one element) and requires
gradient, the function additionally requires specifying <code class="docutils literal notranslate"><span class="pre">gradient</span></code>.
It should be a tensor of matching type and location, that contains
the gradient of the differentiated function w.r.t. <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>This function accumulates gradients in the leaves - you might need to zero
<code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes or set them to <code class="docutils literal notranslate"><span class="pre">None</span></code> before calling it.
See <span class="xref std std-ref">Default gradient layouts</span>
for details on the memory layout of accumulated gradients.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you run any forward ops, create <code class="docutils literal notranslate"><span class="pre">gradient</span></code>, and/or call <code class="docutils literal notranslate"><span class="pre">backward</span></code>
in a user-specified CUDA stream context, see
<span class="xref std std-ref">Stream semantics of backward passes</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are provided and a given input is not a leaf,
the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).
It is an implementation detail on which the user should not rely.
See <a class="reference external" href="https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780">https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780</a> for more details.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>gradient (Tensor or None): Gradient w.r.t. the</dt><dd><p>tensor. If it is a tensor, it will be automatically converted
to a Tensor that does not require grad unless <code class="docutils literal notranslate"><span class="pre">create_graph</span></code> is True.
None values can be specified for scalar Tensors or ones that
don’t require grad. If a None value would be acceptable then
this argument is optional.</p>
</dd>
<dt>retain_graph (bool, optional): If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute</dt><dd><p>the grads will be freed. Note that in nearly all cases setting
this option to True is not needed and often can be worked around
in a much more efficient way. Defaults to the value of
<code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p>
</dd>
<dt>create_graph (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will</dt><dd><p>be constructed, allowing to compute higher order derivative
products. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be</dt><dd><p>accumulated into <code class="docutils literal notranslate"><span class="pre">.grad</span></code>. All other Tensors will be ignored. If not
provided, the gradient is accumulated into all the leaf Tensors that were
used to compute the attr::tensors.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.baddbmm">
<code class="sig-name descname"><span class="pre">baddbmm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.baddbmm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.baddbmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.baddbmm_">
<code class="sig-name descname"><span class="pre">baddbmm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.baddbmm_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">baddbmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bernoulli">
<code class="sig-name descname"><span class="pre">bernoulli</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bernoulli" title="Permalink to this definition"></a></dt>
<dd><p>Returns a result tensor where each <span class="math notranslate nohighlight">\(\texttt{result[i]}\)</span> is independently
sampled from <span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{self[i]})\)</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, and the result will have the same <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bernoulli_">
<code class="sig-name descname"><span class="pre">bernoulli_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bernoulli_" title="Permalink to this definition"></a></dt>
<dd><p>Fills each location of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> with an independent sample from
<span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{p})\)</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> can have integral
<code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> should either be a scalar or tensor containing probabilities to be
used for drawing the binary random number.</p>
<p>If it is a tensor, the <span class="math notranslate nohighlight">\(\text{i}^{th}\)</span> element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor
will be set to a value sampled from
<span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{p\_tensor[i]})\)</span>. In this case <cite>p</cite> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p>See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">bernoulli()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bfloat16">
<code class="sig-name descname"><span class="pre">bfloat16</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bfloat16" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.bfloat16()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bfloat16)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bincount">
<code class="sig-name descname"><span class="pre">bincount</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minlength</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bincount" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bincount()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_and">
<code class="sig-name descname"><span class="pre">bitwise_and</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_and" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_and()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_and_">
<code class="sig-name descname"><span class="pre">bitwise_and_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_and_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_and()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift">
<code class="sig-name descname"><span class="pre">bitwise_left_shift</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_left_shift()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift_">
<code class="sig-name descname"><span class="pre">bitwise_left_shift_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_left_shift_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_left_shift()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_not">
<code class="sig-name descname"><span class="pre">bitwise_not</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_not" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_not()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_not_">
<code class="sig-name descname"><span class="pre">bitwise_not_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_not_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_not()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_or">
<code class="sig-name descname"><span class="pre">bitwise_or</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_or" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_or()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_or_">
<code class="sig-name descname"><span class="pre">bitwise_or_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_or_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_or()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift">
<code class="sig-name descname"><span class="pre">bitwise_right_shift</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_right_shift()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift_">
<code class="sig-name descname"><span class="pre">bitwise_right_shift_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_right_shift_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_right_shift()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_xor">
<code class="sig-name descname"><span class="pre">bitwise_xor</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_xor" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_xor()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bitwise_xor_">
<code class="sig-name descname"><span class="pre">bitwise_xor_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bitwise_xor_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_xor()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bmm">
<code class="sig-name descname"><span class="pre">bmm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bmm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.bool">
<code class="sig-name descname"><span class="pre">bool</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.bool()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bool)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.broadcast_to">
<code class="sig-name descname"><span class="pre">broadcast_to</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.broadcast_to" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.broadcast_to()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.byte">
<code class="sig-name descname"><span class="pre">byte</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.byte" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.byte()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.uint8)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cauchy_">
<code class="sig-name descname"><span class="pre">cauchy_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">median</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cauchy_" title="Permalink to this definition"></a></dt>
<dd><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - \text{median})^2 + \sigma^2}\]</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ccol_indices">
<code class="sig-name descname"><span class="pre">ccol_indices</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ccol_indices" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cdouble">
<code class="sig-name descname"><span class="pre">cdouble</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cdouble" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.cdouble()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex128)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ceil">
<code class="sig-name descname"><span class="pre">ceil</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ceil" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ceil_">
<code class="sig-name descname"><span class="pre">ceil_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ceil_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ceil()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cfloat">
<code class="sig-name descname"><span class="pre">cfloat</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cfloat" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.cfloat()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex64)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.chalf">
<code class="sig-name descname"><span class="pre">chalf</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.chalf" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.chalf()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex32)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.char">
<code class="sig-name descname"><span class="pre">char</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.char" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.char()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int8)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cholesky">
<code class="sig-name descname"><span class="pre">cholesky</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cholesky" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cholesky_inverse">
<code class="sig-name descname"><span class="pre">cholesky_inverse</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cholesky_inverse" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_inverse()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cholesky_solve">
<code class="sig-name descname"><span class="pre">cholesky_solve</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cholesky_solve" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.chunk">
<code class="sig-name descname"><span class="pre">chunk</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chunks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.chunk" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clamp">
<code class="sig-name descname"><span class="pre">clamp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clamp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clamp_">
<code class="sig-name descname"><span class="pre">clamp_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clamp_max">
<code class="sig-name descname"><span class="pre">clamp_max</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_max" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clamp_max_">
<code class="sig-name descname"><span class="pre">clamp_max_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_max_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clamp_min">
<code class="sig-name descname"><span class="pre">clamp_min</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_min" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clamp_min_">
<code class="sig-name descname"><span class="pre">clamp_min_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clamp_min_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clip">
<code class="sig-name descname"><span class="pre">clip</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clip" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clip_">
<code class="sig-name descname"><span class="pre">clip_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clip_" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp_()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.clone">
<code class="sig-name descname"><span class="pre">clone</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.clone" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clone()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.coalesce">
<code class="sig-name descname"><span class="pre">coalesce</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.coalesce" title="Permalink to this definition"></a></dt>
<dd><p>Returns a coalesced copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is an
<span class="xref std std-ref">uncoalesced tensor</span>.</p>
<p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a coalesced tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.col_indices">
<code class="sig-name descname"><span class="pre">col_indices</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">IntTensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.col_indices" title="Permalink to this definition"></a></dt>
<dd><p>Returns the tensor containing the column indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.
The <code class="docutils literal notranslate"><span class="pre">col_indices</span></code> tensor is strictly of shape (<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.nnz())
and of type <code class="docutils literal notranslate"><span class="pre">int32</span></code> or <code class="docutils literal notranslate"><span class="pre">int64</span></code>.  When using MKL routines such as sparse
matrix multiplication, it is necessary to use <code class="docutils literal notranslate"><span class="pre">int32</span></code> indexing in order
to avoid downcasting and potentially losing information.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span><span class="o">.</span><span class="n">col_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 2, 3, 4], dtype=torch.int32)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.conj">
<code class="sig-name descname"><span class="pre">conj</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.conj" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.conj_physical">
<code class="sig-name descname"><span class="pre">conj_physical</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.conj_physical" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj_physical()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.conj_physical_">
<code class="sig-name descname"><span class="pre">conj_physical_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.conj_physical_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">conj_physical()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.contiguous">
<code class="sig-name descname"><span class="pre">contiguous</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.contiguous" title="Permalink to this definition"></a></dt>
<dd><p>Returns a contiguous in memory tensor containing the same data as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is already in the specified memory format, this function returns the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.copy_">
<code class="sig-name descname"><span class="pre">copy_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.copy_" title="Permalink to this definition"></a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor must be <span class="xref std std-ref">broadcastable</span>
with the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. It may be of a different data type or reside on a
different device.</p>
<dl>
<dt>Args:</dt><dd><p>src (Tensor): the source tensor to copy from
non_blocking (bool): if <code class="docutils literal notranslate"><span class="pre">True</span></code> and this copy is between CPU and GPU,</p>
<blockquote>
<div><p>the copy may occur asynchronously with respect to the host. For other
cases, this argument has no effect.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.copysign">
<code class="sig-name descname"><span class="pre">copysign</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.copysign" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.copysign()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.copysign_">
<code class="sig-name descname"><span class="pre">copysign_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.copysign_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">copysign()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.corrcoef">
<code class="sig-name descname"><span class="pre">corrcoef</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.corrcoef" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.corrcoef()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cos">
<code class="sig-name descname"><span class="pre">cos</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cos" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cos_">
<code class="sig-name descname"><span class="pre">cos_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cos_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cos()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cosh">
<code class="sig-name descname"><span class="pre">cosh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cosh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cosh_">
<code class="sig-name descname"><span class="pre">cosh_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cosh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cosh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.count_nonzero">
<code class="sig-name descname"><span class="pre">count_nonzero</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.count_nonzero" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.count_nonzero()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cov">
<code class="sig-name descname"><span class="pre">cov</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fweights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aweights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cov" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cov()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cpu">
<code class="sig-name descname"><span class="pre">cpu</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cpu" title="Permalink to this definition"></a></dt>
<dd><p>Returns a copy of this object in CPU memory.</p>
<p>If this object is already in CPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cross">
<code class="sig-name descname"><span class="pre">cross</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cross" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cross()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.crow_indices">
<code class="sig-name descname"><span class="pre">crow_indices</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">IntTensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.crow_indices" title="Permalink to this definition"></a></dt>
<dd><p>Returns the tensor containing the compressed row indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.
The <code class="docutils literal notranslate"><span class="pre">crow_indices</span></code> tensor is strictly of shape (<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.size(0) + 1)
and of type <code class="docutils literal notranslate"><span class="pre">int32</span></code> or <code class="docutils literal notranslate"><span class="pre">int64</span></code>. When using MKL routines such as sparse
matrix multiplication, it is necessary to use <code class="docutils literal notranslate"><span class="pre">int32</span></code> indexing in order
to avoid downcasting and potentially losing information.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cuda">
<code class="sig-name descname"><span class="pre">cuda</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cuda" title="Permalink to this definition"></a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>): The destination GPU device.</dt><dd><p>Defaults to the current CUDA device.</p>
</dd>
<dt>non_blocking (bool): If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,</dt><dd><p>the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cummax">
<code class="sig-name descname"><span class="pre">cummax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cummax" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummax()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cummin">
<code class="sig-name descname"><span class="pre">cummin</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cummin" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cumprod">
<code class="sig-name descname"><span class="pre">cumprod</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cumprod" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumprod()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cumprod_">
<code class="sig-name descname"><span class="pre">cumprod_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cumprod_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cumprod()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cumsum">
<code class="sig-name descname"><span class="pre">cumsum</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cumsum" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumsum()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.cumsum_">
<code class="sig-name descname"><span class="pre">cumsum_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.cumsum_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">cumsum()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.data">
<code class="sig-name descname"><span class="pre">data</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.data" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.data_ptr">
<code class="sig-name descname"><span class="pre">data_ptr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.data_ptr" title="Permalink to this definition"></a></dt>
<dd><p>Returns the address of the first element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.deg2rad">
<code class="sig-name descname"><span class="pre">deg2rad</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.deg2rad" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.deg2rad()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.deg2rad_">
<code class="sig-name descname"><span class="pre">deg2rad_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.deg2rad_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">deg2rad()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.dense_dim">
<code class="sig-name descname"><span class="pre">dense_dim</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.dense_dim" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of dense dimensions in a <span class="xref std std-ref">sparse tensor</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returns <code class="docutils literal notranslate"><span class="pre">len(self.shape)</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse tensor.</p>
</div>
<p>See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.sparse_dim()</span></code> and <span class="xref std std-ref">hybrid tensors</span>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.dequantize">
<code class="sig-name descname"><span class="pre">dequantize</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.dequantize" title="Permalink to this definition"></a></dt>
<dd><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.det">
<code class="sig-name descname"><span class="pre">det</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.det" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.det()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.detach">
<code class="sig-name descname"><span class="pre">detach</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.detach" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new Tensor, detached from the current graph.</p>
<p>The result will never require gradient.</p>
<p>This method also affects forward mode AD gradients and the result will never
have forward mode AD gradients.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returned Tensor shares the same storage with the original one.
In-place modifications on either of them will be seen, and may trigger
errors in correctness checks.
IMPORTANT NOTE: Previously, in-place size / stride / storage changes
(such as <cite>resize_</cite> / <cite>resize_as_</cite> / <cite>set_</cite> / <cite>transpose_</cite>) to the returned tensor
also update the original tensor. Now, these in-place changes will not update the
original tensor anymore, and will instead trigger an error.
For sparse tensors:
In-place indices / values changes (such as <cite>zero_</cite> / <cite>copy_</cite> / <cite>add_</cite>) to the
returned tensor will not update the original tensor anymore, and will instead
trigger an error.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.detach_">
<code class="sig-name descname"><span class="pre">detach_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.detach_" title="Permalink to this definition"></a></dt>
<dd><p>Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.</p>
<p>This method also affects forward mode AD gradients and the result will never
have forward mode AD gradients.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.device">
<code class="sig-name descname"><span class="pre">device</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.device" title="Permalink to this definition"></a></dt>
<dd><p>Is the <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> where this Tensor is.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.diag">
<code class="sig-name descname"><span class="pre">diag</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.diag" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.diag_embed">
<code class="sig-name descname"><span class="pre">diag_embed</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.diag_embed" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.diagflat">
<code class="sig-name descname"><span class="pre">diagflat</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.diagflat" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.diagonal">
<code class="sig-name descname"><span class="pre">diagonal</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.diagonal" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.diagonal_scatter">
<code class="sig-name descname"><span class="pre">diagonal_scatter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.diagonal_scatter" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal_scatter()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.diff">
<code class="sig-name descname"><span class="pre">diff</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">append</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.diff" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diff()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.digamma">
<code class="sig-name descname"><span class="pre">digamma</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.digamma" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.digamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.digamma_">
<code class="sig-name descname"><span class="pre">digamma_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.digamma_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">digamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.dim">
<code class="sig-name descname"><span class="pre">dim</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.dim" title="Permalink to this definition"></a></dt>
<dd><p>Returns the number of dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.dist">
<code class="sig-name descname"><span class="pre">dist</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.dist" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dist()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.div">
<code class="sig-name descname"><span class="pre">div</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.div" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.div_">
<code class="sig-name descname"><span class="pre">div_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.div_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.divide">
<code class="sig-name descname"><span class="pre">divide</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.divide" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.divide()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.divide_">
<code class="sig-name descname"><span class="pre">divide_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.divide_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">divide()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.dot">
<code class="sig-name descname"><span class="pre">dot</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.dot" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dot()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.double">
<code class="sig-name descname"><span class="pre">double</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.double" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.double()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float64)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.dsplit">
<code class="sig-name descname"><span class="pre">dsplit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size_or_sections</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.dsplit" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dsplit()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.dtype">
<code class="sig-name descname"><span class="pre">dtype</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.eig">
<code class="sig-name descname"><span class="pre">eig</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eigenvectors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.eig" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.element_size">
<code class="sig-name descname"><span class="pre">element_size</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.element_size" title="Permalink to this definition"></a></dt>
<dd><p>Returns the size in bytes of an individual element.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.eq">
<code class="sig-name descname"><span class="pre">eq</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.eq" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.eq_">
<code class="sig-name descname"><span class="pre">eq_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.eq_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.equal">
<code class="sig-name descname"><span class="pre">equal</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.equal" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.erf">
<code class="sig-name descname"><span class="pre">erf</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.erf" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.erf_">
<code class="sig-name descname"><span class="pre">erf_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.erf_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">erf()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.erfc">
<code class="sig-name descname"><span class="pre">erfc</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.erfc" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.erfc_">
<code class="sig-name descname"><span class="pre">erfc_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.erfc_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">erfc()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.erfinv">
<code class="sig-name descname"><span class="pre">erfinv</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.erfinv" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfinv()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.erfinv_">
<code class="sig-name descname"><span class="pre">erfinv_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.erfinv_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">erfinv()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.exp">
<code class="sig-name descname"><span class="pre">exp</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.exp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.exp2">
<code class="sig-name descname"><span class="pre">exp2</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.exp2" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.exp2_">
<code class="sig-name descname"><span class="pre">exp2_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.exp2_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">exp2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.exp_">
<code class="sig-name descname"><span class="pre">exp_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.exp_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">exp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.expand">
<code class="sig-name descname"><span class="pre">expand</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sizes</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.expand" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded
to a larger size.</p>
<p>Passing -1 as the size for a dimension means not changing the size of
that dimension.</p>
<p>Tensor can be also expanded to a larger number of dimensions, and the
new ones will be appended at the front. For the new dimensions, the
size cannot be set to -1.</p>
<p>Expanding a tensor does not allocate new memory, but only creates a
new view on the existing tensor where a dimension of size one is
expanded to a larger size by setting the <code class="docutils literal notranslate"><span class="pre">stride</span></code> to 0. Any dimension
of size 1 can be expanded to an arbitrary value without allocating new
memory.</p>
<dl class="simple">
<dt>Args:</dt><dd><p><a href="#id1"><span class="problematic" id="id2">*</span></a>sizes (torch.Size or int…): the desired expanded size</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>More than one element of an expanded tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensors, please clone them first.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>   <span class="c1"># -1 means not changing the size of that dimension</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.expand_as">
<code class="sig-name descname"><span class="pre">expand_as</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.expand_as" title="Permalink to this definition"></a></dt>
<dd><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.expand_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.expand(other.size())</span></code>.</p>
<p>Please see <code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code> for more information about <code class="docutils literal notranslate"><span class="pre">expand</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>other (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The result tensor has the same size</dt><dd><p>as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.expm1">
<code class="sig-name descname"><span class="pre">expm1</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.expm1" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.expm1_">
<code class="sig-name descname"><span class="pre">expm1_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.expm1_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">expm1()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.exponential_">
<code class="sig-name descname"><span class="pre">exponential_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lambd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.exponential_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the exponential distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \lambda e^{-\lambda x}\]</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fill_">
<code class="sig-name descname"><span class="pre">fill_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fill_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with the specified value.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fill_diagonal_">
<code class="sig-name descname"><span class="pre">fill_diagonal_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fill_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fill_diagonal_" title="Permalink to this definition"></a></dt>
<dd><p>Fill the main diagonal of a tensor that has at least 2-dimensions.
When dims&gt;2, all dimensions of input must be of equal length.
This function modifies the input tensor in-place, and returns the input tensor.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>fill_value (Scalar): the fill value
wrap (bool): the diagonal ‘wrapped’ after N columns for tall matrices.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">wrap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fix">
<code class="sig-name descname"><span class="pre">fix</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fix" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fix()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fix_">
<code class="sig-name descname"><span class="pre">fix_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fix_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">fix()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.flatten">
<code class="sig-name descname"><span class="pre">flatten</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.flatten" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.flip">
<code class="sig-name descname"><span class="pre">flip</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.flip" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flip()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fliplr">
<code class="sig-name descname"><span class="pre">fliplr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fliplr" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fliplr()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.flipud">
<code class="sig-name descname"><span class="pre">flipud</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.flipud" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flipud()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.float">
<code class="sig-name descname"><span class="pre">float</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.float" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.float()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float32)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.float_power">
<code class="sig-name descname"><span class="pre">float_power</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.float_power" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.float_power()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.float_power_">
<code class="sig-name descname"><span class="pre">float_power_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.float_power_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">float_power()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.floor">
<code class="sig-name descname"><span class="pre">floor</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.floor" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.floor_">
<code class="sig-name descname"><span class="pre">floor_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.floor_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">floor()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.floor_divide">
<code class="sig-name descname"><span class="pre">floor_divide</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.floor_divide" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor_divide()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.floor_divide_">
<code class="sig-name descname"><span class="pre">floor_divide_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.floor_divide_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">floor_divide()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fmax">
<code class="sig-name descname"><span class="pre">fmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fmax" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmax()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fmin">
<code class="sig-name descname"><span class="pre">fmin</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fmin" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fmod">
<code class="sig-name descname"><span class="pre">fmod</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fmod" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.fmod_">
<code class="sig-name descname"><span class="pre">fmod_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.fmod_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">fmod()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.frac">
<code class="sig-name descname"><span class="pre">frac</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.frac" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.frac_">
<code class="sig-name descname"><span class="pre">frac_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.frac_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">frac()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.frexp">
<code class="sig-name descname"><span class="pre">frexp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">input)</span> <span class="pre">-&gt;</span> <span class="pre">(Tensor</span> <span class="pre">mantissa</span></em>, <em class="sig-param"><span class="pre">Tensor</span> <span class="pre">exponent</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.frexp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frexp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.gather">
<code class="sig-name descname"><span class="pre">gather</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.gather" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gather()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.gcd">
<code class="sig-name descname"><span class="pre">gcd</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.gcd" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gcd()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.gcd_">
<code class="sig-name descname"><span class="pre">gcd_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.gcd_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">gcd()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ge">
<code class="sig-name descname"><span class="pre">ge</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ge" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ge_">
<code class="sig-name descname"><span class="pre">ge_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ge_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.geometric_">
<code class="sig-name descname"><span class="pre">geometric_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.geometric_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the geometric distribution:</p>
<div class="math notranslate nohighlight">
\[f(X=k) = (1 - p)^{k - 1} p\]</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.geqrf">
<code class="sig-name descname"><span class="pre">geqrf</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.geqrf" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ger">
<code class="sig-name descname"><span class="pre">ger</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ger" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ger()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.get_device">
<code class="sig-name descname"><span class="pre">get_device</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">)</span> <span class="pre">-&gt;</span> <span class="pre">Device</span> <span class="pre">ordinal</span> <span class="pre">(Integer</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.get_device" title="Permalink to this definition"></a></dt>
<dd><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
For CPU tensors, this function returns <cite>-1</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="go">-1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.grad">
<code class="sig-name descname"><span class="pre">grad</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="Permalink to this definition"></a></dt>
<dd><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default and becomes a Tensor the first time a call to
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> computes gradients for <code class="docutils literal notranslate"><span class="pre">self</span></code>.
The attribute will then contain the gradients computed and future calls to
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will accumulate (add) gradients into it.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.grad_fn">
<code class="sig-name descname"><span class="pre">grad_fn</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.grad_fn" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.greater">
<code class="sig-name descname"><span class="pre">greater</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.greater" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.greater_">
<code class="sig-name descname"><span class="pre">greater_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.greater_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">greater()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.greater_equal">
<code class="sig-name descname"><span class="pre">greater_equal</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.greater_equal" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater_equal()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.greater_equal_">
<code class="sig-name descname"><span class="pre">greater_equal_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.greater_equal_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">greater_equal()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.gt">
<code class="sig-name descname"><span class="pre">gt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.gt" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.gt_">
<code class="sig-name descname"><span class="pre">gt_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.gt_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.half">
<code class="sig-name descname"><span class="pre">half</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.half" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.half()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float16)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.hardshrink">
<code class="sig-name descname"><span class="pre">hardshrink</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lambd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.hardshrink" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.hardshrink()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.has_names">
<code class="sig-name descname"><span class="pre">has_names</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.has_names" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if any of this tensor’s dimensions are named. Otherwise, is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.heaviside">
<code class="sig-name descname"><span class="pre">heaviside</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.heaviside" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.heaviside()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.heaviside_">
<code class="sig-name descname"><span class="pre">heaviside_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.heaviside_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">heaviside()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.histc">
<code class="sig-name descname"><span class="pre">histc</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.histc" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histc()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.histogram">
<code class="sig-name descname"><span class="pre">histogram</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bins</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">density</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.histogram" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histogram()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.hsplit">
<code class="sig-name descname"><span class="pre">hsplit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size_or_sections</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.hsplit" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hsplit()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.hypot">
<code class="sig-name descname"><span class="pre">hypot</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.hypot" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hypot()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.hypot_">
<code class="sig-name descname"><span class="pre">hypot_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.hypot_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">hypot()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.i0">
<code class="sig-name descname"><span class="pre">i0</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.i0" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.i0()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.i0_">
<code class="sig-name descname"><span class="pre">i0_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.i0_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">i0()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.igamma">
<code class="sig-name descname"><span class="pre">igamma</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.igamma" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.igamma_">
<code class="sig-name descname"><span class="pre">igamma_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.igamma_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">igamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.igammac">
<code class="sig-name descname"><span class="pre">igammac</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.igammac" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igammac()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.igammac_">
<code class="sig-name descname"><span class="pre">igammac_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.igammac_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">igammac()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.imag">
<code class="sig-name descname"><span class="pre">imag</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.imag" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new tensor containing imaginary values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.
The returned tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> share the same underlying storage.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.imag" title="hxtorch.spiking.modules.synapse.Parameter.imag"><code class="xref py py-func docutils literal notranslate"><span class="pre">imag()</span></code></a> is only supported for tensors with complex dtypes.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">imag</span>
<span class="go">tensor([ 0.3553, -0.7896, -0.0633, -0.8119])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_add">
<code class="sig-name descname"><span class="pre">index_add</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_add" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_add_()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_add_">
<code class="sig-name descname"><span class="pre">index_add_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_add_" title="Permalink to this definition"></a></dt>
<dd><p>Accumulate the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> times <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor by adding to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example,
if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, and <code class="docutils literal notranslate"><span class="pre">alpha=-1</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of
<code class="docutils literal notranslate"><span class="pre">source</span></code> is subtracted from the <code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dim" title="hxtorch.spiking.modules.synapse.Parameter.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <code class="docutils literal notranslate"><span class="pre">source</span></code> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<p>For a 3-D tensor the output is given as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">src</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">src</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<dl>
<dt>Note:</dt><dd><p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</dd>
<dt>Args:</dt><dd><p>dim (int): dimension along which to index
index (Tensor): indices of <code class="docutils literal notranslate"><span class="pre">source</span></code> to select from,</p>
<blockquote>
<div><p>should have dtype either <cite>torch.int64</cite> or <cite>torch.int32</cite></p>
</div></blockquote>
<p>source (Tensor): the tensor containing values to add</p>
</dd>
<dt>Keyword args:</dt><dd><p>alpha (Number): the scalar multiplier for <code class="docutils literal notranslate"><span class="pre">source</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[  2.,   3.,   4.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  8.,   9.,  10.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  5.,   6.,   7.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_copy">
<code class="sig-name descname"><span class="pre">index_copy</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_copy" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_copy_()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_copy_">
<code class="sig-name descname"><span class="pre">index_copy_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_copy_" title="Permalink to this definition"></a></dt>
<dd><p>Copies the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by selecting
the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> is copied to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dim" title="hxtorch.spiking.modules.synapse.Parameter.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> contains duplicate entries, multiple elements from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> will be copied to the same index of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. The result
is nondeterministic since it depends on which copy occurs last.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><p>dim (int): dimension along which to index
index (LongTensor): indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> to select from
tensor (Tensor): the tensor containing values to copy</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.,  3.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 7.,  8.,  9.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_fill">
<code class="sig-name descname"><span class="pre">index_fill</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_fill" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_fill_()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_fill_">
<code class="sig-name descname"><span class="pre">index_fill_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_fill_" title="Permalink to this definition"></a></dt>
<dd><p>Fills the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with value <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> by
selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p>
<dl>
<dt>Args:</dt><dd><p>dim (int): dimension along which to index
index (LongTensor): indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to fill in
value (float): the value to fill with</p>
</dd>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[-1.,  2., -1.],</span>
<span class="go">        [-1.,  5., -1.],</span>
<span class="go">        [-1.,  8., -1.]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_put">
<code class="sig-name descname"><span class="pre">index_put</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_put" title="Permalink to this definition"></a></dt>
<dd><p>Out-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">index_put_()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_put_">
<code class="sig-name descname"><span class="pre">index_put_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_put_" title="Permalink to this definition"></a></dt>
<dd><p>Puts values from the tensor <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.values" title="hxtorch.spiking.modules.synapse.Parameter.values"><code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code></a> into the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> using
the indices specified in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.indices" title="hxtorch.spiking.modules.synapse.Parameter.indices"><code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code></a> (which is a tuple of Tensors). The
expression <code class="docutils literal notranslate"><span class="pre">tensor.index_put_(indices,</span> <span class="pre">values)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">tensor[indices]</span> <span class="pre">=</span> <span class="pre">values</span></code>. Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.values" title="hxtorch.spiking.modules.synapse.Parameter.values"><code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code></a> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if indices
contain duplicate elements.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>indices (tuple of LongTensor): tensors used to index into <cite>self</cite>.
values (Tensor): tensor of same dtype as <cite>self</cite>.
accumulate (bool): whether to accumulate into self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_reduce">
<code class="sig-name descname"><span class="pre">index_reduce</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_reduce" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_reduce_">
<code class="sig-name descname"><span class="pre">index_reduce_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_self</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_reduce_" title="Permalink to this definition"></a></dt>
<dd><p>Accumulate the elements of <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor by accumulating to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>
using the reduction given by the <code class="docutils literal notranslate"><span class="pre">reduce</span></code> argument. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>,
<code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, <code class="docutils literal notranslate"><span class="pre">reduce</span> <span class="pre">==</span> <span class="pre">prod</span></code> and <code class="docutils literal notranslate"><span class="pre">include_self</span> <span class="pre">==</span> <span class="pre">True</span></code> then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th
row of <code class="docutils literal notranslate"><span class="pre">source</span></code> is multiplied by the <code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If
<code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=&quot;True&quot;</span></code>, the values in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor are included
in the reduction, otherwise, rows in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor that are accumulated
to are treated as if they were filled with the reduction identites.</p>
<p>The <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dim" title="hxtorch.spiking.modules.synapse.Parameter.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <code class="docutils literal notranslate"><span class="pre">source</span></code> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<p>For a 3-D tensor with <code class="xref py py-obj docutils literal notranslate"><span class="pre">reduce=&quot;prod&quot;</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=True</span></code> the
output is given as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note:</dt><dd><p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function only supports floating point tensors.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is in beta and may change in the near future.</p>
</div>
<dl>
<dt>Args:</dt><dd><p>dim (int): dimension along which to index
index (Tensor): indices of <code class="docutils literal notranslate"><span class="pre">source</span></code> to select from,</p>
<blockquote>
<div><p>should have dtype either <cite>torch.int64</cite> or <cite>torch.int32</cite></p>
</div></blockquote>
<p>source (FloatTensor): the tensor containing values to accumulate
reduce (str): the reduction operation to apply</p>
<blockquote>
<div><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>)</p>
</div></blockquote>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>include_self (bool): whether the elements from the <code class="docutils literal notranslate"><span class="pre">self</span></code> tensor are</dt><dd><p>included in the reduction</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_reduce_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s1">&#39;prod&#39;</span><span class="p">)</span>
<span class="go">tensor([[20., 44., 72.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [14., 16., 18.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [ 8., 10., 12.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_reduce_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s1">&#39;prod&#39;</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">tensor([[10., 22., 36.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [ 7.,  8.,  9.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.index_select">
<code class="sig-name descname"><span class="pre">index_select</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.index_select" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.index_select()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.indices">
<code class="sig-name descname"><span class="pre">indices</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.indices" title="Permalink to this definition"></a></dt>
<dd><p>Return the indices tensor of a <span class="xref std std-ref">sparse COO tensor</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
<p>See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.values()</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method can only be called on a coalesced sparse tensor. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.coalesce()</span></code> for details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.inner">
<code class="sig-name descname"><span class="pre">inner</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.inner" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inner()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.int">
<code class="sig-name descname"><span class="pre">int</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.int()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int32)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.int_repr">
<code class="sig-name descname"><span class="pre">int_repr</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.int_repr" title="Permalink to this definition"></a></dt>
<dd><p>Given a quantized Tensor,
<code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the
underlying uint8_t values of the given Tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.inverse">
<code class="sig-name descname"><span class="pre">inverse</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.inverse" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inverse()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ipu">
<code class="sig-name descname"><span class="pre">ipu</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ipu" title="Permalink to this definition"></a></dt>
<dd><p>Returns a copy of this object in IPU memory.</p>
<p>If this object is already in IPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>): The destination IPU device.</dt><dd><p>Defaults to the current IPU device.</p>
</dd>
<dt>non_blocking (bool): If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,</dt><dd><p>the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_coalesced">
<code class="sig-name descname"><span class="pre">is_coalesced</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_coalesced" title="Permalink to this definition"></a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a <span class="xref std std-ref">sparse COO tensor</span> that is coalesced, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
<p>See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.coalesce" title="hxtorch.spiking.modules.synapse.Parameter.coalesce"><code class="xref py py-meth docutils literal notranslate"><span class="pre">coalesce()</span></code></a> and <span class="xref std std-ref">uncoalesced tensors</span>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_complex">
<code class="sig-name descname"><span class="pre">is_complex</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_complex" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a complex data type.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_conj">
<code class="sig-name descname"><span class="pre">is_conj</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_conj" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if the conjugate bit of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is set to true.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_contiguous">
<code class="sig-name descname"><span class="pre">is_contiguous</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_contiguous" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous in memory in the order specified
by memory format.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): Specifies memory allocation</dt><dd><p>order. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_cpu">
<code class="sig-name descname"><span class="pre">is_cpu</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_cpu" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the CPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_cuda">
<code class="sig-name descname"><span class="pre">is_cuda</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_cuda" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_distributed">
<code class="sig-name descname"><span class="pre">is_distributed</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_distributed" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_floating_point">
<code class="sig-name descname"><span class="pre">is_floating_point</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_floating_point" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a floating point data type.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_inference">
<code class="sig-name descname"><span class="pre">is_inference</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_inference" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.is_inference()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_ipu">
<code class="sig-name descname"><span class="pre">is_ipu</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_ipu" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the IPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_leaf">
<code class="sig-name descname"><span class="pre">is_leaf</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_leaf" title="Permalink to this definition"></a></dt>
<dd><p>All Tensors that have <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">False</span></code> will be leaf Tensors by convention.</p>
<p>For Tensors that have <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">True</span></code>, they will be leaf Tensors if they were
created by the user. This means that they are not the result of an operation and so
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad_fn" title="hxtorch.spiking.modules.synapse.Parameter.grad_fn"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_fn</span></code></a> is None.</p>
<p>Only leaf Tensors will have their <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated during a call to <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>.
To get <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated for non-leaf Tensors, you can use <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.retain_grad" title="hxtorch.spiking.modules.synapse.Parameter.retain_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">retain_grad()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># b was created by the operation that cast a cpu Tensor into a cuda Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># c was created by the addition operation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># e requires gradients and has no operations creating it</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># f requires grad, has no operation creating it</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_meta">
<code class="sig-name descname"><span class="pre">is_meta</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_meta" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is a meta tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.  Meta tensors
are like normal tensors, but they carry no data.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_mkldnn">
<code class="sig-name descname"><span class="pre">is_mkldnn</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_mkldnn" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_mps">
<code class="sig-name descname"><span class="pre">is_mps</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_mps" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the MPS device, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_neg">
<code class="sig-name descname"><span class="pre">is_neg</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_neg" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if the negative bit of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is set to true.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_nested">
<code class="sig-name descname"><span class="pre">is_nested</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_nested" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_nonzero">
<code class="sig-name descname"><span class="pre">is_nonzero</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_nonzero" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_ort">
<code class="sig-name descname"><span class="pre">is_ort</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_ort" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_pinned">
<code class="sig-name descname"><span class="pre">is_pinned</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_pinned" title="Permalink to this definition"></a></dt>
<dd><p>Returns true if this tensor resides in pinned memory.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_quantized">
<code class="sig-name descname"><span class="pre">is_quantized</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_quantized" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is quantized, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_same_size">
<code class="sig-name descname"><span class="pre">is_same_size</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_same_size" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_set_to">
<code class="sig-name descname"><span class="pre">is_set_to</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_set_to" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if both tensors are pointing to the exact same memory (same
storage, offset, size and stride).</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_shared">
<code class="sig-name descname"><span class="pre">is_shared</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_shared" title="Permalink to this definition"></a></dt>
<dd><p>Checks if tensor is in shared memory.</p>
<p>This is always <code class="docutils literal notranslate"><span class="pre">True</span></code> for CUDA tensors.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_signed">
<code class="sig-name descname"><span class="pre">is_signed</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_signed" title="Permalink to this definition"></a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a signed data type.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_sparse">
<code class="sig-name descname"><span class="pre">is_sparse</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_sparse" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_sparse_csr">
<code class="sig-name descname"><span class="pre">is_sparse_csr</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_sparse_csr" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse CSR storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_vulkan">
<code class="sig-name descname"><span class="pre">is_vulkan</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_vulkan" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.is_xpu">
<code class="sig-name descname"><span class="pre">is_xpu</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.is_xpu" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the XPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.isclose">
<code class="sig-name descname"><span class="pre">isclose</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rtol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">atol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equal_nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.isclose" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isclose()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.isfinite">
<code class="sig-name descname"><span class="pre">isfinite</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.isfinite" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isfinite()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.isinf">
<code class="sig-name descname"><span class="pre">isinf</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.isinf" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isinf()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.isnan">
<code class="sig-name descname"><span class="pre">isnan</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.isnan" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isnan()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.isneginf">
<code class="sig-name descname"><span class="pre">isneginf</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.isneginf" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isneginf()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.isposinf">
<code class="sig-name descname"><span class="pre">isposinf</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.isposinf" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isposinf()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.isreal">
<code class="sig-name descname"><span class="pre">isreal</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.isreal" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isreal()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.istft">
<code class="sig-name descname"><span class="pre">istft</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_fft</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">hop_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">win_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onesided</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_complex</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.istft" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.istft()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.item">
<code class="sig-name descname"><span class="pre">item</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">number</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.item" title="Permalink to this definition"></a></dt>
<dd><p>Returns the value of this tensor as a standard Python number. This only works
for tensors with one element. For other cases, see <code class="xref py py-meth docutils literal notranslate"><span class="pre">tolist()</span></code>.</p>
<p>This operation is not differentiable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.kron">
<code class="sig-name descname"><span class="pre">kron</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.kron" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kron()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.kthvalue">
<code class="sig-name descname"><span class="pre">kthvalue</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.kthvalue" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kthvalue()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.layout">
<code class="sig-name descname"><span class="pre">layout</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.layout" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lcm">
<code class="sig-name descname"><span class="pre">lcm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lcm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lcm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lcm_">
<code class="sig-name descname"><span class="pre">lcm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lcm_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lcm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ldexp">
<code class="sig-name descname"><span class="pre">ldexp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ldexp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ldexp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ldexp_">
<code class="sig-name descname"><span class="pre">ldexp_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ldexp_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ldexp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.le">
<code class="sig-name descname"><span class="pre">le</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.le" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.le_">
<code class="sig-name descname"><span class="pre">le_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.le_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lerp">
<code class="sig-name descname"><span class="pre">lerp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">end</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lerp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lerp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lerp_">
<code class="sig-name descname"><span class="pre">lerp_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">end</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lerp_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lerp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.less">
<code class="sig-name descname"><span class="pre">less</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.less" title="Permalink to this definition"></a></dt>
<dd><p>lt(other) -&gt; Tensor</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.less()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.less_">
<code class="sig-name descname"><span class="pre">less_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.less_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">less()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.less_equal">
<code class="sig-name descname"><span class="pre">less_equal</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.less_equal" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.less_equal()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.less_equal_">
<code class="sig-name descname"><span class="pre">less_equal_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.less_equal_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">less_equal()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lgamma">
<code class="sig-name descname"><span class="pre">lgamma</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lgamma" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lgamma_">
<code class="sig-name descname"><span class="pre">lgamma_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lgamma_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lgamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log">
<code class="sig-name descname"><span class="pre">log</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log10">
<code class="sig-name descname"><span class="pre">log10</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log10" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log10_">
<code class="sig-name descname"><span class="pre">log10_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log10_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log10()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log1p">
<code class="sig-name descname"><span class="pre">log1p</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log1p" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log1p_">
<code class="sig-name descname"><span class="pre">log1p_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log1p_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log2">
<code class="sig-name descname"><span class="pre">log2</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log2" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log2_">
<code class="sig-name descname"><span class="pre">log2_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log2_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log_">
<code class="sig-name descname"><span class="pre">log_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log_normal_">
<code class="sig-name descname"><span class="pre">log_normal_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log_normal_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers samples from the log-normal distribution
parameterized by the given mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation
<span class="math notranslate nohighlight">\(\sigma\)</span>. Note that <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mean" title="hxtorch.spiking.modules.synapse.Parameter.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.std" title="hxtorch.spiking.modules.synapse.Parameter.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a> are the mean and
standard deviation of the underlying normal distribution, and not of the
returned distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}\]</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.log_softmax">
<code class="sig-name descname"><span class="pre">log_softmax</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.log_softmax" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logaddexp">
<code class="sig-name descname"><span class="pre">logaddexp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logaddexp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logaddexp2">
<code class="sig-name descname"><span class="pre">logaddexp2</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logaddexp2" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp2()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logcumsumexp">
<code class="sig-name descname"><span class="pre">logcumsumexp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logcumsumexp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logcumsumexp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logdet">
<code class="sig-name descname"><span class="pre">logdet</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logdet" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logdet()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_and">
<code class="sig-name descname"><span class="pre">logical_and</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_and" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_and()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_and_">
<code class="sig-name descname"><span class="pre">logical_and_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_and_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_and()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_not">
<code class="sig-name descname"><span class="pre">logical_not</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_not" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_not()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_not_">
<code class="sig-name descname"><span class="pre">logical_not_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_not_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_not()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_or">
<code class="sig-name descname"><span class="pre">logical_or</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_or" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_or()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_or_">
<code class="sig-name descname"><span class="pre">logical_or_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_or_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_or()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_xor">
<code class="sig-name descname"><span class="pre">logical_xor</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_xor" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_xor()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logical_xor_">
<code class="sig-name descname"><span class="pre">logical_xor_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logical_xor_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_xor()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logit">
<code class="sig-name descname"><span class="pre">logit</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logit" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logit()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logit_">
<code class="sig-name descname"><span class="pre">logit_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logit_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">logit()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.logsumexp">
<code class="sig-name descname"><span class="pre">logsumexp</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.logsumexp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logsumexp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.long">
<code class="sig-name descname"><span class="pre">long</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.long" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.long()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int64)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lstsq">
<code class="sig-name descname"><span class="pre">lstsq</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lstsq" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lt">
<code class="sig-name descname"><span class="pre">lt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lt" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lt_">
<code class="sig-name descname"><span class="pre">lt_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lt_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lu">
<code class="sig-name descname"><span class="pre">lu</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pivot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_infos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lu" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.lu_solve">
<code class="sig-name descname"><span class="pre">lu_solve</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">LU_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">LU_pivots</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.lu_solve" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu_solve()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mH">
<code class="sig-name descname"><span class="pre">mH</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mH" title="Permalink to this definition"></a></dt>
<dd><p>Accessing this property is equivalent to calling <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.adjoint" title="hxtorch.spiking.modules.synapse.Parameter.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mT">
<code class="sig-name descname"><span class="pre">mT</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mT" title="Permalink to this definition"></a></dt>
<dd><p>Returns a view of this tensor with the last two dimensions transposed.</p>
<p><code class="docutils literal notranslate"><span class="pre">x.mT</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.transpose(-2,</span> <span class="pre">-1)</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.map2_">
<code class="sig-name descname"><span class="pre">map2_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.map2_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.map_">
<code class="sig-name descname"><span class="pre">map_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.map_" title="Permalink to this definition"></a></dt>
<dd><p>Applies <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> and stores the results in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and
the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> must be <span class="xref std std-ref">broadcastable</span>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> should have the signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">number</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.masked_fill">
<code class="sig-name descname"><span class="pre">masked_fill</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.masked_fill" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_fill_()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.masked_fill_">
<code class="sig-name descname"><span class="pre">masked_fill_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.masked_fill_" title="Permalink to this definition"></a></dt>
<dd><p>Fills elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is
True. The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be
<span class="xref std std-ref">broadcastable</span> with the shape of the underlying
tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>mask (BoolTensor): the boolean mask
value (float): the value to fill in with</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.masked_scatter">
<code class="sig-name descname"><span class="pre">masked_scatter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.masked_scatter" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_scatter_()</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The inputs <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>
<span class="xref std std-ref">broadcast</span>.</p>
</div>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">source</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">masked_scatter</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
<span class="go">tensor([[0, 0, 0, 0, 1],</span>
<span class="go">        [2, 3, 0, 4, 5]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.masked_scatter_">
<code class="sig-name descname"><span class="pre">masked_scatter_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.masked_scatter_" title="Permalink to this definition"></a></dt>
<dd><p>Copies elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor at positions where
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is True. Elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> are copied into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
starting at position 0 of <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> and continuing in order one-by-one for each
occurrence of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> being True.
The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be <span class="xref std std-ref">broadcastable</span>
with the shape of the underlying tensor. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> should have at least
as many elements as the number of ones in <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>mask (BoolTensor): the boolean mask
source (Tensor): the tensor to copy from</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> operates on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor, not on the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> tensor.</p>
</div>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">source</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">masked_scatter_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
<span class="go">tensor([[0, 0, 0, 0, 1],</span>
<span class="go">        [2, 3, 0, 4, 5]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.masked_select">
<code class="sig-name descname"><span class="pre">masked_select</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.masked_select" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.masked_select()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.matmul">
<code class="sig-name descname"><span class="pre">matmul</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.matmul" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.matrix_exp">
<code class="sig-name descname"><span class="pre">matrix_exp</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.matrix_exp" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matrix_exp()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.matrix_power">
<code class="sig-name descname"><span class="pre">matrix_power</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.matrix_power" title="Permalink to this definition"></a></dt>
<dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">matrix_power()</span></code> is deprecated, use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_power()</span></code> instead.</p>
</div>
<p>Alias for <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_power()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.max">
<code class="sig-name descname"><span class="pre">max</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.max" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.maximum">
<code class="sig-name descname"><span class="pre">maximum</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.maximum" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.maximum()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mean">
<code class="sig-name descname"><span class="pre">mean</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mean" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.median">
<code class="sig-name descname"><span class="pre">median</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.median" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.median()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.min">
<code class="sig-name descname"><span class="pre">min</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.min" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.minimum">
<code class="sig-name descname"><span class="pre">minimum</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.minimum" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.minimum()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mm">
<code class="sig-name descname"><span class="pre">mm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mode">
<code class="sig-name descname"><span class="pre">mode</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mode" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mode()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.moveaxis">
<code class="sig-name descname"><span class="pre">moveaxis</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destination</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.moveaxis" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.moveaxis()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.movedim">
<code class="sig-name descname"><span class="pre">movedim</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destination</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.movedim" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.movedim()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.msort">
<code class="sig-name descname"><span class="pre">msort</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.msort" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.msort()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mul">
<code class="sig-name descname"><span class="pre">mul</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mul" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mul_">
<code class="sig-name descname"><span class="pre">mul_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mul_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.multinomial">
<code class="sig-name descname"><span class="pre">multinomial</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replacement</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.multinomial" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.multiply">
<code class="sig-name descname"><span class="pre">multiply</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.multiply" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multiply()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.multiply_">
<code class="sig-name descname"><span class="pre">multiply_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.multiply_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">multiply()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mv">
<code class="sig-name descname"><span class="pre">mv</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mv" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mvlgamma">
<code class="sig-name descname"><span class="pre">mvlgamma</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mvlgamma" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mvlgamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.mvlgamma_">
<code class="sig-name descname"><span class="pre">mvlgamma_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.mvlgamma_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">mvlgamma()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.name">
<code class="sig-name descname"><span class="pre">name</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.names">
<code class="sig-name descname"><span class="pre">names</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="Permalink to this definition"></a></dt>
<dd><p>Stores names for each of this tensor’s dimensions.</p>
<p><code class="docutils literal notranslate"><span class="pre">names[idx]</span></code> corresponds to the name of tensor dimension <code class="docutils literal notranslate"><span class="pre">idx</span></code>.
Names are either a string if the dimension is named or <code class="docutils literal notranslate"><span class="pre">None</span></code> if the
dimension is unnamed.</p>
<p>Dimension names may contain characters or underscore. Furthermore, a dimension
name must be a valid Python variable name (i.e., does not start with underscore).</p>
<p>Tensors may not have two named dimensions with the same name.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nan_to_num">
<code class="sig-name descname"><span class="pre">nan_to_num</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posinf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neginf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nan_to_num" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nan_to_num()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nan_to_num_">
<code class="sig-name descname"><span class="pre">nan_to_num_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posinf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neginf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nan_to_num_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">nan_to_num()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nanmean">
<code class="sig-name descname"><span class="pre">nanmean</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nanmean" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmean()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nanmedian">
<code class="sig-name descname"><span class="pre">nanmedian</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nanmedian" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmedian()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nanquantile">
<code class="sig-name descname"><span class="pre">nanquantile</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">interpolation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nanquantile" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanquantile()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nansum">
<code class="sig-name descname"><span class="pre">nansum</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nansum" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nansum()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.narrow">
<code class="sig-name descname"><span class="pre">narrow</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.narrow" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.narrow_copy">
<code class="sig-name descname"><span class="pre">narrow_copy</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.narrow_copy" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow_copy()</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ndim">
<code class="sig-name descname"><span class="pre">ndim</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ndim" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ndimension">
<code class="sig-name descname"><span class="pre">ndimension</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ndimension" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ne">
<code class="sig-name descname"><span class="pre">ne</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ne" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ne_">
<code class="sig-name descname"><span class="pre">ne_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ne_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.neg">
<code class="sig-name descname"><span class="pre">neg</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.neg" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.neg_">
<code class="sig-name descname"><span class="pre">neg_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.neg_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.negative">
<code class="sig-name descname"><span class="pre">negative</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.negative" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.negative()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.negative_">
<code class="sig-name descname"><span class="pre">negative_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.negative_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">negative()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nelement">
<code class="sig-name descname"><span class="pre">nelement</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nelement" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.new">
<code class="sig-name descname"><span class="pre">new</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.new" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.new_empty">
<code class="sig-name descname"><span class="pre">new_empty</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.new_empty" title="Permalink to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with uninitialized data.
By default, the returned Tensor has the same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (int…): a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the</dt><dd><p>shape of the output tensor.</p>
</dd>
</dl>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired type of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> as this tensor.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>pin_memory (bool, optional): If set, returned tensor would be allocated in</dt><dd><p>the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],</span>
<span class="go">        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.new_empty_strided">
<code class="sig-name descname"><span class="pre">new_empty_strided</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.new_empty_strided" title="Permalink to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> and strides <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.stride" title="hxtorch.spiking.modules.synapse.Parameter.stride"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code></a> filled with
uninitialized data. By default, the returned Tensor has the same
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (int…): a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the</dt><dd><p>shape of the output tensor.</p>
</dd>
</dl>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired type of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> as this tensor.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>pin_memory (bool, optional): If set, returned tensor would be allocated in</dt><dd><p>the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_empty_strided</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],</span>
<span class="go">        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.new_full">
<code class="sig-name descname"><span class="pre">new_full</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.new_full" title="Permalink to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.
By default, the returned Tensor has the same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>fill_value (scalar): the number to fill the output tensor with.</p>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired type of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> as this tensor.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>pin_memory (bool, optional): If set, returned tensor would be allocated in</dt><dd><p>the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">3.141592</span><span class="p">)</span>
<span class="go">tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.new_ones">
<code class="sig-name descname"><span class="pre">new_ones</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.new_ones" title="Permalink to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">1</span></code>.
By default, the returned Tensor has the same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (int…): a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the</dt><dd><p>shape of the output tensor.</p>
</dd>
</dl>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired type of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> as this tensor.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>pin_memory (bool, optional): If set, returned tensor would be allocated in</dt><dd><p>the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 1,  1,  1],</span>
<span class="go">        [ 1,  1,  1]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.new_tensor">
<code class="sig-name descname"><span class="pre">new_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.new_tensor" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new Tensor with <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.data" title="hxtorch.spiking.modules.synapse.Parameter.data"><code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code></a> as the tensor data.
By default, the returned Tensor has the same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_tensor" title="hxtorch.spiking.modules.synapse.Parameter.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> always copies <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.data" title="hxtorch.spiking.modules.synapse.Parameter.data"><code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code></a>. If you have a Tensor
<code class="docutils literal notranslate"><span class="pre">data</span></code> and want to avoid a copy, use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code>
or <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.detach()</span></code>.
If you have a numpy array and want to avoid a copy, use
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.from_numpy()</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When data is a tensor <cite>x</cite>, <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.new_tensor" title="hxtorch.spiking.modules.synapse.Parameter.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> reads out ‘the data’ from whatever it is passed,
and constructs a leaf variable. Therefore <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach()</span></code>
and <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x,</span> <span class="pre">requires_grad=True)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach().requires_grad_(True)</span></code>.
The equivalents using <code class="docutils literal notranslate"><span class="pre">clone()</span></code> and <code class="docutils literal notranslate"><span class="pre">detach()</span></code> are recommended.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><p>data (array_like): The returned Tensor copies <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.data" title="hxtorch.spiking.modules.synapse.Parameter.data"><code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code></a>.</p>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired type of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> as this tensor.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>pin_memory (bool, optional): If set, returned tensor would be allocated in</dt><dd><p>the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="go">tensor([[ 0,  1],</span>
<span class="go">        [ 2,  3]], dtype=torch.int8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.new_zeros">
<code class="sig-name descname"><span class="pre">new_zeros</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.new_zeros" title="Permalink to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">0</span></code>.
By default, the returned Tensor has the same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (int…): a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the</dt><dd><p>shape of the output tensor.</p>
</dd>
</dl>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired type of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> as this tensor.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if None, same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as this tensor.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>pin_memory (bool, optional): If set, returned tensor would be allocated in</dt><dd><p>the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nextafter">
<code class="sig-name descname"><span class="pre">nextafter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nextafter" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nextafter()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nextafter_">
<code class="sig-name descname"><span class="pre">nextafter_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nextafter_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">nextafter()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.nonzero">
<code class="sig-name descname"><span class="pre">nonzero</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">LongTensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.nonzero" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nonzero()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.norm">
<code class="sig-name descname"><span class="pre">norm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.float" title="hxtorch.spiking.modules.synapse.Parameter.float"><span class="pre">float</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'fro'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.norm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.normal_">
<code class="sig-name descname"><span class="pre">normal_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.normal_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements samples from the normal distribution
parameterized by <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.mean" title="hxtorch.spiking.modules.synapse.Parameter.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.std" title="hxtorch.spiking.modules.synapse.Parameter.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.not_equal">
<code class="sig-name descname"><span class="pre">not_equal</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.not_equal" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.not_equal()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.not_equal_">
<code class="sig-name descname"><span class="pre">not_equal_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.not_equal_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">not_equal()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.numel">
<code class="sig-name descname"><span class="pre">numel</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.numel" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.numel()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.numpy">
<code class="sig-name descname"><span class="pre">numpy</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">numpy.ndarray</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.numpy" title="Permalink to this definition"></a></dt>
<dd><p>Returns the tensor as a NumPy <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">force</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> (the default), the conversion
is performed only if the tensor is on the CPU, does not require grad,
does not have its conjugate bit set, and is a dtype and layout that
NumPy supports. The returned ndarray and the tensor will share their
storage, so changes to the tensor will be reflected in the ndarray
and vice versa.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">force</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> this is equivalent to
calling <code class="docutils literal notranslate"><span class="pre">t.detach().cpu().resolve_conj().resolve_neg().numpy()</span></code>.
If the tensor isn’t on the CPU or the conjugate or negative bit is set,
the tensor won’t share its storage with the returned ndarray.
Setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">force</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> can be a useful shorthand.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>force (bool): if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the ndarray may be a copy of the tensor</dt><dd><p>instead of always sharing memory, defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.orgqr">
<code class="sig-name descname"><span class="pre">orgqr</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.orgqr" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.orgqr()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ormqr">
<code class="sig-name descname"><span class="pre">ormqr</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">left</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transpose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ormqr" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ormqr()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.outer">
<code class="sig-name descname"><span class="pre">outer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.outer" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.outer()</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.output_nr">
<code class="sig-name descname"><span class="pre">output_nr</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.output_nr" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.permute">
<code class="sig-name descname"><span class="pre">permute</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.permute" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.permute()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.pin_memory">
<code class="sig-name descname"><span class="pre">pin_memory</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.pin_memory" title="Permalink to this definition"></a></dt>
<dd><p>Copies the tensor to pinned memory, if it’s not already pinned.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.pinverse">
<code class="sig-name descname"><span class="pre">pinverse</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.pinverse" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pinverse()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.polygamma">
<code class="sig-name descname"><span class="pre">polygamma</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.polygamma" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.polygamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.polygamma_">
<code class="sig-name descname"><span class="pre">polygamma_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.polygamma_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">polygamma()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.positive">
<code class="sig-name descname"><span class="pre">positive</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.positive" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.positive()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.pow">
<code class="sig-name descname"><span class="pre">pow</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.pow" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.pow_">
<code class="sig-name descname"><span class="pre">pow_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.pow_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.prelu">
<code class="sig-name descname"><span class="pre">prelu</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.prelu" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.prod">
<code class="sig-name descname"><span class="pre">prod</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.prod" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.prod()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.put">
<code class="sig-name descname"><span class="pre">put</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.put" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.put_()</span></code>.
<cite>input</cite> corresponds to <cite>self</cite> in <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.put_()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.put_">
<code class="sig-name descname"><span class="pre">put_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.put_" title="Permalink to this definition"></a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into the positions specified by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For the purpose of indexing, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is treated as if
it were a 1-D tensor.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> need to have the same number of elements, but not necessarily
the same shape.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>
contain duplicate elements.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>index (LongTensor): the indices into self
source (Tensor): the tensor containing values to copy from
accumulate (bool): whether to accumulate into self</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>                    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">put_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>
<span class="go">tensor([[  4,   9,   5],</span>
<span class="go">        [ 10,   7,   8]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.q_per_channel_axis">
<code class="sig-name descname"><span class="pre">q_per_channel_axis</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.q_per_channel_axis" title="Permalink to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns the index of dimension on which per-channel quantization is applied.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.q_per_channel_scales">
<code class="sig-name descname"><span class="pre">q_per_channel_scales</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.q_per_channel_scales" title="Permalink to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns a Tensor of scales of the underlying quantizer. It has the number of
elements that matches the corresponding dimensions (from q_per_channel_axis) of
the tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.q_per_channel_zero_points">
<code class="sig-name descname"><span class="pre">q_per_channel_zero_points</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.q_per_channel_zero_points" title="Permalink to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns a tensor of zero_points of the underlying quantizer. It has the number of
elements that matches the corresponding dimensions (from q_per_channel_axis) of
the tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.q_scale">
<code class="sig-name descname"><span class="pre">q_scale</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.float" title="hxtorch.spiking.modules.synapse.Parameter.float"><span class="pre">float</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.q_scale" title="Permalink to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear(affine) quantization,
returns the scale of the underlying quantizer().</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.q_zero_point">
<code class="sig-name descname"><span class="pre">q_zero_point</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.q_zero_point" title="Permalink to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear(affine) quantization,
returns the zero_point of the underlying quantizer().</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.qr">
<code class="sig-name descname"><span class="pre">qr</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">some</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.qr" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.qscheme">
<code class="sig-name descname"><span class="pre">qscheme</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.qscheme</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.qscheme" title="Permalink to this definition"></a></dt>
<dd><p>Returns the quantization scheme of a given QTensor.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.quantile">
<code class="sig-name descname"><span class="pre">quantile</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">interpolation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.quantile" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantile()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.rad2deg">
<code class="sig-name descname"><span class="pre">rad2deg</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.rad2deg" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rad2deg()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.rad2deg_">
<code class="sig-name descname"><span class="pre">rad2deg_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.rad2deg_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">rad2deg()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.random_">
<code class="sig-name descname"><span class="pre">random_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">from=0</span></em>, <em class="sig-param"><span class="pre">to=None</span></em>, <em class="sig-param"><span class="pre">*</span></em>, <em class="sig-param"><span class="pre">generator=None</span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.random_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the discrete uniform
distribution over <code class="docutils literal notranslate"><span class="pre">[from,</span> <span class="pre">to</span> <span class="pre">-</span> <span class="pre">1]</span></code>. If not specified, the values are usually
only bounded by <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s data type. However, for floating point
types, if unspecified, range will be <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^mantissa]</span></code> to ensure that every
value is representable. For example, <cite>torch.tensor(1, dtype=torch.double).random_()</cite>
will be uniform in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^53]</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.ravel">
<code class="sig-name descname"><span class="pre">ravel</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.ravel" title="Permalink to this definition"></a></dt>
<dd><p>see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ravel()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.real">
<code class="sig-name descname"><span class="pre">real</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.real" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new tensor containing real values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor for a complex-valued input tensor.
The returned tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> share the same underlying storage.</p>
<p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a real-valued tensor tensor.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">real</span>
<span class="go">tensor([ 0.3100, -0.5445, -1.6492, -0.0638])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.reciprocal">
<code class="sig-name descname"><span class="pre">reciprocal</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.reciprocal" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.reciprocal_">
<code class="sig-name descname"><span class="pre">reciprocal_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.reciprocal_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">reciprocal()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.record_stream">
<code class="sig-name descname"><span class="pre">record_stream</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.record_stream" title="Permalink to this definition"></a></dt>
<dd><p>Ensures that the tensor memory is not reused for another tensor until all
current work queued on <code class="xref py py-attr docutils literal notranslate"><span class="pre">stream</span></code> are complete.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The caching allocator is aware of only the stream where a tensor was
allocated. Due to the awareness, it already correctly manages the life
cycle of tensors on only one stream. But if a tensor is used on a stream
different from the stream of origin, the allocator might reuse the memory
unexpectedly. Calling this method lets the allocator know which streams
have used the tensor.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.refine_names">
<code class="sig-name descname"><span class="pre">refine_names</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.refine_names" title="Permalink to this definition"></a></dt>
<dd><p>Refines the dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> according to <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>.</p>
<p>Refining is a special case of renaming that “lifts” unnamed dimensions.
A <code class="docutils literal notranslate"><span class="pre">None</span></code> dim can be refined to have any name; a named dim can only be
refined to have the same name.</p>
<p>Because named tensors can coexist with unnamed tensors, refining names
gives a nice way to write named-tensor-aware code that works with both
named and unnamed tensors.</p>
<p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> may contain up to one Ellipsis (<code class="docutils literal notranslate"><span class="pre">...</span></code>).
The Ellipsis is expanded greedily; it is expanded in-place to fill
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> to the same length as <code class="docutils literal notranslate"><span class="pre">self.dim()</span></code> using names from the
corresponding indices of <code class="docutils literal notranslate"><span class="pre">self.names</span></code>.</p>
<p>Python 2 does not support Ellipsis but one may use a string literal
instead (<code class="docutils literal notranslate"><span class="pre">'...'</span></code>).</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>names (iterable of str): The desired names of the output tensor. May</dt><dd><p>contain up to one Ellipsis.</p>
</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;A&#39;, None, None, &#39;B&#39;, &#39;C&#39;)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.register_hook">
<code class="sig-name descname"><span class="pre">register_hook</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.register_hook" title="Permalink to this definition"></a></dt>
<dd><p>Registers a backward hook.</p>
<p>The hook will be called every time a gradient with respect to the
Tensor is computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify its argument, but it can optionally return
a new gradient which will be used in place of <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>.</p>
<p>This function returns a handle with a method <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">backward-hooks-execution</span> for more information on how when this hook
is executed, and how its execution is ordered relative to other hooks.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="k">lambda</span> <span class="n">grad</span><span class="p">:</span> <span class="n">grad</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># double the gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">grad</span>

<span class="go"> 2</span>
<span class="go"> 4</span>
<span class="go"> 6</span>
<span class="go">[torch.FloatTensor of size (3,)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># removes the hook</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.reinforce">
<code class="sig-name descname"><span class="pre">reinforce</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reward</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.reinforce" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.relu">
<code class="sig-name descname"><span class="pre">relu</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.relu" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.relu_">
<code class="sig-name descname"><span class="pre">relu_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.relu_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.remainder">
<code class="sig-name descname"><span class="pre">remainder</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.remainder" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.remainder()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.remainder_">
<code class="sig-name descname"><span class="pre">remainder_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.remainder_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">remainder()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.rename">
<code class="sig-name descname"><span class="pre">rename</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">rename_map</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.rename" title="Permalink to this definition"></a></dt>
<dd><p>Renames dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>There are two main usages:</p>
<p><code class="docutils literal notranslate"><span class="pre">self.rename(**rename_map)</span></code> returns a view on tensor that has dims
renamed as specified in the mapping <code class="xref py py-attr docutils literal notranslate"><span class="pre">rename_map</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">self.rename(*names)</span></code> returns a view on tensor, renaming all
dimensions positionally using <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>.
Use <code class="docutils literal notranslate"><span class="pre">self.rename(None)</span></code> to drop names on a tensor.</p>
<p>One cannot specify both positional args <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.names" title="hxtorch.spiking.modules.synapse.Parameter.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> and keyword args
<code class="xref py py-attr docutils literal notranslate"><span class="pre">rename_map</span></code>.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="s1">&#39;channels&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;batch&#39;, &#39;channels&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(None, None, None, None)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;channel&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.rename_">
<code class="sig-name descname"><span class="pre">rename_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">rename_map</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.rename_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">rename()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.renorm">
<code class="sig-name descname"><span class="pre">renorm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxnorm</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.renorm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.renorm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.renorm_">
<code class="sig-name descname"><span class="pre">renorm_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxnorm</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.renorm_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">renorm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.repeat">
<code class="sig-name descname"><span class="pre">repeat</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sizes</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.repeat" title="Permalink to this definition"></a></dt>
<dd><p>Repeats this tensor along the specified dimensions.</p>
<p>Unlike <code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code>, this function copies the tensor’s data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">repeat()</span></code> behaves differently from
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html">numpy.repeat</a>,
but is more similar to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html">numpy.tile</a>.
For the operator similar to <cite>numpy.repeat</cite>, see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code>.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>sizes (torch.Size or int…): The number of times to repeat this tensor along each</dt><dd><p>dimension</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.repeat_interleave">
<code class="sig-name descname"><span class="pre">repeat_interleave</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repeats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.repeat_interleave" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.requires_grad">
<code class="sig-name descname"><span class="pre">requires_grad</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this Tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The fact that gradients need to be computed for a Tensor do not mean that the <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>
attribute will be populated, see <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.is_leaf" title="hxtorch.spiking.modules.synapse.Parameter.is_leaf"><code class="xref py py-attr docutils literal notranslate"><span class="pre">is_leaf</span></code></a> for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.requires_grad_">
<code class="sig-name descname"><span class="pre">requires_grad_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad_" title="Permalink to this definition"></a></dt>
<dd><p>Change if autograd should record operations on this tensor: sets this tensor’s
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> attribute in-place. Returns this tensor.</p>
<p><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.requires_grad_" title="hxtorch.spiking.modules.synapse.Parameter.requires_grad_"><code class="xref py py-func docutils literal notranslate"><span class="pre">requires_grad_()</span></code></a>’s main use case is to tell autograd to begin recording
operations on a Tensor <code class="docutils literal notranslate"><span class="pre">tensor</span></code>. If <code class="docutils literal notranslate"><span class="pre">tensor</span></code> has <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>
(because it was obtained through a DataLoader, or required preprocessing or
initialization), <code class="docutils literal notranslate"><span class="pre">tensor.requires_grad_()</span></code> makes it so that autograd will
begin to record operations on <code class="docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>requires_grad (bool): If autograd should record operations on this tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Let&#39;s say we want to preprocess some saved weights and use</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the result as new weights.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">loaded_weights</span><span class="p">)</span>  <span class="c1"># some function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span>
<span class="go">tensor([-0.5503,  0.4926, -2.1158, -0.8303])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now, start to record operations done to weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([-1.1007,  0.9853, -4.2316, -1.6606])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.reshape">
<code class="sig-name descname"><span class="pre">reshape</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.reshape" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
but with the specified shape. This method returns a view if <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.shape" title="hxtorch.spiking.modules.synapse.Parameter.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a> is
compatible with the current shape. See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code> on when it is
possible to return a view.</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reshape()</span></code></p>
<dl class="simple">
<dt>Args:</dt><dd><p>shape (tuple of ints or int…): the desired shape</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.reshape_as">
<code class="sig-name descname"><span class="pre">reshape_as</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.reshape_as" title="Permalink to this definition"></a></dt>
<dd><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.reshape_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.reshape(other.sizes())</span></code>.
This method returns a view if <code class="docutils literal notranslate"><span class="pre">other.sizes()</span></code> is compatible with the current
shape. See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code> on when it is possible to return a view.</p>
<p>Please see <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.reshape" title="hxtorch.spiking.modules.synapse.Parameter.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">reshape</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>other (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The result tensor has the same shape</dt><dd><p>as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.resize">
<code class="sig-name descname"><span class="pre">resize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sizes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.resize" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.resize_">
<code class="sig-name descname"><span class="pre">resize_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sizes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.resize_" title="Permalink to this definition"></a></dt>
<dd><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size. If the number of elements is
larger than the current storage size, then the underlying storage is resized
to fit the new number of elements. If the number of elements is smaller, the
underlying storage is not changed. Existing elements are preserved but any new
memory is uninitialized.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This is a low-level method. The storage is reinterpreted as C-contiguous,
ignoring the current strides (unless the target size equals the current
size, in which case the tensor is left unchanged). For most purposes, you
will instead want to use <code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code>, which checks for
contiguity, or <code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code>, which copies data if needed. To
change the size in-place with custom strides, see <code class="xref py py-meth docutils literal notranslate"><span class="pre">set_()</span></code>.</p>
</div>
<dl>
<dt>Args:</dt><dd><p>sizes (torch.Size or int…): the desired size
memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</p>
<blockquote>
<div><p>Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>. Note that memory format of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is going to be unaffected if <code class="docutils literal notranslate"><span class="pre">self.size()</span></code> matches <code class="docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
</div></blockquote>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2],</span>
<span class="go">        [ 3,  4]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.resize_as">
<code class="sig-name descname"><span class="pre">resize_as</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.resize_as" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.resize_as_">
<code class="sig-name descname"><span class="pre">resize_as_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.resize_as_" title="Permalink to this definition"></a></dt>
<dd><p>Resizes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to be the same size as the specified
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>. This is equivalent to <code class="docutils literal notranslate"><span class="pre">self.resize_(tensor.size())</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>. Note that memory format of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is going to be unaffected if <code class="docutils literal notranslate"><span class="pre">self.size()</span></code> matches <code class="docutils literal notranslate"><span class="pre">tensor.size()</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.resize_as_sparse_">
<code class="sig-name descname"><span class="pre">resize_as_sparse_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.resize_as_sparse_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.resolve_conj">
<code class="sig-name descname"><span class="pre">resolve_conj</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.resolve_conj" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_conj()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.resolve_neg">
<code class="sig-name descname"><span class="pre">resolve_neg</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.resolve_neg" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_neg()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.retain_grad">
<code class="sig-name descname"><span class="pre">retain_grad</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">None</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.retain_grad" title="Permalink to this definition"></a></dt>
<dd><p>Enables this Tensor to have their <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated during
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>. This is a no-op for leaf tensors.</p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.retains_grad">
<code class="sig-name descname"><span class="pre">retains_grad</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.retains_grad" title="Permalink to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if this Tensor is non-leaf and its <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.grad" title="hxtorch.spiking.modules.synapse.Parameter.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> is enabled to be
populated during <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.backward" title="hxtorch.spiking.modules.synapse.Parameter.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.roll">
<code class="sig-name descname"><span class="pre">roll</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shifts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.roll" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.roll()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.rot90">
<code class="sig-name descname"><span class="pre">rot90</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.rot90" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rot90()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.round">
<code class="sig-name descname"><span class="pre">round</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decimals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.round" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.round_">
<code class="sig-name descname"><span class="pre">round_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decimals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.round_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">round()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.row_indices">
<code class="sig-name descname"><span class="pre">row_indices</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.row_indices" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.rsqrt">
<code class="sig-name descname"><span class="pre">rsqrt</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.rsqrt" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rsqrt()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.rsqrt_">
<code class="sig-name descname"><span class="pre">rsqrt_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.rsqrt_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">rsqrt()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.scatter">
<code class="sig-name descname"><span class="pre">scatter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.scatter" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.scatter_">
<code class="sig-name descname"><span class="pre">scatter_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_" title="Permalink to this definition"></a></dt>
<dd><p>Writes all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, its output
index is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by
the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>This is the reverse operation of the manner described in <code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> (if it is a Tensor) should all have
the same number of dimensions. It is also required that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.
Note that <code class="docutils literal notranslate"><span class="pre">index</span></code> and <code class="docutils literal notranslate"><span class="pre">src</span></code> do not broadcast.</p>
<p>Moreover, as for <code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code>, the values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">self.size(dim)</span> <span class="pre">-</span> <span class="pre">1</span></code> inclusive.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When indices are not unique, the behavior is non-deterministic (one of the
values from <code class="docutils literal notranslate"><span class="pre">src</span></code> will be picked arbitrarily) and the gradient will be
incorrect (it will be propagated to all locations in the source that
correspond to the same index)!</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backward pass is implemented only for <code class="docutils literal notranslate"><span class="pre">src.shape</span> <span class="pre">==</span> <span class="pre">index.shape</span></code>.</p>
</div>
<p>Additionally accepts an optional <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> argument that allows
specification of an optional reduction operation, which is applied to all
values in the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, the reduction
operation is applied to an index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by
its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding
value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>Given a 3-D tensor and reduction using the multiplication operation, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>Reducing with the addition operation is the same as using
<code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_add_()</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The reduce argument with Tensor <code class="docutils literal notranslate"><span class="pre">src</span></code> is deprecated and will be removed in
a future PyTorch release. Please use <code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_reduce_()</span></code>
instead for more reduction options.</p>
</div>
<dl>
<dt>Args:</dt><dd><p>dim (int): the axis along which to index
index (LongTensor): the indices of elements to scatter, can be either empty</p>
<blockquote>
<div><p>or of the same dimensionality as <code class="docutils literal notranslate"><span class="pre">src</span></code>. When empty, the operation
returns <code class="docutils literal notranslate"><span class="pre">self</span></code> unchanged.</p>
</div></blockquote>
<p>src (Tensor or float): the source element(s) to scatter.
reduce (str, optional): reduction operation to apply, can be either</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">'add'</span></code> or <code class="docutils literal notranslate"><span class="pre">'multiply'</span></code>.</p>
</div></blockquote>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span>
<span class="go">tensor([[ 1,  2,  3,  4,  5],</span>
<span class="go">        [ 6,  7,  8,  9, 10]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[1, 0, 0, 4, 0],</span>
<span class="go">        [0, 2, 0, 0, 0],</span>
<span class="go">        [0, 0, 3, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[1, 2, 3, 0, 0],</span>
<span class="go">        [6, 7, 0, 0, 8],</span>
<span class="go">        [0, 0, 0, 0, 0]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span>
<span class="gp">... </span>           <span class="mf">1.23</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s1">&#39;multiply&#39;</span><span class="p">)</span>
<span class="go">tensor([[2.0000, 2.0000, 2.4600, 2.0000],</span>
<span class="go">        [2.0000, 2.0000, 2.0000, 2.4600]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span>
<span class="gp">... </span>           <span class="mf">1.23</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s1">&#39;add&#39;</span><span class="p">)</span>
<span class="go">tensor([[2.0000, 2.0000, 3.2300, 2.0000],</span>
<span class="go">        [2.0000, 2.0000, 2.0000, 3.2300]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.scatter_add">
<code class="sig-name descname"><span class="pre">scatter_add</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_add" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.scatter_add_">
<code class="sig-name descname"><span class="pre">scatter_add_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_add_" title="Permalink to this definition"></a></dt>
<dd><p>Adds all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in a similar fashion as
<code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_()</span></code>. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, it is added to
an index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>
for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> should have same number of
dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions
<code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>. Note that <code class="docutils literal notranslate"><span class="pre">index</span></code> and <code class="docutils literal notranslate"><span class="pre">src</span></code> do not broadcast.</p>
<dl class="simple">
<dt>Note:</dt><dd><p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backward pass is implemented only for <code class="docutils literal notranslate"><span class="pre">src.shape</span> <span class="pre">==</span> <span class="pre">index.shape</span></code>.</p>
</div>
<dl>
<dt>Args:</dt><dd><p>dim (int): the axis along which to index
index (LongTensor): the indices of elements to scatter and add, can be</p>
<blockquote>
<div><p>either empty or of the same dimensionality as <code class="docutils literal notranslate"><span class="pre">src</span></code>. When empty, the
operation returns <code class="docutils literal notranslate"><span class="pre">self</span></code> unchanged.</p>
</div></blockquote>
<p>src (Tensor): the source elements to scatter and add</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[1., 0., 0., 1., 1.],</span>
<span class="go">        [0., 1., 0., 0., 0.],</span>
<span class="go">        [0., 0., 1., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[2., 0., 0., 1., 1.],</span>
<span class="go">        [0., 2., 0., 0., 0.],</span>
<span class="go">        [0., 0., 2., 1., 1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.scatter_reduce">
<code class="sig-name descname"><span class="pre">scatter_reduce</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_self</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_reduce" title="Permalink to this definition"></a></dt>
<dd><p>Out-of-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_reduce_()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.scatter_reduce_">
<code class="sig-name descname"><span class="pre">scatter_reduce_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_self</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.scatter_reduce_" title="Permalink to this definition"></a></dt>
<dd><p>Reduces all values from the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor to the indices specified in
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor using the applied reduction
defined via the <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> argument (<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>). For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, it is reduced to an
index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=&quot;True&quot;</span></code>, the values in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor are included in the reduction.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> should all have
the same number of dimensions. It is also required that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.
Note that <code class="docutils literal notranslate"><span class="pre">index</span></code> and <code class="docutils literal notranslate"><span class="pre">src</span></code> do not broadcast.</p>
<p>For a 3-D tensor with <code class="xref py py-obj docutils literal notranslate"><span class="pre">reduce=&quot;sum&quot;</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=True</span></code> the
output is given as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note:</dt><dd><p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backward pass is implemented only for <code class="docutils literal notranslate"><span class="pre">src.shape</span> <span class="pre">==</span> <span class="pre">index.shape</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is in beta and may change in the near future.</p>
</div>
<dl>
<dt>Args:</dt><dd><p>dim (int): the axis along which to index
index (LongTensor): the indices of elements to scatter and reduce.
src (Tensor): the source elements to scatter and reduce
reduce (str): the reduction operation to apply for non-unique indices</p>
<blockquote>
<div><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>)</p>
</div></blockquote>
<dl class="simple">
<dt>include_self (bool): whether elements from the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor are</dt><dd><p>included in the reduction</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
<span class="go">tensor([5., 14., 8., 4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">tensor([4., 12., 5., 4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;amax&quot;</span><span class="p">)</span>
<span class="go">tensor([5., 6., 5., 2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;amax&quot;</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">tensor([3., 6., 5., 2.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.select">
<code class="sig-name descname"><span class="pre">select</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.select" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.select_scatter">
<code class="sig-name descname"><span class="pre">select_scatter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.select_scatter" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select_scatter()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.set_">
<code class="sig-name descname"><span class="pre">set_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.set_" title="Permalink to this definition"></a></dt>
<dd><p>Sets the underlying storage, size, and strides. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a tensor,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will share the same storage and have the same size and
strides as <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code>. Changes to elements in one tensor will be reflected
in the other.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Storage</span></code>, the method sets the underlying
storage, offset, size, and stride.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>source (Tensor or Storage): the tensor or storage to use
storage_offset (int, optional): the offset in the storage
size (torch.Size, optional): the desired size. Defaults to the size of the source.
stride (tuple, optional): the desired stride. Defaults to C-contiguous strides.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sgn">
<code class="sig-name descname"><span class="pre">sgn</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sgn" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sgn()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sgn_">
<code class="sig-name descname"><span class="pre">sgn_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sgn_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sgn()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.shape">
<code class="sig-name descname"><span class="pre">shape</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.shape" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.share_memory_">
<code class="sig-name descname"><span class="pre">share_memory_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.share_memory_" title="Permalink to this definition"></a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.short">
<code class="sig-name descname"><span class="pre">short</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.short" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.short()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int16)</span></code>. See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="hxtorch.spiking.modules.synapse.Parameter.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sigmoid">
<code class="sig-name descname"><span class="pre">sigmoid</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sigmoid" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sigmoid_">
<code class="sig-name descname"><span class="pre">sigmoid_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sigmoid_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sigmoid()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sign">
<code class="sig-name descname"><span class="pre">sign</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sign" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sign()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sign_">
<code class="sig-name descname"><span class="pre">sign_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sign_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sign()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.signbit">
<code class="sig-name descname"><span class="pre">signbit</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.signbit" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.signbit()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sin">
<code class="sig-name descname"><span class="pre">sin</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sin" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sin_">
<code class="sig-name descname"><span class="pre">sin_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sin_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sin()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sinc">
<code class="sig-name descname"><span class="pre">sinc</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sinc" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinc()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sinc_">
<code class="sig-name descname"><span class="pre">sinc_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sinc_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sinc()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sinh">
<code class="sig-name descname"><span class="pre">sinh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sinh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sinh_">
<code class="sig-name descname"><span class="pre">sinh_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sinh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sinh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.size">
<code class="sig-name descname"><span class="pre">size</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Size</span> <span class="pre">or</span> <span class="pre">int</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="Permalink to this definition"></a></dt>
<dd><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If <code class="docutils literal notranslate"><span class="pre">dim</span></code> is not specified,
the returned value is a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>, a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>.
If <code class="docutils literal notranslate"><span class="pre">dim</span></code> is specified, returns an int holding the size of that dimension.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>dim (int, optional): The dimension for which to retrieve the size.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">4</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.slice_scatter">
<code class="sig-name descname"><span class="pre">slice_scatter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.slice_scatter" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slice_scatter()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.slogdet">
<code class="sig-name descname"><span class="pre">slogdet</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.slogdet" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slogdet()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.smm">
<code class="sig-name descname"><span class="pre">smm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.smm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.smm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.softmax">
<code class="sig-name descname"><span class="pre">softmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.softmax" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.softmax()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.solve">
<code class="sig-name descname"><span class="pre">solve</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.solve" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sort">
<code class="sig-name descname"><span class="pre">sort</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">descending</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sort" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sparse_dim">
<code class="sig-name descname"><span class="pre">sparse_dim</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_dim" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of sparse dimensions in a <span class="xref std std-ref">sparse tensor</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returns <code class="docutils literal notranslate"><span class="pre">0</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse tensor.</p>
</div>
<p>See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.dense_dim()</span></code> and <span class="xref std std-ref">hybrid tensors</span>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sparse_mask">
<code class="sig-name descname"><span class="pre">sparse_mask</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_mask" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new <span class="xref std std-ref">sparse tensor</span> with values from a
strided tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> filtered by the indices of the sparse
tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>. The values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> sparse tensor are
ignored. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> tensors must have the same
shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned sparse tensor might contain duplicate values if <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>
is not coalesced. It is therefore advisable to pass <code class="docutils literal notranslate"><span class="pre">mask.coalesce()</span></code>
if such behavior is not desired.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned sparse tensor has the same indices as the sparse tensor
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>, even when the corresponding values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> are
zeros.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><p>mask (Tensor): a sparse tensor whose indices are used as a filter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nse</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nse</span><span class="p">,)),</span>
<span class="gp">... </span>               <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nse</span><span class="p">,))],</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">nse</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nse</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="o">.</span><span class="n">sparse_mask</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[0, 0, 0, 2],</span>
<span class="go">                       [0, 1, 4, 3]]),</span>
<span class="go">       values=tensor([[[ 1.6550,  0.2397],</span>
<span class="go">                       [-0.1611, -0.0779]],</span>

<span class="go">                      [[ 0.2326, -1.0558],</span>
<span class="go">                       [ 1.4711,  1.9678]],</span>

<span class="go">                      [[-0.5138, -0.0411],</span>
<span class="go">                       [ 1.9417,  0.5158]],</span>

<span class="go">                      [[ 0.0793,  0.0036],</span>
<span class="go">                       [-0.2569, -0.1055]]]),</span>
<span class="go">       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sparse_resize_">
<code class="sig-name descname"><span class="pre">sparse_resize_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_resize_" title="Permalink to this definition"></a></dt>
<dd><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> <span class="xref std std-ref">sparse tensor</span> to the desired
size and the number of sparse and dense dimensions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the number of specified elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is zero, then
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a>, <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_dim" title="hxtorch.spiking.modules.synapse.Parameter.sparse_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sparse_dim</span></code></a>, and <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dense_dim" title="hxtorch.spiking.modules.synapse.Parameter.dense_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dense_dim</span></code></a> can be any
size and positive integers such that <code class="docutils literal notranslate"><span class="pre">len(size)</span> <span class="pre">==</span> <span class="pre">sparse_dim</span> <span class="pre">+</span>
<span class="pre">dense_dim</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> specifies one or more elements, however, then each
dimension in <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> must not be smaller than the corresponding
dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_dim" title="hxtorch.spiking.modules.synapse.Parameter.sparse_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sparse_dim</span></code></a> must equal the number
of sparse dimensions in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, and <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dense_dim" title="hxtorch.spiking.modules.synapse.Parameter.dense_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dense_dim</span></code></a> must
equal the number of dense dimensions in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse tensor.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>size (torch.Size): the desired size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is non-empty</dt><dd><p>sparse tensor, the desired size cannot be smaller than the
original size.</p>
</dd>
</dl>
<p>sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sparse_resize_and_clear_">
<code class="sig-name descname"><span class="pre">sparse_resize_and_clear_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sparse_resize_and_clear_" title="Permalink to this definition"></a></dt>
<dd><p>Removes all specified elements from a <span class="xref std std-ref">sparse tensor</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> to the desired
size and the number of sparse and dense dimensions.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>size (torch.Size): the desired size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.split">
<code class="sig-name descname"><span class="pre">split</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.split" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.split_with_sizes">
<code class="sig-name descname"><span class="pre">split_with_sizes</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.split_with_sizes" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sqrt">
<code class="sig-name descname"><span class="pre">sqrt</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sqrt" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sqrt_">
<code class="sig-name descname"><span class="pre">sqrt_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sqrt_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.square">
<code class="sig-name descname"><span class="pre">square</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.square" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.square()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.square_">
<code class="sig-name descname"><span class="pre">square_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.square_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">square()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.squeeze">
<code class="sig-name descname"><span class="pre">squeeze</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.squeeze" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.squeeze_">
<code class="sig-name descname"><span class="pre">squeeze_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.squeeze_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">squeeze()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sspaddmm">
<code class="sig-name descname"><span class="pre">sspaddmm</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sspaddmm" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sspaddmm()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.std">
<code class="sig-name descname"><span class="pre">std</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.std" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.std()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.stft">
<code class="sig-name descname"><span class="pre">stft</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_fft</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">hop_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">win_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'reflect'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onesided</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_complex</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.bool" title="hxtorch.spiking.modules.synapse.Parameter.bool"><span class="pre">bool</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.stft" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.stft()</span></code></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function changed signature at version 0.4.1. Calling with
the previous signature may cause error or return incorrect result.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.storage">
<code class="sig-name descname"><span class="pre">storage</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.TypedStorage</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.storage" title="Permalink to this definition"></a></dt>
<dd><p>Returns the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">TypedStorage</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">TypedStorage</span></code> is deprecated. It will be removed in the future, and
<code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code> will be the only storage class. To access the
<code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code> directly, use <code class="xref py py-attr docutils literal notranslate"><span class="pre">Tensor.untyped_storage()</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.storage_offset">
<code class="sig-name descname"><span class="pre">storage_offset</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.int" title="hxtorch.spiking.modules.synapse.Parameter.int"><span class="pre">int</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.storage_offset" title="Permalink to this definition"></a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s offset in the underlying storage in terms of
number of storage elements (not bytes).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.storage_type">
<code class="sig-name descname"><span class="pre">storage_type</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.type" title="hxtorch.spiking.modules.synapse.Parameter.type"><span class="pre">type</span></a><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.storage_type" title="Permalink to this definition"></a></dt>
<dd><p>Returns the type of the underlying storage.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.stride">
<code class="sig-name descname"><span class="pre">stride</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">tuple</span> <span class="pre">or</span> <span class="pre">int</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.stride" title="Permalink to this definition"></a></dt>
<dd><p>Returns the stride of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
<p>Stride is the jump necessary to go from one element to the next one in the
specified dimension <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dim" title="hxtorch.spiking.modules.synapse.Parameter.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>. A tuple of all strides is returned when no
argument is passed in. Otherwise, an integer value is returned as the stride in
the particular dimension <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dim" title="hxtorch.spiking.modules.synapse.Parameter.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>dim (int, optional): the desired dimension in which stride is required</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sub">
<code class="sig-name descname"><span class="pre">sub</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sub" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sub()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sub_">
<code class="sig-name descname"><span class="pre">sub_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sub_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.subtract">
<code class="sig-name descname"><span class="pre">subtract</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.subtract" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.subtract()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.subtract_">
<code class="sig-name descname"><span class="pre">subtract_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.subtract_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">subtract()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sum">
<code class="sig-name descname"><span class="pre">sum</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sum" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sum()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.sum_to_size">
<code class="sig-name descname"><span class="pre">sum_to_size</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">size</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.sum_to_size" title="Permalink to this definition"></a></dt>
<dd><p>Sum <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor to <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a>.
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> must be broadcastable to <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor size.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>size (int…): a sequence of integers defining the shape of the output tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.svd">
<code class="sig-name descname"><span class="pre">svd</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">some</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_uv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.svd" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.swapaxes">
<code class="sig-name descname"><span class="pre">swapaxes</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.swapaxes" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapaxes()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.swapaxes_">
<code class="sig-name descname"><span class="pre">swapaxes_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.swapaxes_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">swapaxes()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.swapdims">
<code class="sig-name descname"><span class="pre">swapdims</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.swapdims" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapdims()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.swapdims_">
<code class="sig-name descname"><span class="pre">swapdims_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.swapdims_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">swapdims()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.symeig">
<code class="sig-name descname"><span class="pre">symeig</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eigenvectors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.symeig" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.t">
<code class="sig-name descname"><span class="pre">t</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.t" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.t()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.t_">
<code class="sig-name descname"><span class="pre">t_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.t_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.take">
<code class="sig-name descname"><span class="pre">take</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.take" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.take_along_dim">
<code class="sig-name descname"><span class="pre">take_along_dim</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.take_along_dim" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take_along_dim()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tan">
<code class="sig-name descname"><span class="pre">tan</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tan" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tan_">
<code class="sig-name descname"><span class="pre">tan_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tan_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">tan()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tanh">
<code class="sig-name descname"><span class="pre">tanh</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tanh" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tanh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tanh_">
<code class="sig-name descname"><span class="pre">tanh_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tanh_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">tanh()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tensor_split">
<code class="sig-name descname"><span class="pre">tensor_split</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices_or_sections</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tensor_split" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor_split()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tile">
<code class="sig-name descname"><span class="pre">tile</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">reps</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tile" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tile()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to">
<code class="sig-name descname"><span class="pre">to</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to" title="Permalink to this definition"></a></dt>
<dd><p>Performs Tensor dtype and/or device conversion. A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> are
inferred from the arguments of <code class="docutils literal notranslate"><span class="pre">self.to(*args,</span> <span class="pre">**kwargs)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">self</span></code> Tensor already
has the correct <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, then <code class="docutils literal notranslate"><span class="pre">self</span></code> is returned.
Otherwise, the returned tensor is a copy of <code class="docutils literal notranslate"><span class="pre">self</span></code> with the desired
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>.</p>
</div>
<p>Here are the ways to call <code class="docutils literal notranslate"><span class="pre">to</span></code>:</p>
<dl class="py method">
<dt>
<code class="sig-name descname"><span class="pre">to</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span></dt>
<dd><blockquote>
<div><p>Returns a Tensor with the specified <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a></p>
<dl class="simple">
<dt>Args:</dt><dd><p>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt>
<code class="sig-name descname"><span class="pre">to</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span></dt>
<dd><blockquote>
<div><p>Returns a Tensor with the specified <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.device" title="hxtorch.spiking.modules.synapse.Parameter.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> and (optional)
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>. If <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> it is inferred to be <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert asynchronously with respect to
the host if possible, e.g., converting a CPU Tensor with pinned memory to a
CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt>
<code class="sig-name descname"><span class="pre">to</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span></dt>
<dd><blockquote>
<div><p>Returns a Tensor with same <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code> as
the Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert
asynchronously with respect to the host if possible, e.g., converting a CPU
Tensor with pinned memory to a CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
</div></blockquote>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Initially dtype=float32, device=cpu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">other</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_dense">
<code class="sig-name descname"><span class="pre">to_dense</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_dense" title="Permalink to this definition"></a></dt>
<dd><p>Creates a strided copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a strided tensor, otherwise returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
<span class="gp">... </span>       <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span>
<span class="gp">... </span>       <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span>
<span class="gp">... </span>       <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[ 0,  0,  0],</span>
<span class="go">        [ 9,  0, 10],</span>
<span class="go">        [ 0,  0,  0]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_mkldnn">
<code class="sig-name descname"><span class="pre">to_mkldnn</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_mkldnn" title="Permalink to this definition"></a></dt>
<dd><p>Returns a copy of the tensor in <code class="docutils literal notranslate"><span class="pre">torch.mkldnn</span></code> layout.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor">
<code class="sig-name descname"><span class="pre">to_padded_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor" title="Permalink to this definition"></a></dt>
<dd><p>See <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor" title="hxtorch.spiking.modules.synapse.Parameter.to_padded_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_padded_tensor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_sparse">
<code class="sig-name descname"><span class="pre">to_sparse</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparseDims</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse" title="Permalink to this definition"></a></dt>
<dd><p>Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
<span class="xref std std-ref">coordinate format</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>sparseDims (int, optional): the number of sparse dimensions to include in the new sparse tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">tensor([[ 0,  0,  0],</span>
<span class="go">        [ 9,  0, 10],</span>
<span class="go">        [ 0,  0,  0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="go">tensor(indices=tensor([[1, 1],</span>
<span class="go">                       [0, 2]]),</span>
<span class="go">       values=tensor([ 9, 10]),</span>
<span class="go">       size=(3, 3), nnz=2, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[1]]),</span>
<span class="go">       values=tensor([[ 9,  0, 10]]),</span>
<span class="go">       size=(3, 3), nnz=1, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<dl class="py method">
<dt>
<code class="sig-name descname"><span class="pre">to_sparse</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">blocksize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span></dt>
<dd></dd></dl>

<p>Returns a sparse tensor with the specified layout and blocksize.  If
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is strided, the number of dense dimensions could be
specified, and a hybrid sparse tensor will be created, with
<cite>dense_dim</cite> dense dimensions and <cite>self.dim() - 2 - dense_dim</cite> batch
dimension.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> layout and blocksize parameters match
with the specified layout and blocksize, return
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. Otherwise, return a sparse tensor copy of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<p>Args:</p>
<blockquote>
<div><dl class="simple">
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): The desired sparse</dt><dd><p>layout. One of <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.sparse_csr</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.sparse_csc</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.sparse_bsr</span></code>, or
<code class="docutils literal notranslate"><span class="pre">torch.sparse_bsc</span></code>. Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code>.</p>
</dd>
<dt>blocksize (list, tuple, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>, optional): Block size</dt><dd><p>of the resulting BSR or BSC tensor. For other layouts,
specifying the block size that is not <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in a
RuntimeError exception.  A block size must be a tuple of length
two such that its items evenly divide the two sparse dimensions.</p>
</dd>
<dt>dense_dim (int, optional): Number of dense dimensions of the</dt><dd><p>resulting CSR, CSC, BSR or BSC tensor.  This argument should be
used only if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a
value between 0 and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p>
</dd>
</dl>
</div></blockquote>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[0, 2, 2],</span>
<span class="go">                       [0, 0, 1]]),</span>
<span class="go">       values=tensor([1, 2, 3]),</span>
<span class="go">       size=(3, 2), nnz=3, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span> <span class="n">blocksize</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 1, 2]),</span>
<span class="go">       col_indices=tensor([0, 0]),</span>
<span class="go">       values=tensor([[[1, 0]],</span>
<span class="go">                      [[2, 3]]]), size=(3, 2), nnz=2, layout=torch.sparse_bsr)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span> <span class="n">blocksize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">RuntimeError: Tensor size(-2) 3 needs to be divisible by blocksize[0] 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">blocksize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">RuntimeError: to_sparse for Strided to SparseCsr conversion does not use specified blocksize</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 1, 3]),</span>
<span class="go">       col_indices=tensor([0, 0, 1]),</span>
<span class="go">       values=tensor([[1],</span>
<span class="go">                      [2],</span>
<span class="go">                      [3]]), size=(3, 2, 1), nnz=3, layout=torch.sparse_csr)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsc">
<code class="sig-name descname"><span class="pre">to_sparse_bsc</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blocksize</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsc" title="Permalink to this definition"></a></dt>
<dd><p>Convert a tensor to a block sparse column (BSC) storage format of
given blocksize.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is strided, then the number of
dense dimensions could be specified, and a hybrid BSC tensor will be
created, with <cite>dense_dim</cite> dense dimensions and <cite>self.dim() - 2 -
dense_dim</cite> batch dimension.</p>
<p>Args:</p>
<blockquote>
<div><dl class="simple">
<dt>blocksize (list, tuple, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>, optional): Block size</dt><dd><p>of the resulting BSC tensor. A block size must be a tuple of
length two such that its items evenly divide the two sparse
dimensions.</p>
</dd>
<dt>dense_dim (int, optional): Number of dense dimensions of the</dt><dd><p>resulting BSC tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p>
</dd>
</dl>
</div></blockquote>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsc</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">to_sparse_bsc</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsc</span><span class="o">.</span><span class="n">row_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 0, 1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_bsc</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(ccol_indices=tensor([0, 1, 2, 3]),</span>
<span class="go">       row_indices=tensor([0, 1, 0]),</span>
<span class="go">       values=tensor([[[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]]]), size=(4, 3, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_bsc)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsr">
<code class="sig-name descname"><span class="pre">to_sparse_bsr</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blocksize</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_bsr" title="Permalink to this definition"></a></dt>
<dd><p>Convert a tensor to a block sparse row (BSR) storage format of given
blocksize.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is strided, then the number of dense
dimensions could be specified, and a hybrid BSR tensor will be
created, with <cite>dense_dim</cite> dense dimensions and <cite>self.dim() - 2 -
dense_dim</cite> batch dimension.</p>
<p>Args:</p>
<blockquote>
<div><dl class="simple">
<dt>blocksize (list, tuple, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>, optional): Block size</dt><dd><p>of the resulting BSR tensor. A block size must be a tuple of
length two such that its items evenly divide the two sparse
dimensions.</p>
</dd>
<dt>dense_dim (int, optional): Number of dense dimensions of the</dt><dd><p>resulting BSR tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p>
</dd>
</dl>
</div></blockquote>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsr</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">to_sparse_bsr</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsr</span><span class="o">.</span><span class="n">col_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 0, 1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_bsr</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(crow_indices=tensor([0, 2, 3]),</span>
<span class="go">       col_indices=tensor([0, 2, 1]),</span>
<span class="go">       values=tensor([[[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]]]), size=(4, 3, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_bsr)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_sparse_coo">
<code class="sig-name descname"><span class="pre">to_sparse_coo</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_coo" title="Permalink to this definition"></a></dt>
<dd><p>Convert a tensor to <span class="xref std std-ref">coordinate format</span>.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_coo</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
<span class="go">25</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_sparse_csc">
<code class="sig-name descname"><span class="pre">to_sparse_csc</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_csc" title="Permalink to this definition"></a></dt>
<dd><p>Convert a tensor to compressed column storage (CSC) format.  Except
for strided tensors, only works with 2D tensors.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
is strided, then the number of dense dimensions could be specified,
and a hybrid CSC tensor will be created, with <cite>dense_dim</cite> dense
dimensions and <cite>self.dim() - 2 - dense_dim</cite> batch dimension.</p>
<p>Args:</p>
<blockquote>
<div><dl class="simple">
<dt>dense_dim (int, optional): Number of dense dimensions of the</dt><dd><p>resulting CSC tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p>
</dd>
</dl>
</div></blockquote>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csc</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
<span class="go">25</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csc</span><span class="p">(</span><span class="n">dense_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor(ccol_indices=tensor([0, 1, 2, 3]),</span>
<span class="go">       row_indices=tensor([0, 2, 1]),</span>
<span class="go">       values=tensor([[[1.]],</span>

<span class="go">                      [[1.]],</span>

<span class="go">                      [[1.]]]), size=(3, 3, 1, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_csc)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.to_sparse_csr">
<code class="sig-name descname"><span class="pre">to_sparse_csr</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.to_sparse_csr" title="Permalink to this definition"></a></dt>
<dd><p>Convert a tensor to compressed row storage format (CSR).  Except for
strided tensors, only works with 2D tensors.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is
strided, then the number of dense dimensions could be specified, and a
hybrid CSR tensor will be created, with <cite>dense_dim</cite> dense dimensions
and <cite>self.dim() - 2 - dense_dim</cite> batch dimension.</p>
<p>Args:</p>
<blockquote>
<div><dl class="simple">
<dt>dense_dim (int, optional): Number of dense dimensions of the</dt><dd><p>resulting CSR tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p>
</dd>
</dl>
</div></blockquote>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
<span class="go">25</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">(</span><span class="n">dense_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 2, 3]),</span>
<span class="go">       col_indices=tensor([0, 2, 1]),</span>
<span class="go">       values=tensor([[[1.]],</span>

<span class="go">                      [[1.]],</span>

<span class="go">                      [[1.]]]), size=(3, 3, 1, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_csr)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tolist">
<code class="sig-name descname"><span class="pre">tolist</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">list</span> <span class="pre">or</span> <span class="pre">number</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tolist" title="Permalink to this definition"></a></dt>
<dd><p>Returns the tensor as a (nested) list. For scalars, a standard
Python number is returned, just like with <code class="xref py py-meth docutils literal notranslate"><span class="pre">item()</span></code>.
Tensors are automatically moved to the CPU first if necessary.</p>
<p>This operation is not differentiable.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[0.012766935862600803, 0.5415473580360413],</span>
<span class="go"> [-0.08909505605697632, 0.7729271650314331]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">0.012766935862600803</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.topk">
<code class="sig-name descname"><span class="pre">topk</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">largest</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.topk" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.trace">
<code class="sig-name descname"><span class="pre">trace</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.trace" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trace()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.transpose">
<code class="sig-name descname"><span class="pre">transpose</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.transpose" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.transpose_">
<code class="sig-name descname"><span class="pre">transpose_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.transpose_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.triangular_solve">
<code class="sig-name descname"><span class="pre">triangular_solve</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">A</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transpose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unitriangular</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.triangular_solve" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triangular_solve()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tril">
<code class="sig-name descname"><span class="pre">tril</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tril" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tril()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.tril_">
<code class="sig-name descname"><span class="pre">tril_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.tril_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">tril()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.triu">
<code class="sig-name descname"><span class="pre">triu</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.triu" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triu()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.triu_">
<code class="sig-name descname"><span class="pre">triu_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.triu_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">triu()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.true_divide">
<code class="sig-name descname"><span class="pre">true_divide</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.true_divide" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.true_divide()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.true_divide_">
<code class="sig-name descname"><span class="pre">true_divide_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.true_divide_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">true_divide_()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.trunc">
<code class="sig-name descname"><span class="pre">trunc</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.trunc" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.trunc_">
<code class="sig-name descname"><span class="pre">trunc_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.trunc_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">trunc()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.type">
<code class="sig-name descname"><span class="pre">type</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">str</span> <span class="pre">or</span> <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.type" title="Permalink to this definition"></a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<dl>
<dt>Args:</dt><dd><p>dtype (dtype or string): The desired type
non_blocking (bool): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory</p>
<blockquote>
<div><p>and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</p>
</div></blockquote>
<dl class="simple">
<dt><a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of</dt><dd><p>the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.type_as">
<code class="sig-name descname"><span class="pre">type_as</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.type_as" title="Permalink to this definition"></a></dt>
<dd><p>Returns this tensor cast to the type of the given tensor.</p>
<p>This is a no-op if the tensor is already of the correct type. This is
equivalent to <code class="docutils literal notranslate"><span class="pre">self.type(tensor.type())</span></code></p>
<dl class="simple">
<dt>Args:</dt><dd><p>tensor (Tensor): the tensor which has the desired type</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unbind">
<code class="sig-name descname"><span class="pre">unbind</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">seq</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unbind" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unbind()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unflatten">
<code class="sig-name descname"><span class="pre">unflatten</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sizes</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unflatten" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unflatten()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unfold">
<code class="sig-name descname"><span class="pre">unfold</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unfold" title="Permalink to this definition"></a></dt>
<dd><p>Returns a view of the original tensor which contains all slices of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code>.</p>
<p>Step between two slices is given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>.</p>
<p>If <cite>sizedim</cite> is the size of dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code> for <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, the size of
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code> in the returned tensor will be
<cite>(sizedim - size) / step + 1</cite>.</p>
<p>An additional dimension of size <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.size" title="hxtorch.spiking.modules.synapse.Parameter.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> is appended in the returned tensor.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>dimension (int): dimension in which unfolding happens
size (int): the size of each slice that is unfolded
step (int): the step between each slice</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 2.,  3.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 4.,  5.],</span>
<span class="go">        [ 5.,  6.],</span>
<span class="go">        [ 6.,  7.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.uniform_">
<code class="sig-name descname"><span class="pre">uniform_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">from=0</span></em>, <em class="sig-param"><span class="pre">to=1</span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.uniform_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the continuous uniform
distribution:</p>
<div class="math notranslate nohighlight">
\[P(x) = \dfrac{1}{\text{to} - \text{from}}\]</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unique">
<code class="sig-name descname"><span class="pre">unique</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_counts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unique" title="Permalink to this definition"></a></dt>
<dd><p>Returns the unique elements of the input tensor.</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unique_consecutive">
<code class="sig-name descname"><span class="pre">unique_consecutive</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">return_inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_counts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unique_consecutive" title="Permalink to this definition"></a></dt>
<dd><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique_consecutive()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unsafe_chunk">
<code class="sig-name descname"><span class="pre">unsafe_chunk</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chunks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unsafe_chunk" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsafe_chunk()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unsafe_split">
<code class="sig-name descname"><span class="pre">unsafe_split</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unsafe_split" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsafe_split()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unsafe_split_with_sizes">
<code class="sig-name descname"><span class="pre">unsafe_split_with_sizes</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unsafe_split_with_sizes" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unsqueeze">
<code class="sig-name descname"><span class="pre">unsqueeze</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unsqueeze" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.unsqueeze_">
<code class="sig-name descname"><span class="pre">unsqueeze_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.unsqueeze_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.untyped_storage">
<code class="sig-name descname"><span class="pre">untyped_storage</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.UntypedStorage</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.untyped_storage" title="Permalink to this definition"></a></dt>
<dd><p>Returns the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.values">
<code class="sig-name descname"><span class="pre">values</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.values" title="Permalink to this definition"></a></dt>
<dd><p>Return the values tensor of a <span class="xref std std-ref">sparse COO tensor</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
<p>See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.indices()</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method can only be called on a coalesced sparse tensor. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.coalesce()</span></code> for details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.var">
<code class="sig-name descname"><span class="pre">var</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.var" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.var()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.vdot">
<code class="sig-name descname"><span class="pre">vdot</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.vdot" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vdot()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.view">
<code class="sig-name descname"><span class="pre">view</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.view" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a
different <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.shape" title="hxtorch.spiking.modules.synapse.Parameter.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a>.</p>
<p>The returned tensor shares the same data and must have the same number
of elements, but may have a different size. For a tensor to be viewed, the new
view size must be compatible with its original size and stride, i.e., each new
view dimension must either be a subspace of an original dimension, or only span
across original dimensions <span class="math notranslate nohighlight">\(d, d+1, \dots, d+k\)</span> that satisfy the following
contiguity-like condition that <span class="math notranslate nohighlight">\(\forall i = d, \dots, d+k-1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]\]</div>
<p>Otherwise, it will not be possible to view <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor as <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.shape" title="hxtorch.spiking.modules.synapse.Parameter.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a>
without copying it (e.g., via <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.contiguous" title="hxtorch.spiking.modules.synapse.Parameter.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a>). When it is unclear whether a
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.view" title="hxtorch.spiking.modules.synapse.Parameter.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a> can be performed, it is advisable to use <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.reshape" title="hxtorch.spiking.modules.synapse.Parameter.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a>, which
returns a view if the shapes are compatible, and copies (equivalent to calling
<a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.contiguous" title="hxtorch.spiking.modules.synapse.Parameter.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a>) otherwise.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>shape (torch.Size or int…): the desired size</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([16])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 8])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Swaps 2nd and 3rd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Does not change tensor layout in memory</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="py method">
<dt>
<code class="sig-name descname"><span class="pre">view</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span></dt>
<dd></dd></dl>

<p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a
different <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>.</p>
<p>If the element size of <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> is different than that of <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>,
then the size of the last dimension of the output will be scaled
proportionally.  For instance, if <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> element size is twice that of
<code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>, then each pair of elements in the last dimension of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> will be combined, and the size of the last dimension of the output
will be half that of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> element size is half that
of <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>, then each element in the last dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> will
be split in two, and the size of the last dimension of the output will be
double that of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. For this to be possible, the following conditions
must be true:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.dim()</span></code> must be greater than 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.stride(-1)</span></code> must be 1.</p></li>
</ul>
</div></blockquote>
<p>Additionally, if the element size of <a class="reference internal" href="#hxtorch.spiking.modules.synapse.Parameter.dtype" title="hxtorch.spiking.modules.synapse.Parameter.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> is greater than that of
<code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>, the following conditions must be true as well:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.size(-1)</span></code> must be divisible by the ratio between the element
sizes of the dtypes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.storage_offset()</span></code> must be divisible by the ratio between the
element sizes of the dtypes.</p></li>
<li><p>The strides of all dimensions, except the last dimension, must be
divisible by the ratio between the element sizes of the dtypes.</p></li>
</ul>
</div></blockquote>
<p>If any of the above conditions are not met, an error is thrown.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This overload is not supported by TorchScript, and using it in a Torchscript
program will cause undefined behavior.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><p>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>): the desired dtype</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],</span>
<span class="go">        [-0.1520,  0.7472,  0.5617, -0.8649],</span>
<span class="go">        [-2.4724, -0.0334, -0.2976, -0.8499],</span>
<span class="go">        [-0.2109,  1.9913, -0.9607, -0.6123]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],</span>
<span class="go">        [-1105482831,  1061112040,  1057999968, -1084397505],</span>
<span class="go">        [-1071760287, -1123489973, -1097310419, -1084649136],</span>
<span class="go">        [-1101533110,  1073668768, -1082790149, -1088634448]],</span>
<span class="go">    dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000000000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],</span>
<span class="go">        [-0.1520,  0.7472,  0.5617, -0.8649],</span>
<span class="go">        [-2.4724, -0.0334, -0.2976, -0.8499],</span>
<span class="go">        [-0.2109,  1.9913, -0.9607, -0.6123]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span>
<span class="go">tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],</span>
<span class="go">        [-0.1520+0.7472j,  0.5617-0.8649j],</span>
<span class="go">        [-2.4724-0.0334j, -0.2976-0.8499j],</span>
<span class="go">        [-0.2109+1.9913j, -0.9607-0.6123j]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="go">tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,</span>
<span class="go">           8, 191],</span>
<span class="go">        [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,</span>
<span class="go">          93, 191],</span>
<span class="go">        [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,</span>
<span class="go">          89, 191],</span>
<span class="go">        [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,</span>
<span class="go">          28, 191]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 16])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.view_as">
<code class="sig-name descname"><span class="pre">view_as</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.view_as" title="Permalink to this definition"></a></dt>
<dd><p>View this tensor as the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.view_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.view(other.size())</span></code>.</p>
<p>Please see <code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code> for more information about <code class="docutils literal notranslate"><span class="pre">view</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>other (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The result tensor has the same size</dt><dd><p>as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="hxtorch.spiking.modules.synapse.Parameter.volatile">
<code class="sig-name descname"><span class="pre">volatile</span></code><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.volatile" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.vsplit">
<code class="sig-name descname"><span class="pre">vsplit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size_or_sections</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.vsplit" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vsplit()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.where">
<code class="sig-name descname"><span class="pre">where</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.where" title="Permalink to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.where(condition,</span> <span class="pre">y)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">torch.where(condition,</span> <span class="pre">self,</span> <span class="pre">y)</span></code>.
See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.where()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.xlogy">
<code class="sig-name descname"><span class="pre">xlogy</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.xlogy" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.xlogy()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.xlogy_">
<code class="sig-name descname"><span class="pre">xlogy_</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.xlogy_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">xlogy()</span></code></p>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.xpu">
<code class="sig-name descname"><span class="pre">xpu</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.xpu" title="Permalink to this definition"></a></dt>
<dd><p>Returns a copy of this object in XPU memory.</p>
<p>If this object is already in XPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>): The destination XPU device.</dt><dd><p>Defaults to the current XPU device.</p>
</dd>
<dt>non_blocking (bool): If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,</dt><dd><p>the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>, optional): the desired memory format of</dt><dd><p>returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="hxtorch.spiking.modules.synapse.Parameter.zero_">
<code class="sig-name descname"><span class="pre">zero_</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Tensor</span><a class="headerlink" href="#hxtorch.spiking.modules.synapse.Parameter.zero_" title="Permalink to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with zeros.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hxtorch.spiking.modules.synapse.html" class="btn btn-neutral float-left" title="hxtorch.spiking.modules.synapse" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hxtorch.spiking.modules.synapse.Projection.html" class="btn btn-neutral float-right" title="hxtorch.spiking.modules.synapse.Projection" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Electronic Vision(s).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>